<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>Neural.Rectifier API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>Neural.Rectifier</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">#used Modules:
import numpy as np



#content of this Modul:
# #### [Activation class]

# #### [Cost function]

# #### [Optimizers]

# #### [Learning Rate decay]

# #### [Utility function]

# #### [Weights initializer class]







class Activation:
    def __init__(self, activation_type= &#34;linear&#34;):
        self.activation_type = activation_type
        self.funcObj = recPicker(activation_type)
        self.forw = self.funcObj.func
        self.backw = self.funcObj.deriv
    
    def forward(self, X):
        self.X = X
        z = self.forw(X)
        return z
    
    def backpropagation(self, dz):
        f_prime = self.backw(self.X)
        if self.activation_type==&#39;softmax&#39;:
            # because derivative of softmax is a tensor
            dx = np.einsum(&#39;ijk,ik-&gt;ij&#39;, f_prime, dz)
        else:
            dx = dz * f_prime
        return dx


&#39;&#39;&#39;
Parameters

x: input matrix of shape (m, d)
where &#39;m&#39; is the number of samples (in case of batch gradient descent of size m)
and &#39;d&#39; is the number of features
&#39;&#39;&#39;
class Function():
    def __init__(self):
        pass
    
    def func(self, x):
        return x
    
    def deriv(self,x):
        return 1

#replaces the derive function with f(x) = 1
class NoDeriv(Function): 
    def __init__(self, f=&#39;sigmoid&#39;):
        self.f = recPicker(float)
        
    def func(self, x):
        return self.f.func(x)
    
    def deriv(self, x): 
        return 1
    
    def change(self,func):
        self.f = func
        return self

#Sigmoid Funktionen
class Sigmoid(Function):
    def func(self, x):
        return 1 / (1 + np.exp(-x))
    
        
    def deriv(self, x):
        return self.func(x) * (1 - self.func(x))
    
class BSigmoid(Function): #Bipolares Sigmoid f(x) = (1 - e^(-x)) / (1 + e^(-x))
    def func(self,x):
        return (1 - np.exp(-x)) / (1 + np.exp(-x))
    
    def deriv(self,x):
        return 1 / (2 * (0.5 * (np.exp(0.5*x) + np.exp(-0.5*x)))**2)


#Tanh Funktionen
class Tanh(Function):
    def func(self, x):
        z = np.exp(x)
        return (z - 1/z) / (z + 1/z)
    
    def deriv(self, x):
        return 1 - (self.func(x))**2
class LCTanh(Function): #LeCuns Tanh f(x) = 1.7159 * tanh((2/3) * x)
    def __init__(self):
        self.tanh = Tanh()
        
    def func(self,x):
        return 1.7159 * self.tanh.func((2/3)*x)
    
    def deriv(self,x):
        return 1.7159 * self.tanh.deriv((2/3)*x) * (2/3)   
class HTanh(Function): #Hard Tanh f(x) = max(-1, min(1,x))
    def __init__(self,start=-1, end= 1):
        self.s = start
        self.e = end
    
    def func(self, x):
        min = np.where(x&gt;self.e, self.e)
        return np.where(self.s&lt; min, self.s)
    
    def intfunc(self, x):
        return max(self.s, min(x, self.e))
    
    def deriv(self, x):
        #kx = np.where(-1&lt;x&lt;1, 1)
        #return np.where(kx&lt;=-1 or kx&gt;=1, 0)
        return x * (-1 &lt; x &lt; 1)
        
    def intderiv(self,x):
        if -1 &lt; x &lt; 1:return 1
        else: return 0
           
    

#Relu Funktionen    
class ReLU(Function):
    def func(self,x): 
        return x * (x&gt;0)
    
    def deriv(self, x):
        return (x&gt;0) * np.ones(x.shape)
        
    def intderiv(self, x):
        if x &gt; 0: return 0
        return 1
    
class LReLU(Function): #Leaky ReLU
    def __init__(self, a = 0.2):
        self.a = a #slope Parameter
    
    def func(self, x, alpha = None):
        if alpha is None: alpha = self.a
        return np.where(x&gt;0, x, alpha*x)
        
    def deriv(self, x, alpha):
        if alpha is None: alpha = self.a
        return np.where(x&gt;0, 1, alpha)
    
    def intfunc(self, x, alpha = None):
        if alpha is None: alpha = self.a
        if x &gt; 0: return x
        return alpha*x
    
    def intderiv(self, x, alpha = None):
        if alpha is None: alpha = self.a
        if x &gt; 0: return 1
        return alpha
    
class CosReLU(Function): #1. Modifikation mit cos von Relu gut f√ºr mnist f(x) = max(0,x) + cos(x)
    def __init__(self,wende=0):
        self.w = wende #Wendepunkt
    
    def func(self, x):
        return x*(x&gt;0) + np.cos(x)

    def deriv(self, x):
        return np.where(x&gt;0, 1-np.sin(x), -np.sin(x))
    
    def intderiv(self,x):
        if x&gt;0: return   1 - np.sin(x)
        return -np.sin(x)
class SinReLU(Function): #2.Modifikation mit sin
    def __init__(self,wende=0):
        self.w = wende #Wendepunkt
    
    def func(self, x):
        return x*(x&gt;0) + np.cos(x)
    def deriv(self, x):
        return np.where(x&gt;0, 1-np.cos(x), -np.cos(x))
    
    def intderiv(self,x):
        if x&gt;0: return  1 + np.cos(x)
        return np.cos(x)
    
class SReLU(Function): #Smooth rectified Linear Unit/Smooth Max/Soft Plus
    def func(self, x):
        return np.log(1 + np.exp(x))

    def deriv(self,x):
        return 1/(1 + np.exp(-x))


#Lineare Funktionen
class Linear(Function):
    def func(self, x):
        return x
    def deriv(self, x):
        return np.ones(x.shape)
    
class Linear2(Function):
    def __init__(self, m=1, n=0):
        self.m = m
        self.n = n
    
    def func(self, x):
        return self.m * x + self.n
    def deriv(self, x):
        return np.ones(x.shape)
    
    def intderiv(self, x):
        return self.m
    
class PwLinear(Function): #piecewise(St√ºckweise) Linear
    def __init__(self, start, end, before = 0, after = 1):
        self.s = start #Start Stelle der Liniaren Funktion
        self.e = end #Ende der Liniaren Funktion
        self.b = before # f(x) wenn x&lt; start
        self.a = after #fx) wenn x &gt; end
        self.m = (after - before) / (end - start) #Anstieg der Linearen Funktion von start to end m = (y2 - y1)/(x2 - x1)
        self.n = before - (self.m * start) #Schnittstelle mit de y-Achse n = y - (m * x)
    
    def update(self):
        self.m = (self.a - self.b) / (self.e - self.s) #Anstieg der Linearen Funktion von start to end m = (y2 - y1)/(x2 - x1)
        self.n = self.b - (self.m * self.s) #Schnittstelle mit de y-Achse n = y - (m * x)
    
    def func(self, x):
        kx = np.where(self.start&lt;x&lt;self.e, x*self.m+self.n)
        kkx = np.where(self.e&lt;kx, self.a)
        return np.where(self.s&gt;kkx, self.b)
    def deriv(self, x):
        return np.where(self.s&lt;x&lt;self.e, self.m, 0)
    
    def intfunc(self, x):
        if x &lt; self.s: return self.b
        elif x &gt; self.e: return self.a
        else: return x*self.m + self.n
    
    def intderiv(self, x):
        if x &lt; self.s: return 0
        elif x &gt; self.e: return 0
        else: return self.m


class Step(Function):
    def __init__(self, step = 0, b = 0, a=1): #f(x) = { b if x&lt;=step; a if x&gt;step}
        self.step = step
        self.b = b
        self.a = a
    
    def func(self, x, step = None, a = None, b = None):
        if step is None: step = self.step
        if a is None: a = self.a
        if b is None: b = self.b
        return np.where(x&gt; np.step, self.a, self.b)
    
    def deriv(self, x, step = None, a = None, b= None):
        if step is None: step = self.step
        if a is None: a = self.a
        if b is None: b = self.b
        return np.zeros(x.shape)
    
    def intfunc(self,x):
        if x &gt; self.step: return self.a
        return self.b
    
    def deriv(self,x):
        return 0
        
class Abs(Function):
    def func(self, x):
        if x&lt;0: return -x
        return np.abs(x)
    
    def deriv(self, x):
        return np.ones(x.shape)
    
    def intderiv(self, x):
        return 1

#Andere Funktionen
class Softmax(Function):
    def func(self,x):
        &#39;&#39;&#39;
        Parameters

        x: input matrix of shape (m, d)
        where &#39;m&#39; is the number of samples (in case of batch gradient descent of size m)
        and &#39;d&#39; is the number of features
        &#39;&#39;&#39;
        z = x - np.max(x, axis=-1, keepdims=True)
        numerator = np.exp(z)
        denominator = np.sum(numerator, axis=-1, keepdims=True)
        softmax = numerator / denominator
        return softmax
    
    def softmax_grad(s): 
        # Take the derivative of softmax element w.r.t the each logit which is usually Wi * X
        # input s is softmax value of the original input x. 
        # s.shape = (1, n) 
        # i.e. s = np.array([0.3, 0.7]), x = np.array([0, 1])

        # initialize the 2-D jacobian matrix.
        jacobian_m = np.diag(s)

        for i in range(len(jacobian_m)):
            for j in range(len(jacobian_m)):
                if i == j:
                    jacobian_m[i][j] = s[i] * (1-s[i])
                else: 
                    jacobian_m[i][j] = -s[i]*s[j]
        return jacobian_m
    
    def deriv(self, x):
        &#39;&#39;&#39;
        Parameters

        x: input matrix of shape (m, d)
        where &#39;m&#39; is the number of samples (in case of batch gradient descent of size m)
        and &#39;d&#39; is the number of features
        &#39;&#39;&#39;
        if len(x.shape)==1:
            x = np.array(x).reshape(1,-1)
        else:
            x = np.array(x)
        m, d = x.shape
        a = self.func(x)
        tensor1 = np.einsum(&#39;ij,ik-&gt;ijk&#39;, a, a)
        tensor2 = np.einsum(&#39;ij,jk-&gt;ijk&#39;, a, np.eye(d, d))
        return tensor2 - tensor1

class KLL(Function): #komplement√§res Log-Log f(x) = 1 - e^(-e^x)
    def func(self,x):
        return  1 - np.exp(-np.exp(x))
    
    def deriv(self,x):
        return np.exp(-np.exp(x))

class Logit(Function):
    def func(self, x):
        return np.log((x/(1-x)))
    
    def deriv(self, x):
        return (-1) / (x*(x-1))
    
#class Probit(Function):

class Cosinus(Function):
    def func(self,x):
        return np.cos(x)
    def deriv(self,x): 
        return -np.sin(x)

class Sinus(Function):
    def func(self,x):
        return np.sin(x)
    def deriv(self,x): 
        return np.cos(x)



#Radiale Basisfunktion Netzwerkaktivierungsfunktionen
class Gaussche(Function):
    def func(self, x):
        return np.exp(-0.5 * (x**2))
    
    def deriv(self,x):
        return -x * np.exp(-0.5 * x**2)

class Normal(Function): #Normalverteilung / Dichtefunktion
    def __init__(self, mhy, phi):
        self.m = mhy
        self.p = phi
    
    def func(self,x):
        return (1/(np.sqrt(2*np.pi * (self.p**2))))  * np.exp(-0.5 * ((x-self.m)/self.p)**2)
    
    def deriv(self, x):
        return ((self.m * np.exp((-self.m**2)/(2* self.p**2)) * np.sqrt(2)) / (2 * np.abs(self.p**3) * np.sqrt(np.pi))    -    (x * np.exp((-self.m**2)/(2* self.p**2)) * np.sqrt(2)) / (2 * np.abs(self.p**3) * np.sqrt(np.pi)))    *    np.exp(((self.m * x)/(self.p**2))  -  ((x**2) / (2 * self.p**2)))

class Multiquatratisch(Function):#Abstan (x,0) zu Punkt (x,y)
    def __init__(self,dot):
        self.x = dot[0]
        self.y = dot[1]
    
    def func(self, x):
        return np.sqrt((x - self.x)**2 + self.y**2)
    
    def deriv(self, x):
        return (x - self.x)  /  (np.sqrt(x**2 - 2*x*self.x + self.y**2 + self.x**2))
       
       
class IMultiquatratisch(Function):#Inverse multiquatratisch
    def __init__(self,dot):
        self.x = dot[0]
        self.y = dot[1]
    
    def func(self, x):
        return 1 / np.sqrt((x - self.x)**2 + self.y**2)
    
    def deriv(self, x):
        return -(x - self.x)  /  (np.sqrt(x**2 - 2*x*self.x + self.y**2 + self.x**2) ** (3/2))
    


def recPicker(name):
    #Bezeichnungen welche f√ºr die Funktionen eingegeben werden koennen
    names = [[&#34;noderive&#34;,&#34;replacederive&#34;,&#34;noderiv&#34;],[&#34;sigmoid&#34;,&#34;sig&#34;], [&#34;bsigmoid&#34;, &#34;bipolaressigmoid&#34;], [&#34;tanh&#34;,&#34;tangenshyperbolicus&#34;], [&#34;lctanh&#34;, &#34;lecunstanh&#34;, &#34;lecunstangenshyperbolicus&#34;, &#34;lctangenshyperbolicus&#34;, &#34;lecun&#39;stangenshyperbolicus&#34;, &#34;lecun&#39;stanh&#34;], [&#34;htanh&#34;, &#34;hardtanh&#34;, &#34;htangenshyperbolicus&#34;, &#34;hardtangenshyperbolicus&#34;], [&#34;relu&#34;, &#34;rectifiedlinearunit&#34;, &#34;gleichrichter&#34;, &#34;maxfunktion&#34;, &#34;rampenfunktion&#34;], [&#34;lrelu&#34;, &#34;lrectifiedlinearunit&#34;, &#34;lgleichrichter&#34;, &#34;leakyrelu&#34;, &#34;leakyrectifiedlinearunit&#34;, &#34;leakygleichrichter&#34;, &#34;lmaxfunktion&#34;, &#34;lrampenfunktion&#34;, &#34;leakymaxfunktion&#34;, &#34;leakyrampenfunktion&#34;], [&#34;mcrelu&#34;, &#34;mcrectifiedlinearunit&#34;, &#34;mcgleichrichter&#34;, &#34;mcmaxfunktion&#34;, &#34;mcrampenfunktion&#34;, &#34;crelu&#34;, &#34;crectifiedlinearunit&#34;, &#34;cgleichrichter&#34;, &#34;cmaxfunktion&#34;, &#34;crampenfunktion&#34;, &#34;cosrelu&#34;, &#34;cosrectifiedlinearunit&#34;, &#34;cosgleichrichter&#34;, &#34;cosmaxfunktion&#34;, &#34;cosrampenfunktion&#34;, &#34;cosinusrelu&#34;, &#34;cosinusrectifiedlinearunit&#34;, &#34;cosinusgleichrichter&#34;, &#34;cosinusmaxfunktion&#34;, &#34;cosinusrampenfunktion&#34;], [&#34;sinrelu&#34;, &#34;sinrectifiedlinearunit&#34;, &#34;singleichrichter&#34;, &#34;sinmaxfunktion&#34;, &#34;sinrampenfunktion&#34;, &#34;sinusrelu&#34;, &#34;sinusrectifiedlinearunit&#34;, &#34;sinusgleichrichter&#34;, &#34;sinusmaxfunktion&#34;, &#34;sinusrampenfunktion&#34;], [&#34;srelu&#34;, &#34;srectifiedlinearunit&#34;, &#34;sgleichrichter&#34;, &#34;smaxfunktion&#34;, &#34;srampenfunktion&#34;, &#34;smoothrelu&#34;, &#34;smoothrectifiedlinearunit&#34;, &#34;smoothgleichrichter&#34;, &#34;smoothmaxfunktion&#34;, &#34;smoothrampenfunktion&#34;], [&#34;linear&#34;, &#34;lin&#34;], [&#34;pwlinear&#34;,&#34;piecewiselinear&#34;, &#34;pwlin&#34;], [&#34;step&#34;, &#34;schritt&#34;], [&#34;abs&#34;,&#34;absolute&#34;, &#34;absolut&#34;],[&#34;softmax&#34;,&#34;normalisiertesexpotential&#34;, &#34;normalizeexpotential&#34;],[&#34;kll&#34;, &#34;komplement√§resloglog&#34;, &#34;komplement√§reslog-log&#34;,&#34;klog-log&#34;,&#34;kloglog&#34;], [&#34;logit&#34;], [&#34;c&#34;,&#34;cos&#34;,&#34;cosinus&#34;], [&#34;s&#34;,&#34;sin&#34;, &#34;sinus&#34;], [&#34;gaussche&#34;,&#34;gauss&#34;], [&#34;normal&#34;, &#34;dichtefunktion&#34;,&#34;normalfunktion&#34;], [&#34;multiquatratisch&#34;], [&#34;imultiquatratisch&#34;, &#34;inversemultiquatratisch&#34;]]
    #Bezeichungen der Funktionen in diesem Script
    functions = [NoDeriv, Sigmoid, BSigmoid, Tanh, LCTanh, HTanh, ReLU, LReLU, CosReLU, SinReLU, SReLU, Linear, PwLinear, Step, Abs, Softmax, KLL, Logit, Cosinus, Sinus, Gaussche, Normal, Multiquatratisch, IMultiquatratisch]
    
    #in der Liste nach passender Funktion suchen und diese zurueckgeben
    for i in range(len(names)):
        if name.lower() in names[i]:
            return functions[i]()
    return False







class Cost:
    
    names = [&#39;mse&#39;, &#39;cross-entropy&#39;, &#39;weighted&#39;, &#39;log&#39;, &#39;kullback-leibler&#39;, &#39;exp&#39;, &#39;hinge&#39;, &#39;huber&#39;] #possible cost functions to choose from

    def __init__(self, cost_type=&#39;mse&#39;):
        &#39;&#39;&#39;
        Parameters

        cost_type: type of cost function
        available options are &#39;mse&#39;, and &#39;cross-entropy&#39;
        &#39;&#39;&#39;
        self.cost_type = cost_type

    def mse(self, a, y):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        return (1/2)*np.sum((np.linalg.norm(a-y, axis=1))**2)

    def d_mse(self, a, y):
        &#39;&#39;&#39;
        represents dJ/da

        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        return a - y

    def cross_entropy(self, a, y, epsilon=1e-12):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        a = np.clip(a, epsilon, 1. - epsilon)
        return -np.sum(y*np.log(a))

    def d_cross_entropy(self, a, y, epsilon=1e-12):
        &#39;&#39;&#39;
        represents dJ/da

        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        a = np.clip(a, epsilon, 1. - epsilon)
        return -y/a
    
    def weighted(self, a, y, c):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        c: Weight factor array of shape (batch, 1)
        &#39;&#39;&#39;
        return c * (a - y)**2

    def d_weighted(self, a, y, c):
        &#39;&#39;&#39;
        represents dJ/da

        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        c: Weight factor array of shape (batch, 1)
        &#39;&#39;&#39;
        return 2 * c * (a - y)

    def log(self, a, y, epsilon=1e-12):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        a = np.clip(a, epsilon, 1. - epsilon)
        return -y * np.log(a) - (1 - y) * np.log(1 - a)

    def d_log(self, a, y, epsilon=1e-12):
        &#39;&#39;&#39;
        represents dJ/da

        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        a = np.clip(a, epsilon, 1. - epsilon)
        return -y / a + (1 - y) / (1 - a)

    def kl(self, a, y, epsilon=1e-12):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        a = np.clip(a, epsilon, 1. - epsilon)
        return np.sum(y * np.log(y / a), axis=1)

    def d_kl(self, a, y, epsilon=1e-12):
        &#39;&#39;&#39;
        represents dJ/da

        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        a = np.clip(a, epsilon, 1. - epsilon)
        return -y / a

    def exp(self, a, y):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        return np.exp(-y * a)

    def d_exp(self, a, y):
        &#39;&#39;&#39;
        represents dJ/da

        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        return -y * np.exp(-y * a)

    def hinge(self, a, y):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        return np.maximum(0, 1 - y * a)

    def d_hinge(self, a, y):
        &#39;&#39;&#39;
        represents dJ/da

        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        return np.where(y * a &gt;= 1, 0, -y)

    def huber(self, a, y, delta):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        delta: Hyperparameter for sensitivity to outliers
        &#39;&#39;&#39;
        error = a - y
        return np.where(np.abs(error) &lt;= delta, 0.5 * error**2, delta * (np.abs(error) - 0.5 * delta))

    def d_huber(self, a, y, delta):
        &#39;&#39;&#39;
        represents dJ/da

        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        delta: Hyperparameter for sensitivity to outliers
        &#39;&#39;&#39;
        error = a - y
        return np.where(np.abs(error) &lt;= delta, error, delta * np.sign(error))


    def get_cost(self, a, y, *args):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        match self.cost_type:
            case &#39;mse&#39;: return self.mse(a, y)
            case &#39;cross-entropy&#39;: return self.cross_entropy(a, y)
            case &#39;weighted&#39;: return self.weighted(a, y, *args)
            case &#39;log&#39;: return self.log(a, y, *args)
            case &#39;kullback-leibler&#39;: return self.kl(a, y, *args)
            case &#39;exp&#39;: return self.exp(a, y)
            case &#39;hinge&#39;: return self.hinge(a, y)
            case &#39;huber&#39;: return self.huber(a, y, *args)
            case _:   raise ValueError(&#34;Valid cost functions are only: &#34;+ &#34;, &#34;.join(self.names))

    def get_d_cost(self, a, y, *args):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        match self.cost_type:
            case &#39;mse&#39;: return self.d_mse(a, y)
            case &#39;cross-entropy&#39;: return self.d_cross_entropy(a, y)
            case &#39;weighted&#39;: return self.d_weighted(a, y, *args)
            case &#39;log&#39;: return self.d_log(a, y, *args)
            case &#39;kullback-leibler&#39;: return self.d_kl(a, y, *args)
            case &#39;exp&#39;: return self.d_exp(a, y)
            case &#39;hinge&#39;: return self.d_hinge(a, y)
            case &#39;huber&#39;: return self.d_huber(a, y, *args)
            case _:   raise ValueError(&#34;Valid cost functions are only: &#34;+ &#34;, &#34;.join(self.names))








# In[ ] Optimizer:


class Optimizer:

    def __init__(self, optimizer_type=None, shape_W=None, shape_b=None,
                 momentum1=0.9, momentum2=0.999, epsilon=1e-8):
        &#39;&#39;&#39;
        Parameters

        momentum1: float hyperparameter &gt;= 0 that accelerates gradient descent in the relevant
                   direction and dampens oscillations. Defaults to 0, i.e., vanilla gradient descent.
                   Also used in RMSProp
        momentum2: used in Adam only
        optimizer_type: type of optimizer
                        available options are &#39;gd&#39;, &#39;sgd&#39; (This also includes momentum), &#39;adam&#39;, and &#39;rmsprop&#39;
        shape_W: Shape of the weight matrix W/ Kernel K
        shape_b: Shape of the bias matrix b
        epsilon: parameter used in RMSProp and Adam to avoid division by zero error
        &#39;&#39;&#39;

        if optimizer_type is None:
            self.optimizer_type = &#39;adam&#39;
        else:
            self.optimizer_type = optimizer_type

        self.momentum1 = momentum1
        self.momentum2 = momentum2
        self.epsilon = epsilon

        self.vdW = np.zeros(shape_W)
        self.vdb = np.zeros(shape_b)

        self.SdW = np.zeros(shape_W)
        self.Sdb = np.zeros(shape_b)
        
        if(optimizer_type.lower() == &#39;rprop&#39;):
            self.delta_W = np.ones(shape_W)
            self.delta_b= np.ones(shape_b)
            self.eta_plus = 1.2
            self.eta_minus = 0.5
            self.delta_max = 50
            self.delta_min = 1e-6

    &#39;&#39;&#39;
        dW: gradient of Weight W for iteration k
        db: gradient of bias b for iteration k
        k: iteration number
    &#39;&#39;&#39;
    
    def GD(self, dW, db, k = None): #Gradient Descent
        
        return dW, db

    def SGD(self, dW, db, k = None): #Stochastic Gradient Descent (with momentum)
        self.vdW = self.momentum1*self.vdW + (1-self.momentum1)*dW
        self.vdb = self.momentum1*self.vdb + (1-self.momentum1)*db

        return self.vdW, self.vdb

    def RMSProp(self, dW, db, k = None):
        self.SdW = self.momentum2*self.SdW + (1-self.momentum2)*(dW**2)
        self.Sdb = self.momentum2*self.Sdb + (1-self.momentum2)*(db**2)

        den_W = np.sqrt(self.SdW) + self.epsilon
        den_b = np.sqrt(self.Sdb) + self.epsilon

        return dW/den_W, db/den_b

    def Adam(self, dW, db, k):
        &#39;&#39;&#39;
        dW: gradient of Weight W for iteration k
        db: gradient of bias b for iteration k
        k: iteration number
        &#39;&#39;&#39;
        # momentum
        self.vdW = self.momentum1*self.vdW + (1-self.momentum1)*dW
        self.vdb = self.momentum1*self.vdb + (1-self.momentum1)*db

        # rmsprop
        self.SdW = self.momentum2*self.SdW + (1-self.momentum2)*(dW**2)
        self.Sdb = self.momentum2*self.Sdb + (1-self.momentum2)*(db**2)

        # correction
        if k&gt;1:
            vdW_h = self.vdW / (1-(self.momentum1**k))
            vdb_h = self.vdb / (1-(self.momentum1**k))
            SdW_h = self.SdW / (1-(self.momentum2**k))
            Sdb_h = self.Sdb / (1-(self.momentum2**k))
        else:
            vdW_h = self.vdW
            vdb_h = self.vdb
            SdW_h = self.SdW
            Sdb_h = self.Sdb

        den_W = np.sqrt(SdW_h) + self.epsilon
        den_b = np.sqrt(Sdb_h) + self.epsilon

        return vdW_h/den_W, vdb_h/den_b
    
    def Adagrad(self, dW, db, k):
        # Update the sum of squares of gradients
        self.SdW += dW**2
        self.Sdb += db**2

        # Calculate the denominator
        den_W = np.sqrt(self.SdW) + self.epsilon
        den_b = np.sqrt(self.Sdb) + self.epsilon

        # Calculate the updated gradients
        return dW/den_W, db/den_b

    def Adadelta(self, dW, db, k):
        # Update the sum of squares of gradients
        self.SdW = self.momentum2*self.SdW + (1-self.momentum2)*(dW**2)
        self.Sdb = self.momentum2*self.Sdb + (1-self.momentum2)*(db**2)

        # Calculate the numerator
        num_W = np.sqrt(self.SdW_h + self.epsilon)*dW
        num_b = np.sqrt(self.Sdb_h + self.epsilon)*db

        # Calculate the denominator
        den_W = np.sqrt(self.SdW) + self.epsilon
        den_b = np.sqrt(self.Sdb) + self.epsilon

        # Update the sum of squares of deltas
        self.SdW_h = self.momentum1*self.SdW_h + (1-self.momentum1)*(num_W**2)
        self.Sdb_h = self.momentum1*self.Sdb_h + (1-self.momentum1)*(num_b**2)

        # Calculate the updated gradients
        return num_W/den_W, num_b/den_b

    def NAG(self, dW, db, k): #Nesterov_Accelerated_Gradient
        # Store the previous velocity
        vdW_prev = self.vdW
        vdb_prev = self.vdb

        # Update the velocity
        self.vdW = self.momentum1*self.vdW - self.learning_rate*dW
        self.vdb = self.momentum1*self.vdb - self.learning_rate*db

        # Calculate the corrected velocity
        vdW_corr = self.momentum1*self.vdW + (1-self.momentum1)*vdW_prev
        vdb_corr = self.momentum1*self.vdb + (1-self.momentum1)*vdb_prev

        # Calculate the denominator
        den_W = np.sqrt(self.SdW) + self.epsilon
        den_b = np.sqrt(self.Sdb) + self.epsilon

        # Calculate the updated gradients
        return vdW_corr/den_W, vdb_corr/den_b

    def AdaMax(self, dW, db, k):
        # Update the sum of powers of gradients
        self.SdW = self.beta2*self.SdW + (1-self.beta2)*(dW**2)
        self.Sdb = self.beta2*self.Sdb + (1-self.beta2)*(db**2)

        # Calculate the numerator
        num_W = self.learning_rate*dW/(np.sqrt(self.SdW) + self.epsilon)
        num_b = self.learning_rate*db/(np.sqrt(self.Sdb) + self.epsilon)

        # Calculate the updated gradients
        return num_W, num_b

    def Nadam(self, dW, db, k, delta_min, delta_max):
        # Update the momentum
        self.vdW = self.momentum1*self.vdW + (1-self.momentum1)*dW
        self.vdb = self.momentum1*self.vdb + (1-self.momentum1)*db

        # Update the sum of squares of gradients
        self.SdW = self.momentum2*self.SdW + (1-self.momentum2)*(dW**2)
        self.Sdb = self.momentum2*self.Sdb + (1-self.momentum2)*(db**2)

        # Calculate the corrected momentum
        vdW_corr = self.vdW/(1-(self.momentum1**k))
        vdb_corr = self.vdb/(1-(self.momentum1**k))

        # Calculate the corrected sum of squares of gradients
        SdW_corr = self.SdW/(1-(self.momentum2**k))
        Sdb_corr = self.Sdb/(1-(self.momentum2**k))

        # Calculate the numerator
        num_W = self.learning_rate*(self.momentum1*vdW_corr + ((1-self.momentum1)/(1-(self.momentum1**k)))*dW)/(np.sqrt(SdW_corr) + self.epsilon)
        num_b = self.learning_rate*(self.momentum1*vdb_corr + ((1-self.momentum1)/(1-(self.momentum1**k)))*db)/(np.sqrt(Sdb_corr) + self.epsilon)
        return num_W, num_b
        
    def Rprop(self, dW, db, k):
        self.delta_W = np.clip(self.delta_W * np.where(np.sign(dW) == np.sign(self.delta_W), self.eta_plus, self.eta_minus), delta_min, delta_max)
        self.delta_b = np.clip(self.delta_b * np.where(np.sign(db) == np.sign(self.delta_b), self.eta_plus, self.eta_minus), delta_min, delta_max)
        nW = -np.sign(dW) * self.delta_W
        nb = -np.sign(db) * self.delta_b
        return nW, nb

    def get_optimization(self, dW, db, k, *args):
        match self.optimizer_type:
            case &#39;gd&#39;: return self.GD(dW, db, k)
            case &#39;sgd&#39;: return self.SGD(dW, db, k)
            case &#39;rsmprop&#39;: return self.RMSProp(dW, db, k)
            case &#39;adam&#39;: return self.Adam(dW, db, k)
            case &#39;adagrad&#39;: return self.Adagrad(dW, db, k)
            case &#39;adadelta&#39;: return self.Adadelta(dW, db, k)
            case &#39;nag&#39;: return self.NAG(dW, db, k)
            case &#39;adamax&#39;: return self.AdaMax(dW, db, k)
            case &#39;nadam&#39;: return self.Nadam(dW, db, k, *args)
            case &#39;rprop&#39;: return self.Rprop(dW, db, k)
            case _: raise ValueError(&#34;Valid optimizer options are only &#39;gd&#39;, &#39;sgd&#39;, &#39;rmsprop&#39;, &#39;adam&#39;, &#39;adagrad&#39;, &#39;adadelta&#39;, &#39;nag&#39;, &#39;adamax&#39;, &#39;nadam&#39; and &#39;rprop&#39;.&#34;)
        


# In[ ] Learning Rate Decay:


class LearningRateDecay:

    def __init__(self):
        pass

    def constant(self, t, learnrate_0):
        &#39;&#39;&#39;
        t: iteration
        learnrate_0: initial learning rate
        &#39;&#39;&#39;
        return learnrate_0

    def time_decay(self, t, learnrate_0, k):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        k: Decay rate
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 /(1+(k*t))
        return learnrate

    def step_decay(self, t, learnrate_0, F, D):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        F: factor value controlling the rate in which the learning date drops
        D: ‚ÄúDrop every‚Äù iteration
        t: current iteration
        &#39;&#39;&#39;
        mult = F**np.floor((1+t)/D)
        learnrate = learnrate_0 * mult
        return learnrate

    def exponential_decay(self, t, learnrate_0, k):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        k: Exponential Decay rate
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 * np.exp(-k*t)
        return learnrate
    
    def inverse_time_decay(self, t, learnrate_0, k):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        k: Decay rate
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 / (1 + k * t)
        return learnrate

    def natural_exp_decay(self, t, learnrate_0, k):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        k: Decay rate
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 * np.exp(-k * t)
        return learnrate

    def piecewise_constant_decay(self, t, learnrate_0, boundaries, values):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        t: iteration number
        boundaries: list of iteration numbers at which the learning rate changes
        values: list of learning rates corresponding to the intervals defined by boundaries
        &#39;&#39;&#39;
        for b, v in zip(boundaries, values):
            if t &lt; b:
                return v
        return values[-1]
    
    def polynomial_decay(self, t, learnrate_0, power=1):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        power: exponent of the polynomial
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 / (1 + t)**power
        return learnrate

    def cosine_decay(self, t, learnrate_0, T_max):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        T_max: maximum number of iterations
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 * 0.5 * (1 + np.cos(np.pi * t / T_max))
        return learnrate

    def linear_cosine_decay(self, t, learnrate_0, T_max):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        T_max: maximum number of iterations
        t: iteration number
        &#39;&#39;&#39;
        alpha = t / T_max
        learnrate = (1 - alpha) * learnrate_0 * 0.5 * (1 + np.cos(np.pi * t / T_max)) + alpha * learnrate_0
        return learnrate
    
    def warmup_decay(self, t, learnrate_0, warmup_steps=10):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        warmup_steps: number of initial iterations for which to use linearly increasing learning rate
        t: iteration number
        &#39;&#39;&#39;
        if t &lt; warmup_steps:
            return learnrate_0 * (t / warmup_steps)
        else:
            return learnrate_0

    def cyclic_decay(self, t, learnrate_0, step_size, max_lr, mode=&#39;triangular&#39;):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        step_size: number of iterations in one half cycle
        max_lr: maximum learning rate
        mode: type of cyclic decay (&#39;triangular&#39;, &#39;triangular2&#39;, &#39;exp_range&#39;)
        t: iteration number
        &#39;&#39;&#39;
        cycle = np.floor(1 + t / (2 * step_size))
        x = np.abs(t / step_size - 2 * cycle + 1)
        if mode == &#39;triangular&#39;:
            lr = learnrate_0 + (max_lr - learnrate_0) * np.maximum(0, (1 - x))
        elif mode == &#39;triangular2&#39;:
            lr = learnrate_0 + (max_lr - learnrate_0) * np.maximum(0, (1 - x)) / (2 ** (cycle - 1))
        elif mode == &#39;exp_range&#39;:
            lr = learnrate_0 + (max_lr - learnrate_0) * np.maximum(0, (1 - x)) * (0.99999 ** t)
        return lr

    def sqrt_decay(self, t, learnrate_0, k=1):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        k: Decay rate
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 / np.sqrt(k * t + 1)
        return learnrate

    def power_decay(self, t, learnrate_0, k=0.1, power=0.75):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        k: Decay rate
        power: power to which iteration number is raised
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 / (1 + k * t)**power
        return learnrate

    def staircase_decay(self, t, learnrate_0, drop_rate=0.5, epochs_drop=10.0):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        drop_rate: factor by which learning rate is reduced at each stage
        epochs_drop: number of epochs after which learning rate drops
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 * (drop_rate ** np.floor((1+t) / epochs_drop))
        return learnrate

    def cosine_annealing_decay(self, t, learnrate_0, T_max, M):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        T_max: maximum number of iterations
        M: number of cycles
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 / 2 * (np.cos(np.pi * (t % (T_max // M)) / (T_max // M)) + 1)
        return learnrate

    def burnin_decay(self, t, learnrate_0, burnin, learnrate_burnin):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        burnin: number of initial iterations for which to use burnin learning rate
        learnrate_burnin: learning rate used during burnin period
        t: iteration number
        &#39;&#39;&#39;
        if t &lt; burnin:
            return learnrate_burnin
        else:
            return learnrate_0

    def warm_restart_decay(self, t, learnrate_0, T_0, T_mult):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        T_0: initial number of iterations
        T_mult: factor by which number of iterations increases after each restart
        t: iteration number
        &#39;&#39;&#39;
        T_i = T_0
        while t &gt;= T_i:
            t = t - T_i
            T_i = T_i * T_mult
        learnrate = learnrate_0 * 0.5 * (1 + np.cos(np.pi * t / T_i))
        return learnrate
    
    def decay_on_plateau(self, t, learnrate_0, min_lr, factor=0.1, patience=10, cooldown=0):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        min_lr: minimum learning rate
        factor: factor by which the learning rate is reduced
        patience: number of epochs with no improvement after which learning rate will be reduced
        cooldown: number of epochs to wait before resuming normal operation after lr has been reduced
        t: iteration number
        &#39;&#39;&#39;
        if t &lt; cooldown:
            return learnrate_0
        elif t % patience == 0:
            return max(learnrate_0 * factor, min_lr)
        else:
            return learnrate_0

    def one_cycle_policy(self, t, learnrate_max, stepsize, base_lr=None):
        &#39;&#39;&#39;
        learnrate_max: maximum learning rate
        stepsize: number of iterations in one half cycle
        base_lr: initial learning rate
        t: iteration number
        &#39;&#39;&#39;
        if base_lr is None:
            base_lr = learnrate_max / 10
        if t &lt;= stepsize:
            return base_lr + (learnrate_max - base_lr) * t / stepsize
        elif t &lt;= 2 * stepsize:
            return learnrate_max - (learnrate_max - base_lr) * (t - stepsize) / stepsize
        else:
            return base_lr
    
    def cosine_decay_restarts(self, t, learnrate_0, T_0, T_mult=2):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        T_0: initial number of iterations
        T_mult: factor by which number of iterations increases after each restart
        t: iteration number
        &#39;&#39;&#39;
        T_i = T_0
        while t &gt;= T_i:
            t = t - T_i
            T_i = T_i * T_mult
        learnrate = learnrate_0 * 0.5 * (1 + np.cos(np.pi * t / T_i))
        return learnrate

    def triangular_learning_rate(self, t, learnrate_low, learnrate_high, T):
        &#39;&#39;&#39;
        learnrate_low: lower bound for learning rate
        learnrate_high: upper bound for learning rate
        T: total number of iterations
        t: iteration number
        &#39;&#39;&#39;
        cycle = np.floor(1 + t / (2 * T))
        x = np.abs(t / T - 2 * cycle + 1)
        learnrate = learnrate_low + (learnrate_high - learnrate_low) * np.maximum(0, (1 - x))
        return learnrate

    def triangular2_learning_rate(self, t, learnrate_low, learnrate_high, T):
        &#39;&#39;&#39;
        learnrate_low: lower bound for learning rate
        learnrate_high: upper bound for learning rate
        T: total number of iterations
        t: iteration number
        &#39;&#39;&#39;
        cycle = np.floor(1 + t / (2 * T))
        x = np.abs(t / T - 2 * cycle + 1)
        learnrate = learnrate_low + (learnrate_high - learnrate_low) * np.maximum(0, (1 - x)) / (2 ** (cycle - 1))
        return learnrate
    
    def exp_range_decay(self, t, learnrate_0, gamma, step_size):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        gamma: decay rate
        step_size: number of iterations in one half cycle
        t: iteration number
        &#39;&#39;&#39;
        cycle = np.floor(1 + t / (2 * step_size))
        x = np.abs(t / step_size - 2 * cycle + 1)
        learnrate = learnrate_0 * (gamma**(t)) * np.maximum(0, (1 - x))
        return learnrate

    def triangular_lr_decay(self, t, learnrate_low, learnrate_high, T):
        &#39;&#39;&#39;
        learnrate_low: lower bound for learning rate
        learnrate_high: upper bound for learning rate
        T: total number of iterations
        t: iteration number
        &#39;&#39;&#39;
        cycle = np.floor(1 + t / (2 * T))
        x = np.abs(t / T - 2 * cycle + 1)
        learnrate = learnrate_low + (learnrate_high - learnrate_low) * np.maximum(0, (1 - x))
        return learnrate

    def triangular2_lr_decay(self, t, learnrate_low, learnrate_high, T):
        &#39;&#39;&#39;
        learnrate_low: lower bound for learning rate
        learnrate_high: upper bound for learning rate
        T: total number of iterations
        t: iteration number
        &#39;&#39;&#39;
        cycle = np.floor(1 + t / (2 * T))
        x = np.abs(t / T - 2 * cycle + 1)
        learnrate = learnrate_low + (learnrate_high - learnrate_low) * np.maximum(0, (1 - x)) / (2 ** (cycle - 1))
        return learnrate







# In[ ]Utility:


class Utility:

    def __init__(self):
        pass

    def label_encoding(self, Y):
        &#39;&#39;&#39;
        Parameters:
        Y: (batch,d) shape matrix with categorical data
        Return
        result: label encoded data of ùëå
        idx_list: list of the dictionaries containing the unique values
                  of the columns and their mapping to the integer.
        &#39;&#39;&#39;
        idx_list = []
        result = []
        for col in range(Y.shape[1]):
            indexes = {val: idx for idx, val in enumerate(np.unique(Y[:, col]))}
            result.append([indexes[s] for s in Y[:, col]])
            idx_list.append(indexes)
        return np.array(result).T, idx_list

    def onehot(self, X):
        &#39;&#39;&#39;
        Parameters:
        X: 1D array of labels of length &#34;batch&#34;
        Return
        X_onehot: (batch,d) one hot encoded matrix (one-hot of X)
                  (where d is the number of unique values in X)
        indexes: dictionary containing the unique values of X and their mapping to the integer column
        &#39;&#39;&#39;
        indexes = {val: idx for idx, val in enumerate(np.unique(X))}
        y = np.array([indexes[s] for s in X])
        X_onehot = np.zeros((y.size, len(indexes)))
        X_onehot[np.arange(y.size), y] = 1
        return X_onehot, indexes

    def minmax(self, X, min_X=None, max_X=None):
        if min_X is None:
            min_X = np.min(X, axis=0)
        if max_X is None:
            max_X = np.max(X, axis=0)
        Z = (X - min_X) / (max_X - min_X)
        return Z, min_X, max_X

    def standardize(self, X, mu=None, std=None):
        if mu is None:
            mu = np.mean(X, axis=0)
        if std is None:
            std = np.std(X, axis=0)
        Z = (X - mu) / std
        return Z, mu, std

    def inv_standardize(self, Z, mu, std):
        X = Z*std + mu
        return X

    def train_test_split(self, X, y, test_ratio=0.2, seed=None):
        if seed is not None:
            np.random.seed(seed)
        train_ratio = 1-test_ratio
        indices = np.random.permutation(X.shape[0])
        train_idx, test_idx = indices[:int(train_ratio*len(X))], indices[int(train_ratio*len(X)):]
        X_train, X_test = X[train_idx,:], X[test_idx,:]
        y_train, y_test = y[train_idx], y[test_idx]
        return X_train, X_test, y_train, y_test








# In[ ] Weights Initializer :


class Weights_initializer:

    def __init__(self, shape, initializer_type=None, seed=None):
        &#39;&#39;&#39;
        Parameters
        shape: Shape of the weight matrix

        initializer_type: type of weight initializer
        available options are &#39;zeros&#39;, &#39;ones&#39;, &#39;random_normal&#39;, &#39;random_uniform&#39;,
        &#39;he_normal&#39;, &#39;xavier_normal&#39; and &#39;glorot_normal&#39;
        &#39;&#39;&#39;
        self.shape = shape
        if initializer_type is None:
            self.initializer_type = &#34;he_normal&#34;
        else:
            self.initializer_type = initializer_type
        self.seed = seed

    def zeros_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        return np.zeros(self.shape)

    def ones_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        return np.ones(self.shape)

    def random_normal_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        return np.random.normal(size=self.shape)

    def random_uniform_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        return np.random.uniform(size=self.shape)

    def he_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        try:
            F, kernelC, kernelH, kernelW = self.shape
        except:
            kernelH, kernelW = self.shape
        return np.random.randn(*self.shape) * np.sqrt(2/kernelH)

    def xavier_initializer(self):
        &#39;&#39;&#39;
        shape: Shape of the Kernel matrix.
        &#39;&#39;&#39;
        if self.seed is not None:
            np.random.seed(self.seed)
        try:
            F, kernelC, kernelH, kernelW = self.shape
        except:
            kernelH, kernelW = self.shape
        return np.random.randn(*self.shape) * np.sqrt(1/kernelH)

    def glorot_initializer(self):
        &#39;&#39;&#39;
        shape: Shape of the weight matrix.
        &#39;&#39;&#39;
        if self.seed is not None:
            np.random.seed(self.seed)
        try:
            F, kernelC, kernelH, kernelW = self.shape
        except:
            kernelH, kernelW = self.shape
        return np.random.randn(*self.shape) * np.sqrt(2/(kernelH+kernelW))

    def lecun_normal_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        return np.random.randn(*self.shape) * np.sqrt(1. / self.shape[1])

    def lecun_uniform_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        limit = np.sqrt(3. / self.shape[1])
        return np.random.uniform(-limit, limit, size=self.shape)

    def glorot_uniform_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        limit = np.sqrt(6. / (self.shape[0] + self.shape[1]))
        return np.random.uniform(-limit, limit, size=self.shape)
    
    def he_uniform_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        limit = np.sqrt(6. / self.shape[1])
        return np.random.uniform(-limit, limit, size=self.shape)

    def xavier_uniform_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        limit = np.sqrt(3. / (self.shape[0] + self.shape[1]))
        return np.random.uniform(-limit, limit, size=self.shape)
    
    def constant_initializer(self, constant=0):
        if self.seed is not None:
            np.random.seed(self.seed)
        return np.full(self.shape, constant)

    def orthogonal_initializer(self, gain=1.0):
        if self.seed is not None:
            np.random.seed(self.seed)
        flat_shape = (self.shape[0], np.prod(self.shape[1:]))
        a = np.random.normal(0.0, 1.0, flat_shape)
        u, _, v = np.linalg.svd(a, full_matrices=False)
        q = u if u.shape == flat_shape else v
        return gain * q.reshape(self.shape)
    
    def variance_scaling_initializer(self, scale=1.0, mode=&#39;fan_in&#39;, distribution=&#39;normal&#39;):
        if self.seed is not None:
            np.random.seed(self.seed)
        fan_in, fan_out = self.shape[0], self.shape[1]
        if mode == &#39;fan_avg&#39;:
            fan_avg = (fan_in + fan_out) / 2.0
            scale /= max(1., fan_avg)
        elif mode == &#39;fan_in&#39;:
            scale /= max(1., fan_in)
        elif mode == &#39;fan_out&#39;:
            scale /= max(1., fan_out)
        else:
            raise ValueError(&#39;Invalid mode for variance scaling initializer: %s.&#39; % mode)
        if distribution == &#39;normal&#39;:
            stddev = np.sqrt(scale)
            return np.random.normal(0., stddev, size=self.shape)
        elif distribution == &#39;uniform&#39;:
            limit = np.sqrt(3. * scale)
            return np.random.uniform(-limit, limit, size=self.shape)
        else:
            raise ValueError(&#39;Invalid distribution for variance scaling initializer: %s.&#39; % distribution)
    
    def truncated_normal_initializer(self, mean=0.0, stddev=0.05):
        if self.seed is not None:
            np.random.seed(self.seed)
        values = np.random.normal(loc=mean, scale=stddev, size=self.shape)
        return np.clip(values, mean - 2*stddev, mean + 2*stddev)

    def identity_initializer(self, gain=1.0):
        if self.seed is not None:
            np.random.seed(self.seed)
        if len(self.shape) != 2 or self.shape[0] != self.shape[1]:
            raise ValueError(&#39;Identity matrix initializer can only be used for 2D square matrices.&#39;)
        else:
            return np.eye(self.shape[0]) * gain
        
    def uniform_initializer(self, minval=0, maxval=None):
        if self.seed is not None:
            np.random.seed(self.seed)
        return np.random.uniform(minval, maxval, size=self.shape)
    
    def normal_initializer(self, mean=0.0, stddev=1.0):
        if self.seed is not None:
            np.random.seed(self.seed)
        return np.random.normal(loc=mean, scale=stddev, size=self.shape)
    
    def get_initializer(self, *args):
        match self.initializer_type.lower():
            case &#39;zeros&#39;: return self.zeros_initializer(*args)
            case &#39;ones&#39;: return self.ones_initializer(*args)
            case &#39;random_normal&#39;: return self.random_normal_initializer(*args)
            case &#39;random_uniform&#39;: return self.random_uniform_initializer(*args)
            case &#39;he_normal&#39;: return self.he_initializer(*args)
            case &#39;glorot_normal&#39;: return self.glorot_initializer(*args)
            case &#39;xavier_normal&#39;: return self.xavier_initializer()
            case &#39;lecun_normal&#39;: return self.lecun_normal_initializer(*args)
            case &#39;lecun_uniform&#39;: return self.lecun_uniform_initializer(*args)
            case &#39;glorot_uniform&#39;: return self.glorot_uniform_initializer(*args)
            case &#39;he_uniform&#39;: return self.he_uniform_initializer(*args)
            case &#39;xavier_uniform&#39;: return self.xavier_uniform_initializer(*args)
            case &#39;constant&#39;: return self.constant_initializer(*args)
            case &#39;orthogonal&#39;: return self.orthogonal_initializer(*args)
            case &#39;variance_scaling&#39;: return self.variance_scaling_initializer(*args)
            case &#39;truncated_normal&#39;: return self.truncated_normal_initializer(*args)
            case &#39;identity&#39;: return self.identity_initializer(*args)
            case &#39;uniform&#39;: return self.uniform_initializer(*args)
            case &#39;normal&#39;: return self.normal_initializer(*args)
            case _: raise ValueError(&#34;Valid initializer options are &#39;zeros&#39;, &#39;ones&#39;, &#39;random_normal&#39;, &#39;random_uniform&#39;, &#39;he_normal&#39;, &#39;xavier_normal&#39;, and &#39;glorot_normal&#39;, ...&#34;)
       </code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="Neural.Rectifier.recPicker"><code class="name flex">
<span>def <span class="ident">recPicker</span></span>(<span>name)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def recPicker(name):
    #Bezeichnungen welche f√ºr die Funktionen eingegeben werden koennen
    names = [[&#34;noderive&#34;,&#34;replacederive&#34;,&#34;noderiv&#34;],[&#34;sigmoid&#34;,&#34;sig&#34;], [&#34;bsigmoid&#34;, &#34;bipolaressigmoid&#34;], [&#34;tanh&#34;,&#34;tangenshyperbolicus&#34;], [&#34;lctanh&#34;, &#34;lecunstanh&#34;, &#34;lecunstangenshyperbolicus&#34;, &#34;lctangenshyperbolicus&#34;, &#34;lecun&#39;stangenshyperbolicus&#34;, &#34;lecun&#39;stanh&#34;], [&#34;htanh&#34;, &#34;hardtanh&#34;, &#34;htangenshyperbolicus&#34;, &#34;hardtangenshyperbolicus&#34;], [&#34;relu&#34;, &#34;rectifiedlinearunit&#34;, &#34;gleichrichter&#34;, &#34;maxfunktion&#34;, &#34;rampenfunktion&#34;], [&#34;lrelu&#34;, &#34;lrectifiedlinearunit&#34;, &#34;lgleichrichter&#34;, &#34;leakyrelu&#34;, &#34;leakyrectifiedlinearunit&#34;, &#34;leakygleichrichter&#34;, &#34;lmaxfunktion&#34;, &#34;lrampenfunktion&#34;, &#34;leakymaxfunktion&#34;, &#34;leakyrampenfunktion&#34;], [&#34;mcrelu&#34;, &#34;mcrectifiedlinearunit&#34;, &#34;mcgleichrichter&#34;, &#34;mcmaxfunktion&#34;, &#34;mcrampenfunktion&#34;, &#34;crelu&#34;, &#34;crectifiedlinearunit&#34;, &#34;cgleichrichter&#34;, &#34;cmaxfunktion&#34;, &#34;crampenfunktion&#34;, &#34;cosrelu&#34;, &#34;cosrectifiedlinearunit&#34;, &#34;cosgleichrichter&#34;, &#34;cosmaxfunktion&#34;, &#34;cosrampenfunktion&#34;, &#34;cosinusrelu&#34;, &#34;cosinusrectifiedlinearunit&#34;, &#34;cosinusgleichrichter&#34;, &#34;cosinusmaxfunktion&#34;, &#34;cosinusrampenfunktion&#34;], [&#34;sinrelu&#34;, &#34;sinrectifiedlinearunit&#34;, &#34;singleichrichter&#34;, &#34;sinmaxfunktion&#34;, &#34;sinrampenfunktion&#34;, &#34;sinusrelu&#34;, &#34;sinusrectifiedlinearunit&#34;, &#34;sinusgleichrichter&#34;, &#34;sinusmaxfunktion&#34;, &#34;sinusrampenfunktion&#34;], [&#34;srelu&#34;, &#34;srectifiedlinearunit&#34;, &#34;sgleichrichter&#34;, &#34;smaxfunktion&#34;, &#34;srampenfunktion&#34;, &#34;smoothrelu&#34;, &#34;smoothrectifiedlinearunit&#34;, &#34;smoothgleichrichter&#34;, &#34;smoothmaxfunktion&#34;, &#34;smoothrampenfunktion&#34;], [&#34;linear&#34;, &#34;lin&#34;], [&#34;pwlinear&#34;,&#34;piecewiselinear&#34;, &#34;pwlin&#34;], [&#34;step&#34;, &#34;schritt&#34;], [&#34;abs&#34;,&#34;absolute&#34;, &#34;absolut&#34;],[&#34;softmax&#34;,&#34;normalisiertesexpotential&#34;, &#34;normalizeexpotential&#34;],[&#34;kll&#34;, &#34;komplement√§resloglog&#34;, &#34;komplement√§reslog-log&#34;,&#34;klog-log&#34;,&#34;kloglog&#34;], [&#34;logit&#34;], [&#34;c&#34;,&#34;cos&#34;,&#34;cosinus&#34;], [&#34;s&#34;,&#34;sin&#34;, &#34;sinus&#34;], [&#34;gaussche&#34;,&#34;gauss&#34;], [&#34;normal&#34;, &#34;dichtefunktion&#34;,&#34;normalfunktion&#34;], [&#34;multiquatratisch&#34;], [&#34;imultiquatratisch&#34;, &#34;inversemultiquatratisch&#34;]]
    #Bezeichungen der Funktionen in diesem Script
    functions = [NoDeriv, Sigmoid, BSigmoid, Tanh, LCTanh, HTanh, ReLU, LReLU, CosReLU, SinReLU, SReLU, Linear, PwLinear, Step, Abs, Softmax, KLL, Logit, Cosinus, Sinus, Gaussche, Normal, Multiquatratisch, IMultiquatratisch]
    
    #in der Liste nach passender Funktion suchen und diese zurueckgeben
    for i in range(len(names)):
        if name.lower() in names[i]:
            return functions[i]()
    return False</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="Neural.Rectifier.Abs"><code class="flex name class">
<span>class <span class="ident">Abs</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Abs(Function):
    def func(self, x):
        if x&lt;0: return -x
        return np.abs(x)
    
    def deriv(self, x):
        return np.ones(x.shape)
    
    def intderiv(self, x):
        return 1</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Abs.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self, x):
    return np.ones(x.shape)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Abs.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, x):
    if x&lt;0: return -x
    return np.abs(x)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Abs.intderiv"><code class="name flex">
<span>def <span class="ident">intderiv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def intderiv(self, x):
    return 1</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.Activation"><code class="flex name class">
<span>class <span class="ident">Activation</span></span>
<span>(</span><span>activation_type='linear')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Activation:
    def __init__(self, activation_type= &#34;linear&#34;):
        self.activation_type = activation_type
        self.funcObj = recPicker(activation_type)
        self.forw = self.funcObj.func
        self.backw = self.funcObj.deriv
    
    def forward(self, X):
        self.X = X
        z = self.forw(X)
        return z
    
    def backpropagation(self, dz):
        f_prime = self.backw(self.X)
        if self.activation_type==&#39;softmax&#39;:
            # because derivative of softmax is a tensor
            dx = np.einsum(&#39;ijk,ik-&gt;ij&#39;, f_prime, dz)
        else:
            dx = dz * f_prime
        return dx</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Activation.backpropagation"><code class="name flex">
<span>def <span class="ident">backpropagation</span></span>(<span>self, dz)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backpropagation(self, dz):
    f_prime = self.backw(self.X)
    if self.activation_type==&#39;softmax&#39;:
        # because derivative of softmax is a tensor
        dx = np.einsum(&#39;ijk,ik-&gt;ij&#39;, f_prime, dz)
    else:
        dx = dz * f_prime
    return dx</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Activation.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X):
    self.X = X
    z = self.forw(X)
    return z</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.BSigmoid"><code class="flex name class">
<span>class <span class="ident">BSigmoid</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class BSigmoid(Function): #Bipolares Sigmoid f(x) = (1 - e^(-x)) / (1 + e^(-x))
    def func(self,x):
        return (1 - np.exp(-x)) / (1 + np.exp(-x))
    
    def deriv(self,x):
        return 1 / (2 * (0.5 * (np.exp(0.5*x) + np.exp(-0.5*x)))**2)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.BSigmoid.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self,x):
    return 1 / (2 * (0.5 * (np.exp(0.5*x) + np.exp(-0.5*x)))**2)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.BSigmoid.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self,x):
    return (1 - np.exp(-x)) / (1 + np.exp(-x))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.CosReLU"><code class="flex name class">
<span>class <span class="ident">CosReLU</span></span>
<span>(</span><span>wende=0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CosReLU(Function): #1. Modifikation mit cos von Relu gut f√ºr mnist f(x) = max(0,x) + cos(x)
    def __init__(self,wende=0):
        self.w = wende #Wendepunkt
    
    def func(self, x):
        return x*(x&gt;0) + np.cos(x)

    def deriv(self, x):
        return np.where(x&gt;0, 1-np.sin(x), -np.sin(x))
    
    def intderiv(self,x):
        if x&gt;0: return   1 - np.sin(x)
        return -np.sin(x)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.CosReLU.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self, x):
    return np.where(x&gt;0, 1-np.sin(x), -np.sin(x))</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.CosReLU.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, x):
    return x*(x&gt;0) + np.cos(x)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.CosReLU.intderiv"><code class="name flex">
<span>def <span class="ident">intderiv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def intderiv(self,x):
    if x&gt;0: return   1 - np.sin(x)
    return -np.sin(x)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.Cosinus"><code class="flex name class">
<span>class <span class="ident">Cosinus</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Cosinus(Function):
    def func(self,x):
        return np.cos(x)
    def deriv(self,x): 
        return -np.sin(x)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Cosinus.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self,x): 
    return -np.sin(x)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Cosinus.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self,x):
    return np.cos(x)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.Cost"><code class="flex name class">
<span>class <span class="ident">Cost</span></span>
<span>(</span><span>cost_type='mse')</span>
</code></dt>
<dd>
<div class="desc"><p>Parameters</p>
<p>cost_type: type of cost function
available options are 'mse', and 'cross-entropy'</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Cost:
    
    names = [&#39;mse&#39;, &#39;cross-entropy&#39;, &#39;weighted&#39;, &#39;log&#39;, &#39;kullback-leibler&#39;, &#39;exp&#39;, &#39;hinge&#39;, &#39;huber&#39;] #possible cost functions to choose from

    def __init__(self, cost_type=&#39;mse&#39;):
        &#39;&#39;&#39;
        Parameters

        cost_type: type of cost function
        available options are &#39;mse&#39;, and &#39;cross-entropy&#39;
        &#39;&#39;&#39;
        self.cost_type = cost_type

    def mse(self, a, y):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        return (1/2)*np.sum((np.linalg.norm(a-y, axis=1))**2)

    def d_mse(self, a, y):
        &#39;&#39;&#39;
        represents dJ/da

        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        return a - y

    def cross_entropy(self, a, y, epsilon=1e-12):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        a = np.clip(a, epsilon, 1. - epsilon)
        return -np.sum(y*np.log(a))

    def d_cross_entropy(self, a, y, epsilon=1e-12):
        &#39;&#39;&#39;
        represents dJ/da

        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        a = np.clip(a, epsilon, 1. - epsilon)
        return -y/a
    
    def weighted(self, a, y, c):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        c: Weight factor array of shape (batch, 1)
        &#39;&#39;&#39;
        return c * (a - y)**2

    def d_weighted(self, a, y, c):
        &#39;&#39;&#39;
        represents dJ/da

        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        c: Weight factor array of shape (batch, 1)
        &#39;&#39;&#39;
        return 2 * c * (a - y)

    def log(self, a, y, epsilon=1e-12):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        a = np.clip(a, epsilon, 1. - epsilon)
        return -y * np.log(a) - (1 - y) * np.log(1 - a)

    def d_log(self, a, y, epsilon=1e-12):
        &#39;&#39;&#39;
        represents dJ/da

        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        a = np.clip(a, epsilon, 1. - epsilon)
        return -y / a + (1 - y) / (1 - a)

    def kl(self, a, y, epsilon=1e-12):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        a = np.clip(a, epsilon, 1. - epsilon)
        return np.sum(y * np.log(y / a), axis=1)

    def d_kl(self, a, y, epsilon=1e-12):
        &#39;&#39;&#39;
        represents dJ/da

        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        a = np.clip(a, epsilon, 1. - epsilon)
        return -y / a

    def exp(self, a, y):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        return np.exp(-y * a)

    def d_exp(self, a, y):
        &#39;&#39;&#39;
        represents dJ/da

        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        return -y * np.exp(-y * a)

    def hinge(self, a, y):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        return np.maximum(0, 1 - y * a)

    def d_hinge(self, a, y):
        &#39;&#39;&#39;
        represents dJ/da

        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        return np.where(y * a &gt;= 1, 0, -y)

    def huber(self, a, y, delta):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        delta: Hyperparameter for sensitivity to outliers
        &#39;&#39;&#39;
        error = a - y
        return np.where(np.abs(error) &lt;= delta, 0.5 * error**2, delta * (np.abs(error) - 0.5 * delta))

    def d_huber(self, a, y, delta):
        &#39;&#39;&#39;
        represents dJ/da

        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        delta: Hyperparameter for sensitivity to outliers
        &#39;&#39;&#39;
        error = a - y
        return np.where(np.abs(error) &lt;= delta, error, delta * np.sign(error))


    def get_cost(self, a, y, *args):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        match self.cost_type:
            case &#39;mse&#39;: return self.mse(a, y)
            case &#39;cross-entropy&#39;: return self.cross_entropy(a, y)
            case &#39;weighted&#39;: return self.weighted(a, y, *args)
            case &#39;log&#39;: return self.log(a, y, *args)
            case &#39;kullback-leibler&#39;: return self.kl(a, y, *args)
            case &#39;exp&#39;: return self.exp(a, y)
            case &#39;hinge&#39;: return self.hinge(a, y)
            case &#39;huber&#39;: return self.huber(a, y, *args)
            case _:   raise ValueError(&#34;Valid cost functions are only: &#34;+ &#34;, &#34;.join(self.names))

    def get_d_cost(self, a, y, *args):
        &#39;&#39;&#39;
        Parameters

        a: Predicted output array of shape (batch, d)
        y: Actual output array of shape (batch, d)
        &#39;&#39;&#39;
        match self.cost_type:
            case &#39;mse&#39;: return self.d_mse(a, y)
            case &#39;cross-entropy&#39;: return self.d_cross_entropy(a, y)
            case &#39;weighted&#39;: return self.d_weighted(a, y, *args)
            case &#39;log&#39;: return self.d_log(a, y, *args)
            case &#39;kullback-leibler&#39;: return self.d_kl(a, y, *args)
            case &#39;exp&#39;: return self.d_exp(a, y)
            case &#39;hinge&#39;: return self.d_hinge(a, y)
            case &#39;huber&#39;: return self.d_huber(a, y, *args)
            case _:   raise ValueError(&#34;Valid cost functions are only: &#34;+ &#34;, &#34;.join(self.names))</code></pre>
</details>
<h3>Class variables</h3>
<dl>
<dt id="Neural.Rectifier.Cost.names"><code class="name">var <span class="ident">names</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Cost.cross_entropy"><code class="name flex">
<span>def <span class="ident">cross_entropy</span></span>(<span>self, a, y, epsilon=1e-12)</span>
</code></dt>
<dd>
<div class="desc"><p>Parameters</p>
<p>a: Predicted output array of shape (batch, d)
y: Actual output array of shape (batch, d)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cross_entropy(self, a, y, epsilon=1e-12):
    &#39;&#39;&#39;
    Parameters

    a: Predicted output array of shape (batch, d)
    y: Actual output array of shape (batch, d)
    &#39;&#39;&#39;
    a = np.clip(a, epsilon, 1. - epsilon)
    return -np.sum(y*np.log(a))</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Cost.d_cross_entropy"><code class="name flex">
<span>def <span class="ident">d_cross_entropy</span></span>(<span>self, a, y, epsilon=1e-12)</span>
</code></dt>
<dd>
<div class="desc"><p>represents dJ/da</p>
<p>Parameters</p>
<p>a: Predicted output array of shape (batch, d)
y: Actual output array of shape (batch, d)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def d_cross_entropy(self, a, y, epsilon=1e-12):
    &#39;&#39;&#39;
    represents dJ/da

    Parameters

    a: Predicted output array of shape (batch, d)
    y: Actual output array of shape (batch, d)
    &#39;&#39;&#39;
    a = np.clip(a, epsilon, 1. - epsilon)
    return -y/a</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Cost.d_exp"><code class="name flex">
<span>def <span class="ident">d_exp</span></span>(<span>self, a, y)</span>
</code></dt>
<dd>
<div class="desc"><p>represents dJ/da</p>
<p>Parameters</p>
<p>a: Predicted output array of shape (batch, d)
y: Actual output array of shape (batch, d)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def d_exp(self, a, y):
    &#39;&#39;&#39;
    represents dJ/da

    Parameters

    a: Predicted output array of shape (batch, d)
    y: Actual output array of shape (batch, d)
    &#39;&#39;&#39;
    return -y * np.exp(-y * a)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Cost.d_hinge"><code class="name flex">
<span>def <span class="ident">d_hinge</span></span>(<span>self, a, y)</span>
</code></dt>
<dd>
<div class="desc"><p>represents dJ/da</p>
<p>Parameters</p>
<p>a: Predicted output array of shape (batch, d)
y: Actual output array of shape (batch, d)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def d_hinge(self, a, y):
    &#39;&#39;&#39;
    represents dJ/da

    Parameters

    a: Predicted output array of shape (batch, d)
    y: Actual output array of shape (batch, d)
    &#39;&#39;&#39;
    return np.where(y * a &gt;= 1, 0, -y)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Cost.d_huber"><code class="name flex">
<span>def <span class="ident">d_huber</span></span>(<span>self, a, y, delta)</span>
</code></dt>
<dd>
<div class="desc"><p>represents dJ/da</p>
<p>Parameters</p>
<p>a: Predicted output array of shape (batch, d)
y: Actual output array of shape (batch, d)
delta: Hyperparameter for sensitivity to outliers</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def d_huber(self, a, y, delta):
    &#39;&#39;&#39;
    represents dJ/da

    Parameters

    a: Predicted output array of shape (batch, d)
    y: Actual output array of shape (batch, d)
    delta: Hyperparameter for sensitivity to outliers
    &#39;&#39;&#39;
    error = a - y
    return np.where(np.abs(error) &lt;= delta, error, delta * np.sign(error))</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Cost.d_kl"><code class="name flex">
<span>def <span class="ident">d_kl</span></span>(<span>self, a, y, epsilon=1e-12)</span>
</code></dt>
<dd>
<div class="desc"><p>represents dJ/da</p>
<p>Parameters</p>
<p>a: Predicted output array of shape (batch, d)
y: Actual output array of shape (batch, d)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def d_kl(self, a, y, epsilon=1e-12):
    &#39;&#39;&#39;
    represents dJ/da

    Parameters

    a: Predicted output array of shape (batch, d)
    y: Actual output array of shape (batch, d)
    &#39;&#39;&#39;
    a = np.clip(a, epsilon, 1. - epsilon)
    return -y / a</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Cost.d_log"><code class="name flex">
<span>def <span class="ident">d_log</span></span>(<span>self, a, y, epsilon=1e-12)</span>
</code></dt>
<dd>
<div class="desc"><p>represents dJ/da</p>
<p>Parameters</p>
<p>a: Predicted output array of shape (batch, d)
y: Actual output array of shape (batch, d)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def d_log(self, a, y, epsilon=1e-12):
    &#39;&#39;&#39;
    represents dJ/da

    Parameters

    a: Predicted output array of shape (batch, d)
    y: Actual output array of shape (batch, d)
    &#39;&#39;&#39;
    a = np.clip(a, epsilon, 1. - epsilon)
    return -y / a + (1 - y) / (1 - a)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Cost.d_mse"><code class="name flex">
<span>def <span class="ident">d_mse</span></span>(<span>self, a, y)</span>
</code></dt>
<dd>
<div class="desc"><p>represents dJ/da</p>
<p>Parameters</p>
<p>a: Predicted output array of shape (batch, d)
y: Actual output array of shape (batch, d)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def d_mse(self, a, y):
    &#39;&#39;&#39;
    represents dJ/da

    Parameters

    a: Predicted output array of shape (batch, d)
    y: Actual output array of shape (batch, d)
    &#39;&#39;&#39;
    return a - y</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Cost.d_weighted"><code class="name flex">
<span>def <span class="ident">d_weighted</span></span>(<span>self, a, y, c)</span>
</code></dt>
<dd>
<div class="desc"><p>represents dJ/da</p>
<p>Parameters</p>
<p>a: Predicted output array of shape (batch, d)
y: Actual output array of shape (batch, d)
c: Weight factor array of shape (batch, 1)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def d_weighted(self, a, y, c):
    &#39;&#39;&#39;
    represents dJ/da

    Parameters

    a: Predicted output array of shape (batch, d)
    y: Actual output array of shape (batch, d)
    c: Weight factor array of shape (batch, 1)
    &#39;&#39;&#39;
    return 2 * c * (a - y)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Cost.exp"><code class="name flex">
<span>def <span class="ident">exp</span></span>(<span>self, a, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Parameters</p>
<p>a: Predicted output array of shape (batch, d)
y: Actual output array of shape (batch, d)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def exp(self, a, y):
    &#39;&#39;&#39;
    Parameters

    a: Predicted output array of shape (batch, d)
    y: Actual output array of shape (batch, d)
    &#39;&#39;&#39;
    return np.exp(-y * a)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Cost.get_cost"><code class="name flex">
<span>def <span class="ident">get_cost</span></span>(<span>self, a, y, *args)</span>
</code></dt>
<dd>
<div class="desc"><p>Parameters</p>
<p>a: Predicted output array of shape (batch, d)
y: Actual output array of shape (batch, d)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_cost(self, a, y, *args):
    &#39;&#39;&#39;
    Parameters

    a: Predicted output array of shape (batch, d)
    y: Actual output array of shape (batch, d)
    &#39;&#39;&#39;
    match self.cost_type:
        case &#39;mse&#39;: return self.mse(a, y)
        case &#39;cross-entropy&#39;: return self.cross_entropy(a, y)
        case &#39;weighted&#39;: return self.weighted(a, y, *args)
        case &#39;log&#39;: return self.log(a, y, *args)
        case &#39;kullback-leibler&#39;: return self.kl(a, y, *args)
        case &#39;exp&#39;: return self.exp(a, y)
        case &#39;hinge&#39;: return self.hinge(a, y)
        case &#39;huber&#39;: return self.huber(a, y, *args)
        case _:   raise ValueError(&#34;Valid cost functions are only: &#34;+ &#34;, &#34;.join(self.names))</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Cost.get_d_cost"><code class="name flex">
<span>def <span class="ident">get_d_cost</span></span>(<span>self, a, y, *args)</span>
</code></dt>
<dd>
<div class="desc"><p>Parameters</p>
<p>a: Predicted output array of shape (batch, d)
y: Actual output array of shape (batch, d)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_d_cost(self, a, y, *args):
    &#39;&#39;&#39;
    Parameters

    a: Predicted output array of shape (batch, d)
    y: Actual output array of shape (batch, d)
    &#39;&#39;&#39;
    match self.cost_type:
        case &#39;mse&#39;: return self.d_mse(a, y)
        case &#39;cross-entropy&#39;: return self.d_cross_entropy(a, y)
        case &#39;weighted&#39;: return self.d_weighted(a, y, *args)
        case &#39;log&#39;: return self.d_log(a, y, *args)
        case &#39;kullback-leibler&#39;: return self.d_kl(a, y, *args)
        case &#39;exp&#39;: return self.d_exp(a, y)
        case &#39;hinge&#39;: return self.d_hinge(a, y)
        case &#39;huber&#39;: return self.d_huber(a, y, *args)
        case _:   raise ValueError(&#34;Valid cost functions are only: &#34;+ &#34;, &#34;.join(self.names))</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Cost.hinge"><code class="name flex">
<span>def <span class="ident">hinge</span></span>(<span>self, a, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Parameters</p>
<p>a: Predicted output array of shape (batch, d)
y: Actual output array of shape (batch, d)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def hinge(self, a, y):
    &#39;&#39;&#39;
    Parameters

    a: Predicted output array of shape (batch, d)
    y: Actual output array of shape (batch, d)
    &#39;&#39;&#39;
    return np.maximum(0, 1 - y * a)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Cost.huber"><code class="name flex">
<span>def <span class="ident">huber</span></span>(<span>self, a, y, delta)</span>
</code></dt>
<dd>
<div class="desc"><p>Parameters</p>
<p>a: Predicted output array of shape (batch, d)
y: Actual output array of shape (batch, d)
delta: Hyperparameter for sensitivity to outliers</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def huber(self, a, y, delta):
    &#39;&#39;&#39;
    Parameters

    a: Predicted output array of shape (batch, d)
    y: Actual output array of shape (batch, d)
    delta: Hyperparameter for sensitivity to outliers
    &#39;&#39;&#39;
    error = a - y
    return np.where(np.abs(error) &lt;= delta, 0.5 * error**2, delta * (np.abs(error) - 0.5 * delta))</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Cost.kl"><code class="name flex">
<span>def <span class="ident">kl</span></span>(<span>self, a, y, epsilon=1e-12)</span>
</code></dt>
<dd>
<div class="desc"><p>Parameters</p>
<p>a: Predicted output array of shape (batch, d)
y: Actual output array of shape (batch, d)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def kl(self, a, y, epsilon=1e-12):
    &#39;&#39;&#39;
    Parameters

    a: Predicted output array of shape (batch, d)
    y: Actual output array of shape (batch, d)
    &#39;&#39;&#39;
    a = np.clip(a, epsilon, 1. - epsilon)
    return np.sum(y * np.log(y / a), axis=1)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Cost.log"><code class="name flex">
<span>def <span class="ident">log</span></span>(<span>self, a, y, epsilon=1e-12)</span>
</code></dt>
<dd>
<div class="desc"><p>Parameters</p>
<p>a: Predicted output array of shape (batch, d)
y: Actual output array of shape (batch, d)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def log(self, a, y, epsilon=1e-12):
    &#39;&#39;&#39;
    Parameters

    a: Predicted output array of shape (batch, d)
    y: Actual output array of shape (batch, d)
    &#39;&#39;&#39;
    a = np.clip(a, epsilon, 1. - epsilon)
    return -y * np.log(a) - (1 - y) * np.log(1 - a)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Cost.mse"><code class="name flex">
<span>def <span class="ident">mse</span></span>(<span>self, a, y)</span>
</code></dt>
<dd>
<div class="desc"><p>Parameters</p>
<p>a: Predicted output array of shape (batch, d)
y: Actual output array of shape (batch, d)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mse(self, a, y):
    &#39;&#39;&#39;
    Parameters

    a: Predicted output array of shape (batch, d)
    y: Actual output array of shape (batch, d)
    &#39;&#39;&#39;
    return (1/2)*np.sum((np.linalg.norm(a-y, axis=1))**2)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Cost.weighted"><code class="name flex">
<span>def <span class="ident">weighted</span></span>(<span>self, a, y, c)</span>
</code></dt>
<dd>
<div class="desc"><p>Parameters</p>
<p>a: Predicted output array of shape (batch, d)
y: Actual output array of shape (batch, d)
c: Weight factor array of shape (batch, 1)</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def weighted(self, a, y, c):
    &#39;&#39;&#39;
    Parameters

    a: Predicted output array of shape (batch, d)
    y: Actual output array of shape (batch, d)
    c: Weight factor array of shape (batch, 1)
    &#39;&#39;&#39;
    return c * (a - y)**2</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.Function"><code class="flex name class">
<span>class <span class="ident">Function</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Function():
    def __init__(self):
        pass
    
    def func(self, x):
        return x
    
    def deriv(self,x):
        return 1</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Abs" href="#Neural.Rectifier.Abs">Abs</a></li>
<li><a title="Neural.Rectifier.BSigmoid" href="#Neural.Rectifier.BSigmoid">BSigmoid</a></li>
<li><a title="Neural.Rectifier.CosReLU" href="#Neural.Rectifier.CosReLU">CosReLU</a></li>
<li><a title="Neural.Rectifier.Cosinus" href="#Neural.Rectifier.Cosinus">Cosinus</a></li>
<li><a title="Neural.Rectifier.Gaussche" href="#Neural.Rectifier.Gaussche">Gaussche</a></li>
<li><a title="Neural.Rectifier.HTanh" href="#Neural.Rectifier.HTanh">HTanh</a></li>
<li><a title="Neural.Rectifier.IMultiquatratisch" href="#Neural.Rectifier.IMultiquatratisch">IMultiquatratisch</a></li>
<li><a title="Neural.Rectifier.KLL" href="#Neural.Rectifier.KLL">KLL</a></li>
<li><a title="Neural.Rectifier.LCTanh" href="#Neural.Rectifier.LCTanh">LCTanh</a></li>
<li><a title="Neural.Rectifier.LReLU" href="#Neural.Rectifier.LReLU">LReLU</a></li>
<li><a title="Neural.Rectifier.Linear" href="#Neural.Rectifier.Linear">Linear</a></li>
<li><a title="Neural.Rectifier.Linear2" href="#Neural.Rectifier.Linear2">Linear2</a></li>
<li><a title="Neural.Rectifier.Logit" href="#Neural.Rectifier.Logit">Logit</a></li>
<li><a title="Neural.Rectifier.Multiquatratisch" href="#Neural.Rectifier.Multiquatratisch">Multiquatratisch</a></li>
<li><a title="Neural.Rectifier.NoDeriv" href="#Neural.Rectifier.NoDeriv">NoDeriv</a></li>
<li><a title="Neural.Rectifier.Normal" href="#Neural.Rectifier.Normal">Normal</a></li>
<li><a title="Neural.Rectifier.PwLinear" href="#Neural.Rectifier.PwLinear">PwLinear</a></li>
<li><a title="Neural.Rectifier.ReLU" href="#Neural.Rectifier.ReLU">ReLU</a></li>
<li><a title="Neural.Rectifier.SReLU" href="#Neural.Rectifier.SReLU">SReLU</a></li>
<li><a title="Neural.Rectifier.Sigmoid" href="#Neural.Rectifier.Sigmoid">Sigmoid</a></li>
<li><a title="Neural.Rectifier.SinReLU" href="#Neural.Rectifier.SinReLU">SinReLU</a></li>
<li><a title="Neural.Rectifier.Sinus" href="#Neural.Rectifier.Sinus">Sinus</a></li>
<li><a title="Neural.Rectifier.Softmax" href="#Neural.Rectifier.Softmax">Softmax</a></li>
<li><a title="Neural.Rectifier.Step" href="#Neural.Rectifier.Step">Step</a></li>
<li><a title="Neural.Rectifier.Tanh" href="#Neural.Rectifier.Tanh">Tanh</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Function.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self,x):
    return 1</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Function.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, x):
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.Gaussche"><code class="flex name class">
<span>class <span class="ident">Gaussche</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Gaussche(Function):
    def func(self, x):
        return np.exp(-0.5 * (x**2))
    
    def deriv(self,x):
        return -x * np.exp(-0.5 * x**2)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Gaussche.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self,x):
    return -x * np.exp(-0.5 * x**2)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Gaussche.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, x):
    return np.exp(-0.5 * (x**2))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.HTanh"><code class="flex name class">
<span>class <span class="ident">HTanh</span></span>
<span>(</span><span>start=-1, end=1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class HTanh(Function): #Hard Tanh f(x) = max(-1, min(1,x))
    def __init__(self,start=-1, end= 1):
        self.s = start
        self.e = end
    
    def func(self, x):
        min = np.where(x&gt;self.e, self.e)
        return np.where(self.s&lt; min, self.s)
    
    def intfunc(self, x):
        return max(self.s, min(x, self.e))
    
    def deriv(self, x):
        #kx = np.where(-1&lt;x&lt;1, 1)
        #return np.where(kx&lt;=-1 or kx&gt;=1, 0)
        return x * (-1 &lt; x &lt; 1)
        
    def intderiv(self,x):
        if -1 &lt; x &lt; 1:return 1
        else: return 0</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.HTanh.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self, x):
    #kx = np.where(-1&lt;x&lt;1, 1)
    #return np.where(kx&lt;=-1 or kx&gt;=1, 0)
    return x * (-1 &lt; x &lt; 1)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.HTanh.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, x):
    min = np.where(x&gt;self.e, self.e)
    return np.where(self.s&lt; min, self.s)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.HTanh.intderiv"><code class="name flex">
<span>def <span class="ident">intderiv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def intderiv(self,x):
    if -1 &lt; x &lt; 1:return 1
    else: return 0</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.HTanh.intfunc"><code class="name flex">
<span>def <span class="ident">intfunc</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def intfunc(self, x):
    return max(self.s, min(x, self.e))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.IMultiquatratisch"><code class="flex name class">
<span>class <span class="ident">IMultiquatratisch</span></span>
<span>(</span><span>dot)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class IMultiquatratisch(Function):#Inverse multiquatratisch
    def __init__(self,dot):
        self.x = dot[0]
        self.y = dot[1]
    
    def func(self, x):
        return 1 / np.sqrt((x - self.x)**2 + self.y**2)
    
    def deriv(self, x):
        return -(x - self.x)  /  (np.sqrt(x**2 - 2*x*self.x + self.y**2 + self.x**2) ** (3/2))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.IMultiquatratisch.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self, x):
    return -(x - self.x)  /  (np.sqrt(x**2 - 2*x*self.x + self.y**2 + self.x**2) ** (3/2))</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.IMultiquatratisch.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, x):
    return 1 / np.sqrt((x - self.x)**2 + self.y**2)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.KLL"><code class="flex name class">
<span>class <span class="ident">KLL</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class KLL(Function): #komplement√§res Log-Log f(x) = 1 - e^(-e^x)
    def func(self,x):
        return  1 - np.exp(-np.exp(x))
    
    def deriv(self,x):
        return np.exp(-np.exp(x))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.KLL.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self,x):
    return np.exp(-np.exp(x))</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.KLL.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self,x):
    return  1 - np.exp(-np.exp(x))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.LCTanh"><code class="flex name class">
<span>class <span class="ident">LCTanh</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LCTanh(Function): #LeCuns Tanh f(x) = 1.7159 * tanh((2/3) * x)
    def __init__(self):
        self.tanh = Tanh()
        
    def func(self,x):
        return 1.7159 * self.tanh.func((2/3)*x)
    
    def deriv(self,x):
        return 1.7159 * self.tanh.deriv((2/3)*x) * (2/3)   </code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.LCTanh.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self,x):
    return 1.7159 * self.tanh.deriv((2/3)*x) * (2/3)   </code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LCTanh.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self,x):
    return 1.7159 * self.tanh.func((2/3)*x)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.LReLU"><code class="flex name class">
<span>class <span class="ident">LReLU</span></span>
<span>(</span><span>a=0.2)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LReLU(Function): #Leaky ReLU
    def __init__(self, a = 0.2):
        self.a = a #slope Parameter
    
    def func(self, x, alpha = None):
        if alpha is None: alpha = self.a
        return np.where(x&gt;0, x, alpha*x)
        
    def deriv(self, x, alpha):
        if alpha is None: alpha = self.a
        return np.where(x&gt;0, 1, alpha)
    
    def intfunc(self, x, alpha = None):
        if alpha is None: alpha = self.a
        if x &gt; 0: return x
        return alpha*x
    
    def intderiv(self, x, alpha = None):
        if alpha is None: alpha = self.a
        if x &gt; 0: return 1
        return alpha</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.LReLU.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x, alpha)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self, x, alpha):
    if alpha is None: alpha = self.a
    return np.where(x&gt;0, 1, alpha)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LReLU.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x, alpha=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, x, alpha = None):
    if alpha is None: alpha = self.a
    return np.where(x&gt;0, x, alpha*x)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LReLU.intderiv"><code class="name flex">
<span>def <span class="ident">intderiv</span></span>(<span>self, x, alpha=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def intderiv(self, x, alpha = None):
    if alpha is None: alpha = self.a
    if x &gt; 0: return 1
    return alpha</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LReLU.intfunc"><code class="name flex">
<span>def <span class="ident">intfunc</span></span>(<span>self, x, alpha=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def intfunc(self, x, alpha = None):
    if alpha is None: alpha = self.a
    if x &gt; 0: return x
    return alpha*x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay"><code class="flex name class">
<span>class <span class="ident">LearningRateDecay</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class LearningRateDecay:

    def __init__(self):
        pass

    def constant(self, t, learnrate_0):
        &#39;&#39;&#39;
        t: iteration
        learnrate_0: initial learning rate
        &#39;&#39;&#39;
        return learnrate_0

    def time_decay(self, t, learnrate_0, k):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        k: Decay rate
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 /(1+(k*t))
        return learnrate

    def step_decay(self, t, learnrate_0, F, D):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        F: factor value controlling the rate in which the learning date drops
        D: ‚ÄúDrop every‚Äù iteration
        t: current iteration
        &#39;&#39;&#39;
        mult = F**np.floor((1+t)/D)
        learnrate = learnrate_0 * mult
        return learnrate

    def exponential_decay(self, t, learnrate_0, k):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        k: Exponential Decay rate
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 * np.exp(-k*t)
        return learnrate
    
    def inverse_time_decay(self, t, learnrate_0, k):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        k: Decay rate
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 / (1 + k * t)
        return learnrate

    def natural_exp_decay(self, t, learnrate_0, k):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        k: Decay rate
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 * np.exp(-k * t)
        return learnrate

    def piecewise_constant_decay(self, t, learnrate_0, boundaries, values):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        t: iteration number
        boundaries: list of iteration numbers at which the learning rate changes
        values: list of learning rates corresponding to the intervals defined by boundaries
        &#39;&#39;&#39;
        for b, v in zip(boundaries, values):
            if t &lt; b:
                return v
        return values[-1]
    
    def polynomial_decay(self, t, learnrate_0, power=1):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        power: exponent of the polynomial
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 / (1 + t)**power
        return learnrate

    def cosine_decay(self, t, learnrate_0, T_max):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        T_max: maximum number of iterations
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 * 0.5 * (1 + np.cos(np.pi * t / T_max))
        return learnrate

    def linear_cosine_decay(self, t, learnrate_0, T_max):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        T_max: maximum number of iterations
        t: iteration number
        &#39;&#39;&#39;
        alpha = t / T_max
        learnrate = (1 - alpha) * learnrate_0 * 0.5 * (1 + np.cos(np.pi * t / T_max)) + alpha * learnrate_0
        return learnrate
    
    def warmup_decay(self, t, learnrate_0, warmup_steps=10):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        warmup_steps: number of initial iterations for which to use linearly increasing learning rate
        t: iteration number
        &#39;&#39;&#39;
        if t &lt; warmup_steps:
            return learnrate_0 * (t / warmup_steps)
        else:
            return learnrate_0

    def cyclic_decay(self, t, learnrate_0, step_size, max_lr, mode=&#39;triangular&#39;):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        step_size: number of iterations in one half cycle
        max_lr: maximum learning rate
        mode: type of cyclic decay (&#39;triangular&#39;, &#39;triangular2&#39;, &#39;exp_range&#39;)
        t: iteration number
        &#39;&#39;&#39;
        cycle = np.floor(1 + t / (2 * step_size))
        x = np.abs(t / step_size - 2 * cycle + 1)
        if mode == &#39;triangular&#39;:
            lr = learnrate_0 + (max_lr - learnrate_0) * np.maximum(0, (1 - x))
        elif mode == &#39;triangular2&#39;:
            lr = learnrate_0 + (max_lr - learnrate_0) * np.maximum(0, (1 - x)) / (2 ** (cycle - 1))
        elif mode == &#39;exp_range&#39;:
            lr = learnrate_0 + (max_lr - learnrate_0) * np.maximum(0, (1 - x)) * (0.99999 ** t)
        return lr

    def sqrt_decay(self, t, learnrate_0, k=1):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        k: Decay rate
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 / np.sqrt(k * t + 1)
        return learnrate

    def power_decay(self, t, learnrate_0, k=0.1, power=0.75):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        k: Decay rate
        power: power to which iteration number is raised
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 / (1 + k * t)**power
        return learnrate

    def staircase_decay(self, t, learnrate_0, drop_rate=0.5, epochs_drop=10.0):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        drop_rate: factor by which learning rate is reduced at each stage
        epochs_drop: number of epochs after which learning rate drops
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 * (drop_rate ** np.floor((1+t) / epochs_drop))
        return learnrate

    def cosine_annealing_decay(self, t, learnrate_0, T_max, M):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        T_max: maximum number of iterations
        M: number of cycles
        t: iteration number
        &#39;&#39;&#39;
        learnrate = learnrate_0 / 2 * (np.cos(np.pi * (t % (T_max // M)) / (T_max // M)) + 1)
        return learnrate

    def burnin_decay(self, t, learnrate_0, burnin, learnrate_burnin):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        burnin: number of initial iterations for which to use burnin learning rate
        learnrate_burnin: learning rate used during burnin period
        t: iteration number
        &#39;&#39;&#39;
        if t &lt; burnin:
            return learnrate_burnin
        else:
            return learnrate_0

    def warm_restart_decay(self, t, learnrate_0, T_0, T_mult):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        T_0: initial number of iterations
        T_mult: factor by which number of iterations increases after each restart
        t: iteration number
        &#39;&#39;&#39;
        T_i = T_0
        while t &gt;= T_i:
            t = t - T_i
            T_i = T_i * T_mult
        learnrate = learnrate_0 * 0.5 * (1 + np.cos(np.pi * t / T_i))
        return learnrate
    
    def decay_on_plateau(self, t, learnrate_0, min_lr, factor=0.1, patience=10, cooldown=0):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        min_lr: minimum learning rate
        factor: factor by which the learning rate is reduced
        patience: number of epochs with no improvement after which learning rate will be reduced
        cooldown: number of epochs to wait before resuming normal operation after lr has been reduced
        t: iteration number
        &#39;&#39;&#39;
        if t &lt; cooldown:
            return learnrate_0
        elif t % patience == 0:
            return max(learnrate_0 * factor, min_lr)
        else:
            return learnrate_0

    def one_cycle_policy(self, t, learnrate_max, stepsize, base_lr=None):
        &#39;&#39;&#39;
        learnrate_max: maximum learning rate
        stepsize: number of iterations in one half cycle
        base_lr: initial learning rate
        t: iteration number
        &#39;&#39;&#39;
        if base_lr is None:
            base_lr = learnrate_max / 10
        if t &lt;= stepsize:
            return base_lr + (learnrate_max - base_lr) * t / stepsize
        elif t &lt;= 2 * stepsize:
            return learnrate_max - (learnrate_max - base_lr) * (t - stepsize) / stepsize
        else:
            return base_lr
    
    def cosine_decay_restarts(self, t, learnrate_0, T_0, T_mult=2):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        T_0: initial number of iterations
        T_mult: factor by which number of iterations increases after each restart
        t: iteration number
        &#39;&#39;&#39;
        T_i = T_0
        while t &gt;= T_i:
            t = t - T_i
            T_i = T_i * T_mult
        learnrate = learnrate_0 * 0.5 * (1 + np.cos(np.pi * t / T_i))
        return learnrate

    def triangular_learning_rate(self, t, learnrate_low, learnrate_high, T):
        &#39;&#39;&#39;
        learnrate_low: lower bound for learning rate
        learnrate_high: upper bound for learning rate
        T: total number of iterations
        t: iteration number
        &#39;&#39;&#39;
        cycle = np.floor(1 + t / (2 * T))
        x = np.abs(t / T - 2 * cycle + 1)
        learnrate = learnrate_low + (learnrate_high - learnrate_low) * np.maximum(0, (1 - x))
        return learnrate

    def triangular2_learning_rate(self, t, learnrate_low, learnrate_high, T):
        &#39;&#39;&#39;
        learnrate_low: lower bound for learning rate
        learnrate_high: upper bound for learning rate
        T: total number of iterations
        t: iteration number
        &#39;&#39;&#39;
        cycle = np.floor(1 + t / (2 * T))
        x = np.abs(t / T - 2 * cycle + 1)
        learnrate = learnrate_low + (learnrate_high - learnrate_low) * np.maximum(0, (1 - x)) / (2 ** (cycle - 1))
        return learnrate
    
    def exp_range_decay(self, t, learnrate_0, gamma, step_size):
        &#39;&#39;&#39;
        learnrate_0: initial learning rate
        gamma: decay rate
        step_size: number of iterations in one half cycle
        t: iteration number
        &#39;&#39;&#39;
        cycle = np.floor(1 + t / (2 * step_size))
        x = np.abs(t / step_size - 2 * cycle + 1)
        learnrate = learnrate_0 * (gamma**(t)) * np.maximum(0, (1 - x))
        return learnrate

    def triangular_lr_decay(self, t, learnrate_low, learnrate_high, T):
        &#39;&#39;&#39;
        learnrate_low: lower bound for learning rate
        learnrate_high: upper bound for learning rate
        T: total number of iterations
        t: iteration number
        &#39;&#39;&#39;
        cycle = np.floor(1 + t / (2 * T))
        x = np.abs(t / T - 2 * cycle + 1)
        learnrate = learnrate_low + (learnrate_high - learnrate_low) * np.maximum(0, (1 - x))
        return learnrate

    def triangular2_lr_decay(self, t, learnrate_low, learnrate_high, T):
        &#39;&#39;&#39;
        learnrate_low: lower bound for learning rate
        learnrate_high: upper bound for learning rate
        T: total number of iterations
        t: iteration number
        &#39;&#39;&#39;
        cycle = np.floor(1 + t / (2 * T))
        x = np.abs(t / T - 2 * cycle + 1)
        learnrate = learnrate_low + (learnrate_high - learnrate_low) * np.maximum(0, (1 - x)) / (2 ** (cycle - 1))
        return learnrate</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.LearningRateDecay.burnin_decay"><code class="name flex">
<span>def <span class="ident">burnin_decay</span></span>(<span>self, t, learnrate_0, burnin, learnrate_burnin)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
burnin: number of initial iterations for which to use burnin learning rate
learnrate_burnin: learning rate used during burnin period
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def burnin_decay(self, t, learnrate_0, burnin, learnrate_burnin):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    burnin: number of initial iterations for which to use burnin learning rate
    learnrate_burnin: learning rate used during burnin period
    t: iteration number
    &#39;&#39;&#39;
    if t &lt; burnin:
        return learnrate_burnin
    else:
        return learnrate_0</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.constant"><code class="name flex">
<span>def <span class="ident">constant</span></span>(<span>self, t, learnrate_0)</span>
</code></dt>
<dd>
<div class="desc"><p>t: iteration
learnrate_0: initial learning rate</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def constant(self, t, learnrate_0):
    &#39;&#39;&#39;
    t: iteration
    learnrate_0: initial learning rate
    &#39;&#39;&#39;
    return learnrate_0</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.cosine_annealing_decay"><code class="name flex">
<span>def <span class="ident">cosine_annealing_decay</span></span>(<span>self, t, learnrate_0, T_max, M)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
T_max: maximum number of iterations
M: number of cycles
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cosine_annealing_decay(self, t, learnrate_0, T_max, M):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    T_max: maximum number of iterations
    M: number of cycles
    t: iteration number
    &#39;&#39;&#39;
    learnrate = learnrate_0 / 2 * (np.cos(np.pi * (t % (T_max // M)) / (T_max // M)) + 1)
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.cosine_decay"><code class="name flex">
<span>def <span class="ident">cosine_decay</span></span>(<span>self, t, learnrate_0, T_max)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
T_max: maximum number of iterations
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cosine_decay(self, t, learnrate_0, T_max):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    T_max: maximum number of iterations
    t: iteration number
    &#39;&#39;&#39;
    learnrate = learnrate_0 * 0.5 * (1 + np.cos(np.pi * t / T_max))
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.cosine_decay_restarts"><code class="name flex">
<span>def <span class="ident">cosine_decay_restarts</span></span>(<span>self, t, learnrate_0, T_0, T_mult=2)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
T_0: initial number of iterations
T_mult: factor by which number of iterations increases after each restart
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cosine_decay_restarts(self, t, learnrate_0, T_0, T_mult=2):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    T_0: initial number of iterations
    T_mult: factor by which number of iterations increases after each restart
    t: iteration number
    &#39;&#39;&#39;
    T_i = T_0
    while t &gt;= T_i:
        t = t - T_i
        T_i = T_i * T_mult
    learnrate = learnrate_0 * 0.5 * (1 + np.cos(np.pi * t / T_i))
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.cyclic_decay"><code class="name flex">
<span>def <span class="ident">cyclic_decay</span></span>(<span>self, t, learnrate_0, step_size, max_lr, mode='triangular')</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
step_size: number of iterations in one half cycle
max_lr: maximum learning rate
mode: type of cyclic decay ('triangular', 'triangular2', 'exp_range')
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def cyclic_decay(self, t, learnrate_0, step_size, max_lr, mode=&#39;triangular&#39;):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    step_size: number of iterations in one half cycle
    max_lr: maximum learning rate
    mode: type of cyclic decay (&#39;triangular&#39;, &#39;triangular2&#39;, &#39;exp_range&#39;)
    t: iteration number
    &#39;&#39;&#39;
    cycle = np.floor(1 + t / (2 * step_size))
    x = np.abs(t / step_size - 2 * cycle + 1)
    if mode == &#39;triangular&#39;:
        lr = learnrate_0 + (max_lr - learnrate_0) * np.maximum(0, (1 - x))
    elif mode == &#39;triangular2&#39;:
        lr = learnrate_0 + (max_lr - learnrate_0) * np.maximum(0, (1 - x)) / (2 ** (cycle - 1))
    elif mode == &#39;exp_range&#39;:
        lr = learnrate_0 + (max_lr - learnrate_0) * np.maximum(0, (1 - x)) * (0.99999 ** t)
    return lr</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.decay_on_plateau"><code class="name flex">
<span>def <span class="ident">decay_on_plateau</span></span>(<span>self, t, learnrate_0, min_lr, factor=0.1, patience=10, cooldown=0)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
min_lr: minimum learning rate
factor: factor by which the learning rate is reduced
patience: number of epochs with no improvement after which learning rate will be reduced
cooldown: number of epochs to wait before resuming normal operation after lr has been reduced
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def decay_on_plateau(self, t, learnrate_0, min_lr, factor=0.1, patience=10, cooldown=0):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    min_lr: minimum learning rate
    factor: factor by which the learning rate is reduced
    patience: number of epochs with no improvement after which learning rate will be reduced
    cooldown: number of epochs to wait before resuming normal operation after lr has been reduced
    t: iteration number
    &#39;&#39;&#39;
    if t &lt; cooldown:
        return learnrate_0
    elif t % patience == 0:
        return max(learnrate_0 * factor, min_lr)
    else:
        return learnrate_0</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.exp_range_decay"><code class="name flex">
<span>def <span class="ident">exp_range_decay</span></span>(<span>self, t, learnrate_0, gamma, step_size)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
gamma: decay rate
step_size: number of iterations in one half cycle
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def exp_range_decay(self, t, learnrate_0, gamma, step_size):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    gamma: decay rate
    step_size: number of iterations in one half cycle
    t: iteration number
    &#39;&#39;&#39;
    cycle = np.floor(1 + t / (2 * step_size))
    x = np.abs(t / step_size - 2 * cycle + 1)
    learnrate = learnrate_0 * (gamma**(t)) * np.maximum(0, (1 - x))
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.exponential_decay"><code class="name flex">
<span>def <span class="ident">exponential_decay</span></span>(<span>self, t, learnrate_0, k)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
k: Exponential Decay rate
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def exponential_decay(self, t, learnrate_0, k):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    k: Exponential Decay rate
    t: iteration number
    &#39;&#39;&#39;
    learnrate = learnrate_0 * np.exp(-k*t)
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.inverse_time_decay"><code class="name flex">
<span>def <span class="ident">inverse_time_decay</span></span>(<span>self, t, learnrate_0, k)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
k: Decay rate
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inverse_time_decay(self, t, learnrate_0, k):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    k: Decay rate
    t: iteration number
    &#39;&#39;&#39;
    learnrate = learnrate_0 / (1 + k * t)
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.linear_cosine_decay"><code class="name flex">
<span>def <span class="ident">linear_cosine_decay</span></span>(<span>self, t, learnrate_0, T_max)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
T_max: maximum number of iterations
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def linear_cosine_decay(self, t, learnrate_0, T_max):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    T_max: maximum number of iterations
    t: iteration number
    &#39;&#39;&#39;
    alpha = t / T_max
    learnrate = (1 - alpha) * learnrate_0 * 0.5 * (1 + np.cos(np.pi * t / T_max)) + alpha * learnrate_0
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.natural_exp_decay"><code class="name flex">
<span>def <span class="ident">natural_exp_decay</span></span>(<span>self, t, learnrate_0, k)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
k: Decay rate
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def natural_exp_decay(self, t, learnrate_0, k):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    k: Decay rate
    t: iteration number
    &#39;&#39;&#39;
    learnrate = learnrate_0 * np.exp(-k * t)
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.one_cycle_policy"><code class="name flex">
<span>def <span class="ident">one_cycle_policy</span></span>(<span>self, t, learnrate_max, stepsize, base_lr=None)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_max: maximum learning rate
stepsize: number of iterations in one half cycle
base_lr: initial learning rate
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def one_cycle_policy(self, t, learnrate_max, stepsize, base_lr=None):
    &#39;&#39;&#39;
    learnrate_max: maximum learning rate
    stepsize: number of iterations in one half cycle
    base_lr: initial learning rate
    t: iteration number
    &#39;&#39;&#39;
    if base_lr is None:
        base_lr = learnrate_max / 10
    if t &lt;= stepsize:
        return base_lr + (learnrate_max - base_lr) * t / stepsize
    elif t &lt;= 2 * stepsize:
        return learnrate_max - (learnrate_max - base_lr) * (t - stepsize) / stepsize
    else:
        return base_lr</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.piecewise_constant_decay"><code class="name flex">
<span>def <span class="ident">piecewise_constant_decay</span></span>(<span>self, t, learnrate_0, boundaries, values)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
t: iteration number
boundaries: list of iteration numbers at which the learning rate changes
values: list of learning rates corresponding to the intervals defined by boundaries</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def piecewise_constant_decay(self, t, learnrate_0, boundaries, values):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    t: iteration number
    boundaries: list of iteration numbers at which the learning rate changes
    values: list of learning rates corresponding to the intervals defined by boundaries
    &#39;&#39;&#39;
    for b, v in zip(boundaries, values):
        if t &lt; b:
            return v
    return values[-1]</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.polynomial_decay"><code class="name flex">
<span>def <span class="ident">polynomial_decay</span></span>(<span>self, t, learnrate_0, power=1)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
power: exponent of the polynomial
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def polynomial_decay(self, t, learnrate_0, power=1):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    power: exponent of the polynomial
    t: iteration number
    &#39;&#39;&#39;
    learnrate = learnrate_0 / (1 + t)**power
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.power_decay"><code class="name flex">
<span>def <span class="ident">power_decay</span></span>(<span>self, t, learnrate_0, k=0.1, power=0.75)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
k: Decay rate
power: power to which iteration number is raised
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def power_decay(self, t, learnrate_0, k=0.1, power=0.75):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    k: Decay rate
    power: power to which iteration number is raised
    t: iteration number
    &#39;&#39;&#39;
    learnrate = learnrate_0 / (1 + k * t)**power
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.sqrt_decay"><code class="name flex">
<span>def <span class="ident">sqrt_decay</span></span>(<span>self, t, learnrate_0, k=1)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
k: Decay rate
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def sqrt_decay(self, t, learnrate_0, k=1):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    k: Decay rate
    t: iteration number
    &#39;&#39;&#39;
    learnrate = learnrate_0 / np.sqrt(k * t + 1)
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.staircase_decay"><code class="name flex">
<span>def <span class="ident">staircase_decay</span></span>(<span>self, t, learnrate_0, drop_rate=0.5, epochs_drop=10.0)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
drop_rate: factor by which learning rate is reduced at each stage
epochs_drop: number of epochs after which learning rate drops
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def staircase_decay(self, t, learnrate_0, drop_rate=0.5, epochs_drop=10.0):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    drop_rate: factor by which learning rate is reduced at each stage
    epochs_drop: number of epochs after which learning rate drops
    t: iteration number
    &#39;&#39;&#39;
    learnrate = learnrate_0 * (drop_rate ** np.floor((1+t) / epochs_drop))
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.step_decay"><code class="name flex">
<span>def <span class="ident">step_decay</span></span>(<span>self, t, learnrate_0, F, D)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
F: factor value controlling the rate in which the learning date drops
D: ‚ÄúDrop every‚Äù iteration
t: current iteration</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def step_decay(self, t, learnrate_0, F, D):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    F: factor value controlling the rate in which the learning date drops
    D: ‚ÄúDrop every‚Äù iteration
    t: current iteration
    &#39;&#39;&#39;
    mult = F**np.floor((1+t)/D)
    learnrate = learnrate_0 * mult
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.time_decay"><code class="name flex">
<span>def <span class="ident">time_decay</span></span>(<span>self, t, learnrate_0, k)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
k: Decay rate
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def time_decay(self, t, learnrate_0, k):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    k: Decay rate
    t: iteration number
    &#39;&#39;&#39;
    learnrate = learnrate_0 /(1+(k*t))
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.triangular2_learning_rate"><code class="name flex">
<span>def <span class="ident">triangular2_learning_rate</span></span>(<span>self, t, learnrate_low, learnrate_high, T)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_low: lower bound for learning rate
learnrate_high: upper bound for learning rate
T: total number of iterations
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def triangular2_learning_rate(self, t, learnrate_low, learnrate_high, T):
    &#39;&#39;&#39;
    learnrate_low: lower bound for learning rate
    learnrate_high: upper bound for learning rate
    T: total number of iterations
    t: iteration number
    &#39;&#39;&#39;
    cycle = np.floor(1 + t / (2 * T))
    x = np.abs(t / T - 2 * cycle + 1)
    learnrate = learnrate_low + (learnrate_high - learnrate_low) * np.maximum(0, (1 - x)) / (2 ** (cycle - 1))
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.triangular2_lr_decay"><code class="name flex">
<span>def <span class="ident">triangular2_lr_decay</span></span>(<span>self, t, learnrate_low, learnrate_high, T)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_low: lower bound for learning rate
learnrate_high: upper bound for learning rate
T: total number of iterations
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def triangular2_lr_decay(self, t, learnrate_low, learnrate_high, T):
    &#39;&#39;&#39;
    learnrate_low: lower bound for learning rate
    learnrate_high: upper bound for learning rate
    T: total number of iterations
    t: iteration number
    &#39;&#39;&#39;
    cycle = np.floor(1 + t / (2 * T))
    x = np.abs(t / T - 2 * cycle + 1)
    learnrate = learnrate_low + (learnrate_high - learnrate_low) * np.maximum(0, (1 - x)) / (2 ** (cycle - 1))
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.triangular_learning_rate"><code class="name flex">
<span>def <span class="ident">triangular_learning_rate</span></span>(<span>self, t, learnrate_low, learnrate_high, T)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_low: lower bound for learning rate
learnrate_high: upper bound for learning rate
T: total number of iterations
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def triangular_learning_rate(self, t, learnrate_low, learnrate_high, T):
    &#39;&#39;&#39;
    learnrate_low: lower bound for learning rate
    learnrate_high: upper bound for learning rate
    T: total number of iterations
    t: iteration number
    &#39;&#39;&#39;
    cycle = np.floor(1 + t / (2 * T))
    x = np.abs(t / T - 2 * cycle + 1)
    learnrate = learnrate_low + (learnrate_high - learnrate_low) * np.maximum(0, (1 - x))
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.triangular_lr_decay"><code class="name flex">
<span>def <span class="ident">triangular_lr_decay</span></span>(<span>self, t, learnrate_low, learnrate_high, T)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_low: lower bound for learning rate
learnrate_high: upper bound for learning rate
T: total number of iterations
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def triangular_lr_decay(self, t, learnrate_low, learnrate_high, T):
    &#39;&#39;&#39;
    learnrate_low: lower bound for learning rate
    learnrate_high: upper bound for learning rate
    T: total number of iterations
    t: iteration number
    &#39;&#39;&#39;
    cycle = np.floor(1 + t / (2 * T))
    x = np.abs(t / T - 2 * cycle + 1)
    learnrate = learnrate_low + (learnrate_high - learnrate_low) * np.maximum(0, (1 - x))
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.warm_restart_decay"><code class="name flex">
<span>def <span class="ident">warm_restart_decay</span></span>(<span>self, t, learnrate_0, T_0, T_mult)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
T_0: initial number of iterations
T_mult: factor by which number of iterations increases after each restart
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def warm_restart_decay(self, t, learnrate_0, T_0, T_mult):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    T_0: initial number of iterations
    T_mult: factor by which number of iterations increases after each restart
    t: iteration number
    &#39;&#39;&#39;
    T_i = T_0
    while t &gt;= T_i:
        t = t - T_i
        T_i = T_i * T_mult
    learnrate = learnrate_0 * 0.5 * (1 + np.cos(np.pi * t / T_i))
    return learnrate</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.LearningRateDecay.warmup_decay"><code class="name flex">
<span>def <span class="ident">warmup_decay</span></span>(<span>self, t, learnrate_0, warmup_steps=10)</span>
</code></dt>
<dd>
<div class="desc"><p>learnrate_0: initial learning rate
warmup_steps: number of initial iterations for which to use linearly increasing learning rate
t: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def warmup_decay(self, t, learnrate_0, warmup_steps=10):
    &#39;&#39;&#39;
    learnrate_0: initial learning rate
    warmup_steps: number of initial iterations for which to use linearly increasing learning rate
    t: iteration number
    &#39;&#39;&#39;
    if t &lt; warmup_steps:
        return learnrate_0 * (t / warmup_steps)
    else:
        return learnrate_0</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.Linear"><code class="flex name class">
<span>class <span class="ident">Linear</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Linear(Function):
    def func(self, x):
        return x
    def deriv(self, x):
        return np.ones(x.shape)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Linear.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self, x):
    return np.ones(x.shape)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Linear.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, x):
    return x</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.Linear2"><code class="flex name class">
<span>class <span class="ident">Linear2</span></span>
<span>(</span><span>m=1, n=0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Linear2(Function):
    def __init__(self, m=1, n=0):
        self.m = m
        self.n = n
    
    def func(self, x):
        return self.m * x + self.n
    def deriv(self, x):
        return np.ones(x.shape)
    
    def intderiv(self, x):
        return self.m</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Linear2.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self, x):
    return np.ones(x.shape)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Linear2.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, x):
    return self.m * x + self.n</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Linear2.intderiv"><code class="name flex">
<span>def <span class="ident">intderiv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def intderiv(self, x):
    return self.m</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.Logit"><code class="flex name class">
<span>class <span class="ident">Logit</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Logit(Function):
    def func(self, x):
        return np.log((x/(1-x)))
    
    def deriv(self, x):
        return (-1) / (x*(x-1))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Logit.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self, x):
    return (-1) / (x*(x-1))</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Logit.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, x):
    return np.log((x/(1-x)))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.Multiquatratisch"><code class="flex name class">
<span>class <span class="ident">Multiquatratisch</span></span>
<span>(</span><span>dot)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Multiquatratisch(Function):#Abstan (x,0) zu Punkt (x,y)
    def __init__(self,dot):
        self.x = dot[0]
        self.y = dot[1]
    
    def func(self, x):
        return np.sqrt((x - self.x)**2 + self.y**2)
    
    def deriv(self, x):
        return (x - self.x)  /  (np.sqrt(x**2 - 2*x*self.x + self.y**2 + self.x**2))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Multiquatratisch.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self, x):
    return (x - self.x)  /  (np.sqrt(x**2 - 2*x*self.x + self.y**2 + self.x**2))</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Multiquatratisch.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, x):
    return np.sqrt((x - self.x)**2 + self.y**2)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.NoDeriv"><code class="flex name class">
<span>class <span class="ident">NoDeriv</span></span>
<span>(</span><span>f='sigmoid')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class NoDeriv(Function): 
    def __init__(self, f=&#39;sigmoid&#39;):
        self.f = recPicker(float)
        
    def func(self, x):
        return self.f.func(x)
    
    def deriv(self, x): 
        return 1
    
    def change(self,func):
        self.f = func
        return self</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.NoDeriv.change"><code class="name flex">
<span>def <span class="ident">change</span></span>(<span>self, func)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def change(self,func):
    self.f = func
    return self</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.NoDeriv.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self, x): 
    return 1</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.NoDeriv.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, x):
    return self.f.func(x)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.Normal"><code class="flex name class">
<span>class <span class="ident">Normal</span></span>
<span>(</span><span>mhy, phi)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Normal(Function): #Normalverteilung / Dichtefunktion
    def __init__(self, mhy, phi):
        self.m = mhy
        self.p = phi
    
    def func(self,x):
        return (1/(np.sqrt(2*np.pi * (self.p**2))))  * np.exp(-0.5 * ((x-self.m)/self.p)**2)
    
    def deriv(self, x):
        return ((self.m * np.exp((-self.m**2)/(2* self.p**2)) * np.sqrt(2)) / (2 * np.abs(self.p**3) * np.sqrt(np.pi))    -    (x * np.exp((-self.m**2)/(2* self.p**2)) * np.sqrt(2)) / (2 * np.abs(self.p**3) * np.sqrt(np.pi)))    *    np.exp(((self.m * x)/(self.p**2))  -  ((x**2) / (2 * self.p**2)))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Normal.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self, x):
    return ((self.m * np.exp((-self.m**2)/(2* self.p**2)) * np.sqrt(2)) / (2 * np.abs(self.p**3) * np.sqrt(np.pi))    -    (x * np.exp((-self.m**2)/(2* self.p**2)) * np.sqrt(2)) / (2 * np.abs(self.p**3) * np.sqrt(np.pi)))    *    np.exp(((self.m * x)/(self.p**2))  -  ((x**2) / (2 * self.p**2)))</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Normal.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self,x):
    return (1/(np.sqrt(2*np.pi * (self.p**2))))  * np.exp(-0.5 * ((x-self.m)/self.p)**2)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.Optimizer"><code class="flex name class">
<span>class <span class="ident">Optimizer</span></span>
<span>(</span><span>optimizer_type=None, shape_W=None, shape_b=None, momentum1=0.9, momentum2=0.999, epsilon=1e-08)</span>
</code></dt>
<dd>
<div class="desc"><p>Parameters</p>
<p>momentum1: float hyperparameter &gt;= 0 that accelerates gradient descent in the relevant
direction and dampens oscillations. Defaults to 0, i.e., vanilla gradient descent.
Also used in RMSProp
momentum2: used in Adam only
optimizer_type: type of optimizer
available options are 'gd', 'sgd' (This also includes momentum), 'adam', and 'rmsprop'
shape_W: Shape of the weight matrix W/ Kernel K
shape_b: Shape of the bias matrix b
epsilon: parameter used in RMSProp and Adam to avoid division by zero error</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Optimizer:

    def __init__(self, optimizer_type=None, shape_W=None, shape_b=None,
                 momentum1=0.9, momentum2=0.999, epsilon=1e-8):
        &#39;&#39;&#39;
        Parameters

        momentum1: float hyperparameter &gt;= 0 that accelerates gradient descent in the relevant
                   direction and dampens oscillations. Defaults to 0, i.e., vanilla gradient descent.
                   Also used in RMSProp
        momentum2: used in Adam only
        optimizer_type: type of optimizer
                        available options are &#39;gd&#39;, &#39;sgd&#39; (This also includes momentum), &#39;adam&#39;, and &#39;rmsprop&#39;
        shape_W: Shape of the weight matrix W/ Kernel K
        shape_b: Shape of the bias matrix b
        epsilon: parameter used in RMSProp and Adam to avoid division by zero error
        &#39;&#39;&#39;

        if optimizer_type is None:
            self.optimizer_type = &#39;adam&#39;
        else:
            self.optimizer_type = optimizer_type

        self.momentum1 = momentum1
        self.momentum2 = momentum2
        self.epsilon = epsilon

        self.vdW = np.zeros(shape_W)
        self.vdb = np.zeros(shape_b)

        self.SdW = np.zeros(shape_W)
        self.Sdb = np.zeros(shape_b)
        
        if(optimizer_type.lower() == &#39;rprop&#39;):
            self.delta_W = np.ones(shape_W)
            self.delta_b= np.ones(shape_b)
            self.eta_plus = 1.2
            self.eta_minus = 0.5
            self.delta_max = 50
            self.delta_min = 1e-6

    &#39;&#39;&#39;
        dW: gradient of Weight W for iteration k
        db: gradient of bias b for iteration k
        k: iteration number
    &#39;&#39;&#39;
    
    def GD(self, dW, db, k = None): #Gradient Descent
        
        return dW, db

    def SGD(self, dW, db, k = None): #Stochastic Gradient Descent (with momentum)
        self.vdW = self.momentum1*self.vdW + (1-self.momentum1)*dW
        self.vdb = self.momentum1*self.vdb + (1-self.momentum1)*db

        return self.vdW, self.vdb

    def RMSProp(self, dW, db, k = None):
        self.SdW = self.momentum2*self.SdW + (1-self.momentum2)*(dW**2)
        self.Sdb = self.momentum2*self.Sdb + (1-self.momentum2)*(db**2)

        den_W = np.sqrt(self.SdW) + self.epsilon
        den_b = np.sqrt(self.Sdb) + self.epsilon

        return dW/den_W, db/den_b

    def Adam(self, dW, db, k):
        &#39;&#39;&#39;
        dW: gradient of Weight W for iteration k
        db: gradient of bias b for iteration k
        k: iteration number
        &#39;&#39;&#39;
        # momentum
        self.vdW = self.momentum1*self.vdW + (1-self.momentum1)*dW
        self.vdb = self.momentum1*self.vdb + (1-self.momentum1)*db

        # rmsprop
        self.SdW = self.momentum2*self.SdW + (1-self.momentum2)*(dW**2)
        self.Sdb = self.momentum2*self.Sdb + (1-self.momentum2)*(db**2)

        # correction
        if k&gt;1:
            vdW_h = self.vdW / (1-(self.momentum1**k))
            vdb_h = self.vdb / (1-(self.momentum1**k))
            SdW_h = self.SdW / (1-(self.momentum2**k))
            Sdb_h = self.Sdb / (1-(self.momentum2**k))
        else:
            vdW_h = self.vdW
            vdb_h = self.vdb
            SdW_h = self.SdW
            Sdb_h = self.Sdb

        den_W = np.sqrt(SdW_h) + self.epsilon
        den_b = np.sqrt(Sdb_h) + self.epsilon

        return vdW_h/den_W, vdb_h/den_b
    
    def Adagrad(self, dW, db, k):
        # Update the sum of squares of gradients
        self.SdW += dW**2
        self.Sdb += db**2

        # Calculate the denominator
        den_W = np.sqrt(self.SdW) + self.epsilon
        den_b = np.sqrt(self.Sdb) + self.epsilon

        # Calculate the updated gradients
        return dW/den_W, db/den_b

    def Adadelta(self, dW, db, k):
        # Update the sum of squares of gradients
        self.SdW = self.momentum2*self.SdW + (1-self.momentum2)*(dW**2)
        self.Sdb = self.momentum2*self.Sdb + (1-self.momentum2)*(db**2)

        # Calculate the numerator
        num_W = np.sqrt(self.SdW_h + self.epsilon)*dW
        num_b = np.sqrt(self.Sdb_h + self.epsilon)*db

        # Calculate the denominator
        den_W = np.sqrt(self.SdW) + self.epsilon
        den_b = np.sqrt(self.Sdb) + self.epsilon

        # Update the sum of squares of deltas
        self.SdW_h = self.momentum1*self.SdW_h + (1-self.momentum1)*(num_W**2)
        self.Sdb_h = self.momentum1*self.Sdb_h + (1-self.momentum1)*(num_b**2)

        # Calculate the updated gradients
        return num_W/den_W, num_b/den_b

    def NAG(self, dW, db, k): #Nesterov_Accelerated_Gradient
        # Store the previous velocity
        vdW_prev = self.vdW
        vdb_prev = self.vdb

        # Update the velocity
        self.vdW = self.momentum1*self.vdW - self.learning_rate*dW
        self.vdb = self.momentum1*self.vdb - self.learning_rate*db

        # Calculate the corrected velocity
        vdW_corr = self.momentum1*self.vdW + (1-self.momentum1)*vdW_prev
        vdb_corr = self.momentum1*self.vdb + (1-self.momentum1)*vdb_prev

        # Calculate the denominator
        den_W = np.sqrt(self.SdW) + self.epsilon
        den_b = np.sqrt(self.Sdb) + self.epsilon

        # Calculate the updated gradients
        return vdW_corr/den_W, vdb_corr/den_b

    def AdaMax(self, dW, db, k):
        # Update the sum of powers of gradients
        self.SdW = self.beta2*self.SdW + (1-self.beta2)*(dW**2)
        self.Sdb = self.beta2*self.Sdb + (1-self.beta2)*(db**2)

        # Calculate the numerator
        num_W = self.learning_rate*dW/(np.sqrt(self.SdW) + self.epsilon)
        num_b = self.learning_rate*db/(np.sqrt(self.Sdb) + self.epsilon)

        # Calculate the updated gradients
        return num_W, num_b

    def Nadam(self, dW, db, k, delta_min, delta_max):
        # Update the momentum
        self.vdW = self.momentum1*self.vdW + (1-self.momentum1)*dW
        self.vdb = self.momentum1*self.vdb + (1-self.momentum1)*db

        # Update the sum of squares of gradients
        self.SdW = self.momentum2*self.SdW + (1-self.momentum2)*(dW**2)
        self.Sdb = self.momentum2*self.Sdb + (1-self.momentum2)*(db**2)

        # Calculate the corrected momentum
        vdW_corr = self.vdW/(1-(self.momentum1**k))
        vdb_corr = self.vdb/(1-(self.momentum1**k))

        # Calculate the corrected sum of squares of gradients
        SdW_corr = self.SdW/(1-(self.momentum2**k))
        Sdb_corr = self.Sdb/(1-(self.momentum2**k))

        # Calculate the numerator
        num_W = self.learning_rate*(self.momentum1*vdW_corr + ((1-self.momentum1)/(1-(self.momentum1**k)))*dW)/(np.sqrt(SdW_corr) + self.epsilon)
        num_b = self.learning_rate*(self.momentum1*vdb_corr + ((1-self.momentum1)/(1-(self.momentum1**k)))*db)/(np.sqrt(Sdb_corr) + self.epsilon)
        return num_W, num_b
        
    def Rprop(self, dW, db, k):
        self.delta_W = np.clip(self.delta_W * np.where(np.sign(dW) == np.sign(self.delta_W), self.eta_plus, self.eta_minus), delta_min, delta_max)
        self.delta_b = np.clip(self.delta_b * np.where(np.sign(db) == np.sign(self.delta_b), self.eta_plus, self.eta_minus), delta_min, delta_max)
        nW = -np.sign(dW) * self.delta_W
        nb = -np.sign(db) * self.delta_b
        return nW, nb

    def get_optimization(self, dW, db, k, *args):
        match self.optimizer_type:
            case &#39;gd&#39;: return self.GD(dW, db, k)
            case &#39;sgd&#39;: return self.SGD(dW, db, k)
            case &#39;rsmprop&#39;: return self.RMSProp(dW, db, k)
            case &#39;adam&#39;: return self.Adam(dW, db, k)
            case &#39;adagrad&#39;: return self.Adagrad(dW, db, k)
            case &#39;adadelta&#39;: return self.Adadelta(dW, db, k)
            case &#39;nag&#39;: return self.NAG(dW, db, k)
            case &#39;adamax&#39;: return self.AdaMax(dW, db, k)
            case &#39;nadam&#39;: return self.Nadam(dW, db, k, *args)
            case &#39;rprop&#39;: return self.Rprop(dW, db, k)
            case _: raise ValueError(&#34;Valid optimizer options are only &#39;gd&#39;, &#39;sgd&#39;, &#39;rmsprop&#39;, &#39;adam&#39;, &#39;adagrad&#39;, &#39;adadelta&#39;, &#39;nag&#39;, &#39;adamax&#39;, &#39;nadam&#39; and &#39;rprop&#39;.&#34;)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Optimizer.AdaMax"><code class="name flex">
<span>def <span class="ident">AdaMax</span></span>(<span>self, dW, db, k)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def AdaMax(self, dW, db, k):
    # Update the sum of powers of gradients
    self.SdW = self.beta2*self.SdW + (1-self.beta2)*(dW**2)
    self.Sdb = self.beta2*self.Sdb + (1-self.beta2)*(db**2)

    # Calculate the numerator
    num_W = self.learning_rate*dW/(np.sqrt(self.SdW) + self.epsilon)
    num_b = self.learning_rate*db/(np.sqrt(self.Sdb) + self.epsilon)

    # Calculate the updated gradients
    return num_W, num_b</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Optimizer.Adadelta"><code class="name flex">
<span>def <span class="ident">Adadelta</span></span>(<span>self, dW, db, k)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Adadelta(self, dW, db, k):
    # Update the sum of squares of gradients
    self.SdW = self.momentum2*self.SdW + (1-self.momentum2)*(dW**2)
    self.Sdb = self.momentum2*self.Sdb + (1-self.momentum2)*(db**2)

    # Calculate the numerator
    num_W = np.sqrt(self.SdW_h + self.epsilon)*dW
    num_b = np.sqrt(self.Sdb_h + self.epsilon)*db

    # Calculate the denominator
    den_W = np.sqrt(self.SdW) + self.epsilon
    den_b = np.sqrt(self.Sdb) + self.epsilon

    # Update the sum of squares of deltas
    self.SdW_h = self.momentum1*self.SdW_h + (1-self.momentum1)*(num_W**2)
    self.Sdb_h = self.momentum1*self.Sdb_h + (1-self.momentum1)*(num_b**2)

    # Calculate the updated gradients
    return num_W/den_W, num_b/den_b</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Optimizer.Adagrad"><code class="name flex">
<span>def <span class="ident">Adagrad</span></span>(<span>self, dW, db, k)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Adagrad(self, dW, db, k):
    # Update the sum of squares of gradients
    self.SdW += dW**2
    self.Sdb += db**2

    # Calculate the denominator
    den_W = np.sqrt(self.SdW) + self.epsilon
    den_b = np.sqrt(self.Sdb) + self.epsilon

    # Calculate the updated gradients
    return dW/den_W, db/den_b</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Optimizer.Adam"><code class="name flex">
<span>def <span class="ident">Adam</span></span>(<span>self, dW, db, k)</span>
</code></dt>
<dd>
<div class="desc"><p>dW: gradient of Weight W for iteration k
db: gradient of bias b for iteration k
k: iteration number</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Adam(self, dW, db, k):
    &#39;&#39;&#39;
    dW: gradient of Weight W for iteration k
    db: gradient of bias b for iteration k
    k: iteration number
    &#39;&#39;&#39;
    # momentum
    self.vdW = self.momentum1*self.vdW + (1-self.momentum1)*dW
    self.vdb = self.momentum1*self.vdb + (1-self.momentum1)*db

    # rmsprop
    self.SdW = self.momentum2*self.SdW + (1-self.momentum2)*(dW**2)
    self.Sdb = self.momentum2*self.Sdb + (1-self.momentum2)*(db**2)

    # correction
    if k&gt;1:
        vdW_h = self.vdW / (1-(self.momentum1**k))
        vdb_h = self.vdb / (1-(self.momentum1**k))
        SdW_h = self.SdW / (1-(self.momentum2**k))
        Sdb_h = self.Sdb / (1-(self.momentum2**k))
    else:
        vdW_h = self.vdW
        vdb_h = self.vdb
        SdW_h = self.SdW
        Sdb_h = self.Sdb

    den_W = np.sqrt(SdW_h) + self.epsilon
    den_b = np.sqrt(Sdb_h) + self.epsilon

    return vdW_h/den_W, vdb_h/den_b</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Optimizer.GD"><code class="name flex">
<span>def <span class="ident">GD</span></span>(<span>self, dW, db, k=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def GD(self, dW, db, k = None): #Gradient Descent
    
    return dW, db</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Optimizer.NAG"><code class="name flex">
<span>def <span class="ident">NAG</span></span>(<span>self, dW, db, k)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def NAG(self, dW, db, k): #Nesterov_Accelerated_Gradient
    # Store the previous velocity
    vdW_prev = self.vdW
    vdb_prev = self.vdb

    # Update the velocity
    self.vdW = self.momentum1*self.vdW - self.learning_rate*dW
    self.vdb = self.momentum1*self.vdb - self.learning_rate*db

    # Calculate the corrected velocity
    vdW_corr = self.momentum1*self.vdW + (1-self.momentum1)*vdW_prev
    vdb_corr = self.momentum1*self.vdb + (1-self.momentum1)*vdb_prev

    # Calculate the denominator
    den_W = np.sqrt(self.SdW) + self.epsilon
    den_b = np.sqrt(self.Sdb) + self.epsilon

    # Calculate the updated gradients
    return vdW_corr/den_W, vdb_corr/den_b</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Optimizer.Nadam"><code class="name flex">
<span>def <span class="ident">Nadam</span></span>(<span>self, dW, db, k, delta_min, delta_max)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Nadam(self, dW, db, k, delta_min, delta_max):
    # Update the momentum
    self.vdW = self.momentum1*self.vdW + (1-self.momentum1)*dW
    self.vdb = self.momentum1*self.vdb + (1-self.momentum1)*db

    # Update the sum of squares of gradients
    self.SdW = self.momentum2*self.SdW + (1-self.momentum2)*(dW**2)
    self.Sdb = self.momentum2*self.Sdb + (1-self.momentum2)*(db**2)

    # Calculate the corrected momentum
    vdW_corr = self.vdW/(1-(self.momentum1**k))
    vdb_corr = self.vdb/(1-(self.momentum1**k))

    # Calculate the corrected sum of squares of gradients
    SdW_corr = self.SdW/(1-(self.momentum2**k))
    Sdb_corr = self.Sdb/(1-(self.momentum2**k))

    # Calculate the numerator
    num_W = self.learning_rate*(self.momentum1*vdW_corr + ((1-self.momentum1)/(1-(self.momentum1**k)))*dW)/(np.sqrt(SdW_corr) + self.epsilon)
    num_b = self.learning_rate*(self.momentum1*vdb_corr + ((1-self.momentum1)/(1-(self.momentum1**k)))*db)/(np.sqrt(Sdb_corr) + self.epsilon)
    return num_W, num_b</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Optimizer.RMSProp"><code class="name flex">
<span>def <span class="ident">RMSProp</span></span>(<span>self, dW, db, k=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def RMSProp(self, dW, db, k = None):
    self.SdW = self.momentum2*self.SdW + (1-self.momentum2)*(dW**2)
    self.Sdb = self.momentum2*self.Sdb + (1-self.momentum2)*(db**2)

    den_W = np.sqrt(self.SdW) + self.epsilon
    den_b = np.sqrt(self.Sdb) + self.epsilon

    return dW/den_W, db/den_b</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Optimizer.Rprop"><code class="name flex">
<span>def <span class="ident">Rprop</span></span>(<span>self, dW, db, k)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def Rprop(self, dW, db, k):
    self.delta_W = np.clip(self.delta_W * np.where(np.sign(dW) == np.sign(self.delta_W), self.eta_plus, self.eta_minus), delta_min, delta_max)
    self.delta_b = np.clip(self.delta_b * np.where(np.sign(db) == np.sign(self.delta_b), self.eta_plus, self.eta_minus), delta_min, delta_max)
    nW = -np.sign(dW) * self.delta_W
    nb = -np.sign(db) * self.delta_b
    return nW, nb</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Optimizer.SGD"><code class="name flex">
<span>def <span class="ident">SGD</span></span>(<span>self, dW, db, k=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def SGD(self, dW, db, k = None): #Stochastic Gradient Descent (with momentum)
    self.vdW = self.momentum1*self.vdW + (1-self.momentum1)*dW
    self.vdb = self.momentum1*self.vdb + (1-self.momentum1)*db

    return self.vdW, self.vdb</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Optimizer.get_optimization"><code class="name flex">
<span>def <span class="ident">get_optimization</span></span>(<span>self, dW, db, k, *args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_optimization(self, dW, db, k, *args):
    match self.optimizer_type:
        case &#39;gd&#39;: return self.GD(dW, db, k)
        case &#39;sgd&#39;: return self.SGD(dW, db, k)
        case &#39;rsmprop&#39;: return self.RMSProp(dW, db, k)
        case &#39;adam&#39;: return self.Adam(dW, db, k)
        case &#39;adagrad&#39;: return self.Adagrad(dW, db, k)
        case &#39;adadelta&#39;: return self.Adadelta(dW, db, k)
        case &#39;nag&#39;: return self.NAG(dW, db, k)
        case &#39;adamax&#39;: return self.AdaMax(dW, db, k)
        case &#39;nadam&#39;: return self.Nadam(dW, db, k, *args)
        case &#39;rprop&#39;: return self.Rprop(dW, db, k)
        case _: raise ValueError(&#34;Valid optimizer options are only &#39;gd&#39;, &#39;sgd&#39;, &#39;rmsprop&#39;, &#39;adam&#39;, &#39;adagrad&#39;, &#39;adadelta&#39;, &#39;nag&#39;, &#39;adamax&#39;, &#39;nadam&#39; and &#39;rprop&#39;.&#34;)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.PwLinear"><code class="flex name class">
<span>class <span class="ident">PwLinear</span></span>
<span>(</span><span>start, end, before=0, after=1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class PwLinear(Function): #piecewise(St√ºckweise) Linear
    def __init__(self, start, end, before = 0, after = 1):
        self.s = start #Start Stelle der Liniaren Funktion
        self.e = end #Ende der Liniaren Funktion
        self.b = before # f(x) wenn x&lt; start
        self.a = after #fx) wenn x &gt; end
        self.m = (after - before) / (end - start) #Anstieg der Linearen Funktion von start to end m = (y2 - y1)/(x2 - x1)
        self.n = before - (self.m * start) #Schnittstelle mit de y-Achse n = y - (m * x)
    
    def update(self):
        self.m = (self.a - self.b) / (self.e - self.s) #Anstieg der Linearen Funktion von start to end m = (y2 - y1)/(x2 - x1)
        self.n = self.b - (self.m * self.s) #Schnittstelle mit de y-Achse n = y - (m * x)
    
    def func(self, x):
        kx = np.where(self.start&lt;x&lt;self.e, x*self.m+self.n)
        kkx = np.where(self.e&lt;kx, self.a)
        return np.where(self.s&gt;kkx, self.b)
    def deriv(self, x):
        return np.where(self.s&lt;x&lt;self.e, self.m, 0)
    
    def intfunc(self, x):
        if x &lt; self.s: return self.b
        elif x &gt; self.e: return self.a
        else: return x*self.m + self.n
    
    def intderiv(self, x):
        if x &lt; self.s: return 0
        elif x &gt; self.e: return 0
        else: return self.m</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.PwLinear.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self, x):
    return np.where(self.s&lt;x&lt;self.e, self.m, 0)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.PwLinear.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, x):
    kx = np.where(self.start&lt;x&lt;self.e, x*self.m+self.n)
    kkx = np.where(self.e&lt;kx, self.a)
    return np.where(self.s&gt;kkx, self.b)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.PwLinear.intderiv"><code class="name flex">
<span>def <span class="ident">intderiv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def intderiv(self, x):
    if x &lt; self.s: return 0
    elif x &gt; self.e: return 0
    else: return self.m</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.PwLinear.intfunc"><code class="name flex">
<span>def <span class="ident">intfunc</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def intfunc(self, x):
    if x &lt; self.s: return self.b
    elif x &gt; self.e: return self.a
    else: return x*self.m + self.n</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.PwLinear.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self):
    self.m = (self.a - self.b) / (self.e - self.s) #Anstieg der Linearen Funktion von start to end m = (y2 - y1)/(x2 - x1)
    self.n = self.b - (self.m * self.s) #Schnittstelle mit de y-Achse n = y - (m * x)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.ReLU"><code class="flex name class">
<span>class <span class="ident">ReLU</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class ReLU(Function):
    def func(self,x): 
        return x * (x&gt;0)
    
    def deriv(self, x):
        return (x&gt;0) * np.ones(x.shape)
        
    def intderiv(self, x):
        if x &gt; 0: return 0
        return 1</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.ReLU.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self, x):
    return (x&gt;0) * np.ones(x.shape)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.ReLU.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self,x): 
    return x * (x&gt;0)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.ReLU.intderiv"><code class="name flex">
<span>def <span class="ident">intderiv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def intderiv(self, x):
    if x &gt; 0: return 0
    return 1</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.SReLU"><code class="flex name class">
<span>class <span class="ident">SReLU</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SReLU(Function): #Smooth rectified Linear Unit/Smooth Max/Soft Plus
    def func(self, x):
        return np.log(1 + np.exp(x))

    def deriv(self,x):
        return 1/(1 + np.exp(-x))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.SReLU.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self,x):
    return 1/(1 + np.exp(-x))</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.SReLU.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, x):
    return np.log(1 + np.exp(x))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.Sigmoid"><code class="flex name class">
<span>class <span class="ident">Sigmoid</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sigmoid(Function):
    def func(self, x):
        return 1 / (1 + np.exp(-x))
    
        
    def deriv(self, x):
        return self.func(x) * (1 - self.func(x))</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Sigmoid.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self, x):
    return self.func(x) * (1 - self.func(x))</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Sigmoid.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, x):
    return 1 / (1 + np.exp(-x))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.SinReLU"><code class="flex name class">
<span>class <span class="ident">SinReLU</span></span>
<span>(</span><span>wende=0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class SinReLU(Function): #2.Modifikation mit sin
    def __init__(self,wende=0):
        self.w = wende #Wendepunkt
    
    def func(self, x):
        return x*(x&gt;0) + np.cos(x)
    def deriv(self, x):
        return np.where(x&gt;0, 1-np.cos(x), -np.cos(x))
    
    def intderiv(self,x):
        if x&gt;0: return  1 + np.cos(x)
        return np.cos(x)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.SinReLU.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self, x):
    return np.where(x&gt;0, 1-np.cos(x), -np.cos(x))</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.SinReLU.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, x):
    return x*(x&gt;0) + np.cos(x)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.SinReLU.intderiv"><code class="name flex">
<span>def <span class="ident">intderiv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def intderiv(self,x):
    if x&gt;0: return  1 + np.cos(x)
    return np.cos(x)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.Sinus"><code class="flex name class">
<span>class <span class="ident">Sinus</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Sinus(Function):
    def func(self,x):
        return np.sin(x)
    def deriv(self,x): 
        return np.cos(x)</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Sinus.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self,x): 
    return np.cos(x)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Sinus.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self,x):
    return np.sin(x)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.Softmax"><code class="flex name class">
<span>class <span class="ident">Softmax</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Softmax(Function):
    def func(self,x):
        &#39;&#39;&#39;
        Parameters

        x: input matrix of shape (m, d)
        where &#39;m&#39; is the number of samples (in case of batch gradient descent of size m)
        and &#39;d&#39; is the number of features
        &#39;&#39;&#39;
        z = x - np.max(x, axis=-1, keepdims=True)
        numerator = np.exp(z)
        denominator = np.sum(numerator, axis=-1, keepdims=True)
        softmax = numerator / denominator
        return softmax
    
    def softmax_grad(s): 
        # Take the derivative of softmax element w.r.t the each logit which is usually Wi * X
        # input s is softmax value of the original input x. 
        # s.shape = (1, n) 
        # i.e. s = np.array([0.3, 0.7]), x = np.array([0, 1])

        # initialize the 2-D jacobian matrix.
        jacobian_m = np.diag(s)

        for i in range(len(jacobian_m)):
            for j in range(len(jacobian_m)):
                if i == j:
                    jacobian_m[i][j] = s[i] * (1-s[i])
                else: 
                    jacobian_m[i][j] = -s[i]*s[j]
        return jacobian_m
    
    def deriv(self, x):
        &#39;&#39;&#39;
        Parameters

        x: input matrix of shape (m, d)
        where &#39;m&#39; is the number of samples (in case of batch gradient descent of size m)
        and &#39;d&#39; is the number of features
        &#39;&#39;&#39;
        if len(x.shape)==1:
            x = np.array(x).reshape(1,-1)
        else:
            x = np.array(x)
        m, d = x.shape
        a = self.func(x)
        tensor1 = np.einsum(&#39;ij,ik-&gt;ijk&#39;, a, a)
        tensor2 = np.einsum(&#39;ij,jk-&gt;ijk&#39;, a, np.eye(d, d))
        return tensor2 - tensor1</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Softmax.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Parameters</p>
<p>x: input matrix of shape (m, d)
where 'm' is the number of samples (in case of batch gradient descent of size m)
and 'd' is the number of features</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self, x):
    &#39;&#39;&#39;
    Parameters

    x: input matrix of shape (m, d)
    where &#39;m&#39; is the number of samples (in case of batch gradient descent of size m)
    and &#39;d&#39; is the number of features
    &#39;&#39;&#39;
    if len(x.shape)==1:
        x = np.array(x).reshape(1,-1)
    else:
        x = np.array(x)
    m, d = x.shape
    a = self.func(x)
    tensor1 = np.einsum(&#39;ij,ik-&gt;ijk&#39;, a, a)
    tensor2 = np.einsum(&#39;ij,jk-&gt;ijk&#39;, a, np.eye(d, d))
    return tensor2 - tensor1</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Softmax.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"><p>Parameters</p>
<p>x: input matrix of shape (m, d)
where 'm' is the number of samples (in case of batch gradient descent of size m)
and 'd' is the number of features</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self,x):
    &#39;&#39;&#39;
    Parameters

    x: input matrix of shape (m, d)
    where &#39;m&#39; is the number of samples (in case of batch gradient descent of size m)
    and &#39;d&#39; is the number of features
    &#39;&#39;&#39;
    z = x - np.max(x, axis=-1, keepdims=True)
    numerator = np.exp(z)
    denominator = np.sum(numerator, axis=-1, keepdims=True)
    softmax = numerator / denominator
    return softmax</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Softmax.softmax_grad"><code class="name flex">
<span>def <span class="ident">softmax_grad</span></span>(<span>s)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def softmax_grad(s): 
    # Take the derivative of softmax element w.r.t the each logit which is usually Wi * X
    # input s is softmax value of the original input x. 
    # s.shape = (1, n) 
    # i.e. s = np.array([0.3, 0.7]), x = np.array([0, 1])

    # initialize the 2-D jacobian matrix.
    jacobian_m = np.diag(s)

    for i in range(len(jacobian_m)):
        for j in range(len(jacobian_m)):
            if i == j:
                jacobian_m[i][j] = s[i] * (1-s[i])
            else: 
                jacobian_m[i][j] = -s[i]*s[j]
    return jacobian_m</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.Step"><code class="flex name class">
<span>class <span class="ident">Step</span></span>
<span>(</span><span>step=0, b=0, a=1)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Step(Function):
    def __init__(self, step = 0, b = 0, a=1): #f(x) = { b if x&lt;=step; a if x&gt;step}
        self.step = step
        self.b = b
        self.a = a
    
    def func(self, x, step = None, a = None, b = None):
        if step is None: step = self.step
        if a is None: a = self.a
        if b is None: b = self.b
        return np.where(x&gt; np.step, self.a, self.b)
    
    def deriv(self, x, step = None, a = None, b= None):
        if step is None: step = self.step
        if a is None: a = self.a
        if b is None: b = self.b
        return np.zeros(x.shape)
    
    def intfunc(self,x):
        if x &gt; self.step: return self.a
        return self.b
    
    def deriv(self,x):
        return 0</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Step.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self,x):
    return 0</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Step.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x, step=None, a=None, b=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, x, step = None, a = None, b = None):
    if step is None: step = self.step
    if a is None: a = self.a
    if b is None: b = self.b
    return np.where(x&gt; np.step, self.a, self.b)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Step.intfunc"><code class="name flex">
<span>def <span class="ident">intfunc</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def intfunc(self,x):
    if x &gt; self.step: return self.a
    return self.b</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.Tanh"><code class="flex name class">
<span>class <span class="ident">Tanh</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Tanh(Function):
    def func(self, x):
        z = np.exp(x)
        return (z - 1/z) / (z + 1/z)
    
    def deriv(self, x):
        return 1 - (self.func(x))**2</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Tanh.deriv"><code class="name flex">
<span>def <span class="ident">deriv</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def deriv(self, x):
    return 1 - (self.func(x))**2</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Tanh.func"><code class="name flex">
<span>def <span class="ident">func</span></span>(<span>self, x)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def func(self, x):
    z = np.exp(x)
    return (z - 1/z) / (z + 1/z)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.Utility"><code class="flex name class">
<span>class <span class="ident">Utility</span></span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Utility:

    def __init__(self):
        pass

    def label_encoding(self, Y):
        &#39;&#39;&#39;
        Parameters:
        Y: (batch,d) shape matrix with categorical data
        Return
        result: label encoded data of ùëå
        idx_list: list of the dictionaries containing the unique values
                  of the columns and their mapping to the integer.
        &#39;&#39;&#39;
        idx_list = []
        result = []
        for col in range(Y.shape[1]):
            indexes = {val: idx for idx, val in enumerate(np.unique(Y[:, col]))}
            result.append([indexes[s] for s in Y[:, col]])
            idx_list.append(indexes)
        return np.array(result).T, idx_list

    def onehot(self, X):
        &#39;&#39;&#39;
        Parameters:
        X: 1D array of labels of length &#34;batch&#34;
        Return
        X_onehot: (batch,d) one hot encoded matrix (one-hot of X)
                  (where d is the number of unique values in X)
        indexes: dictionary containing the unique values of X and their mapping to the integer column
        &#39;&#39;&#39;
        indexes = {val: idx for idx, val in enumerate(np.unique(X))}
        y = np.array([indexes[s] for s in X])
        X_onehot = np.zeros((y.size, len(indexes)))
        X_onehot[np.arange(y.size), y] = 1
        return X_onehot, indexes

    def minmax(self, X, min_X=None, max_X=None):
        if min_X is None:
            min_X = np.min(X, axis=0)
        if max_X is None:
            max_X = np.max(X, axis=0)
        Z = (X - min_X) / (max_X - min_X)
        return Z, min_X, max_X

    def standardize(self, X, mu=None, std=None):
        if mu is None:
            mu = np.mean(X, axis=0)
        if std is None:
            std = np.std(X, axis=0)
        Z = (X - mu) / std
        return Z, mu, std

    def inv_standardize(self, Z, mu, std):
        X = Z*std + mu
        return X

    def train_test_split(self, X, y, test_ratio=0.2, seed=None):
        if seed is not None:
            np.random.seed(seed)
        train_ratio = 1-test_ratio
        indices = np.random.permutation(X.shape[0])
        train_idx, test_idx = indices[:int(train_ratio*len(X))], indices[int(train_ratio*len(X)):]
        X_train, X_test = X[train_idx,:], X[test_idx,:]
        y_train, y_test = y[train_idx], y[test_idx]
        return X_train, X_test, y_train, y_test</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Utility.inv_standardize"><code class="name flex">
<span>def <span class="ident">inv_standardize</span></span>(<span>self, Z, mu, std)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def inv_standardize(self, Z, mu, std):
    X = Z*std + mu
    return X</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Utility.label_encoding"><code class="name flex">
<span>def <span class="ident">label_encoding</span></span>(<span>self, Y)</span>
</code></dt>
<dd>
<div class="desc"><p>Parameters:
Y: (batch,d) shape matrix with categorical data
Return
result: label encoded data of ùëå
idx_list: list of the dictionaries containing the unique values
of the columns and their mapping to the integer.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def label_encoding(self, Y):
    &#39;&#39;&#39;
    Parameters:
    Y: (batch,d) shape matrix with categorical data
    Return
    result: label encoded data of ùëå
    idx_list: list of the dictionaries containing the unique values
              of the columns and their mapping to the integer.
    &#39;&#39;&#39;
    idx_list = []
    result = []
    for col in range(Y.shape[1]):
        indexes = {val: idx for idx, val in enumerate(np.unique(Y[:, col]))}
        result.append([indexes[s] for s in Y[:, col]])
        idx_list.append(indexes)
    return np.array(result).T, idx_list</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Utility.minmax"><code class="name flex">
<span>def <span class="ident">minmax</span></span>(<span>self, X, min_X=None, max_X=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def minmax(self, X, min_X=None, max_X=None):
    if min_X is None:
        min_X = np.min(X, axis=0)
    if max_X is None:
        max_X = np.max(X, axis=0)
    Z = (X - min_X) / (max_X - min_X)
    return Z, min_X, max_X</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Utility.onehot"><code class="name flex">
<span>def <span class="ident">onehot</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Parameters:
X: 1D array of labels of length "batch"
Return
X_onehot: (batch,d) one hot encoded matrix (one-hot of X)
(where d is the number of unique values in X)
indexes: dictionary containing the unique values of X and their mapping to the integer column</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def onehot(self, X):
    &#39;&#39;&#39;
    Parameters:
    X: 1D array of labels of length &#34;batch&#34;
    Return
    X_onehot: (batch,d) one hot encoded matrix (one-hot of X)
              (where d is the number of unique values in X)
    indexes: dictionary containing the unique values of X and their mapping to the integer column
    &#39;&#39;&#39;
    indexes = {val: idx for idx, val in enumerate(np.unique(X))}
    y = np.array([indexes[s] for s in X])
    X_onehot = np.zeros((y.size, len(indexes)))
    X_onehot[np.arange(y.size), y] = 1
    return X_onehot, indexes</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Utility.standardize"><code class="name flex">
<span>def <span class="ident">standardize</span></span>(<span>self, X, mu=None, std=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def standardize(self, X, mu=None, std=None):
    if mu is None:
        mu = np.mean(X, axis=0)
    if std is None:
        std = np.std(X, axis=0)
    Z = (X - mu) / std
    return Z, mu, std</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Utility.train_test_split"><code class="name flex">
<span>def <span class="ident">train_test_split</span></span>(<span>self, X, y, test_ratio=0.2, seed=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def train_test_split(self, X, y, test_ratio=0.2, seed=None):
    if seed is not None:
        np.random.seed(seed)
    train_ratio = 1-test_ratio
    indices = np.random.permutation(X.shape[0])
    train_idx, test_idx = indices[:int(train_ratio*len(X))], indices[int(train_ratio*len(X)):]
    X_train, X_test = X[train_idx,:], X[test_idx,:]
    y_train, y_test = y[train_idx], y[test_idx]
    return X_train, X_test, y_train, y_test</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Rectifier.Weights_initializer"><code class="flex name class">
<span>class <span class="ident">Weights_initializer</span></span>
<span>(</span><span>shape, initializer_type=None, seed=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Parameters
shape: Shape of the weight matrix</p>
<p>initializer_type: type of weight initializer
available options are 'zeros', 'ones', 'random_normal', 'random_uniform',
'he_normal', 'xavier_normal' and 'glorot_normal'</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Weights_initializer:

    def __init__(self, shape, initializer_type=None, seed=None):
        &#39;&#39;&#39;
        Parameters
        shape: Shape of the weight matrix

        initializer_type: type of weight initializer
        available options are &#39;zeros&#39;, &#39;ones&#39;, &#39;random_normal&#39;, &#39;random_uniform&#39;,
        &#39;he_normal&#39;, &#39;xavier_normal&#39; and &#39;glorot_normal&#39;
        &#39;&#39;&#39;
        self.shape = shape
        if initializer_type is None:
            self.initializer_type = &#34;he_normal&#34;
        else:
            self.initializer_type = initializer_type
        self.seed = seed

    def zeros_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        return np.zeros(self.shape)

    def ones_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        return np.ones(self.shape)

    def random_normal_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        return np.random.normal(size=self.shape)

    def random_uniform_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        return np.random.uniform(size=self.shape)

    def he_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        try:
            F, kernelC, kernelH, kernelW = self.shape
        except:
            kernelH, kernelW = self.shape
        return np.random.randn(*self.shape) * np.sqrt(2/kernelH)

    def xavier_initializer(self):
        &#39;&#39;&#39;
        shape: Shape of the Kernel matrix.
        &#39;&#39;&#39;
        if self.seed is not None:
            np.random.seed(self.seed)
        try:
            F, kernelC, kernelH, kernelW = self.shape
        except:
            kernelH, kernelW = self.shape
        return np.random.randn(*self.shape) * np.sqrt(1/kernelH)

    def glorot_initializer(self):
        &#39;&#39;&#39;
        shape: Shape of the weight matrix.
        &#39;&#39;&#39;
        if self.seed is not None:
            np.random.seed(self.seed)
        try:
            F, kernelC, kernelH, kernelW = self.shape
        except:
            kernelH, kernelW = self.shape
        return np.random.randn(*self.shape) * np.sqrt(2/(kernelH+kernelW))

    def lecun_normal_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        return np.random.randn(*self.shape) * np.sqrt(1. / self.shape[1])

    def lecun_uniform_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        limit = np.sqrt(3. / self.shape[1])
        return np.random.uniform(-limit, limit, size=self.shape)

    def glorot_uniform_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        limit = np.sqrt(6. / (self.shape[0] + self.shape[1]))
        return np.random.uniform(-limit, limit, size=self.shape)
    
    def he_uniform_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        limit = np.sqrt(6. / self.shape[1])
        return np.random.uniform(-limit, limit, size=self.shape)

    def xavier_uniform_initializer(self):
        if self.seed is not None:
            np.random.seed(self.seed)
        limit = np.sqrt(3. / (self.shape[0] + self.shape[1]))
        return np.random.uniform(-limit, limit, size=self.shape)
    
    def constant_initializer(self, constant=0):
        if self.seed is not None:
            np.random.seed(self.seed)
        return np.full(self.shape, constant)

    def orthogonal_initializer(self, gain=1.0):
        if self.seed is not None:
            np.random.seed(self.seed)
        flat_shape = (self.shape[0], np.prod(self.shape[1:]))
        a = np.random.normal(0.0, 1.0, flat_shape)
        u, _, v = np.linalg.svd(a, full_matrices=False)
        q = u if u.shape == flat_shape else v
        return gain * q.reshape(self.shape)
    
    def variance_scaling_initializer(self, scale=1.0, mode=&#39;fan_in&#39;, distribution=&#39;normal&#39;):
        if self.seed is not None:
            np.random.seed(self.seed)
        fan_in, fan_out = self.shape[0], self.shape[1]
        if mode == &#39;fan_avg&#39;:
            fan_avg = (fan_in + fan_out) / 2.0
            scale /= max(1., fan_avg)
        elif mode == &#39;fan_in&#39;:
            scale /= max(1., fan_in)
        elif mode == &#39;fan_out&#39;:
            scale /= max(1., fan_out)
        else:
            raise ValueError(&#39;Invalid mode for variance scaling initializer: %s.&#39; % mode)
        if distribution == &#39;normal&#39;:
            stddev = np.sqrt(scale)
            return np.random.normal(0., stddev, size=self.shape)
        elif distribution == &#39;uniform&#39;:
            limit = np.sqrt(3. * scale)
            return np.random.uniform(-limit, limit, size=self.shape)
        else:
            raise ValueError(&#39;Invalid distribution for variance scaling initializer: %s.&#39; % distribution)
    
    def truncated_normal_initializer(self, mean=0.0, stddev=0.05):
        if self.seed is not None:
            np.random.seed(self.seed)
        values = np.random.normal(loc=mean, scale=stddev, size=self.shape)
        return np.clip(values, mean - 2*stddev, mean + 2*stddev)

    def identity_initializer(self, gain=1.0):
        if self.seed is not None:
            np.random.seed(self.seed)
        if len(self.shape) != 2 or self.shape[0] != self.shape[1]:
            raise ValueError(&#39;Identity matrix initializer can only be used for 2D square matrices.&#39;)
        else:
            return np.eye(self.shape[0]) * gain
        
    def uniform_initializer(self, minval=0, maxval=None):
        if self.seed is not None:
            np.random.seed(self.seed)
        return np.random.uniform(minval, maxval, size=self.shape)
    
    def normal_initializer(self, mean=0.0, stddev=1.0):
        if self.seed is not None:
            np.random.seed(self.seed)
        return np.random.normal(loc=mean, scale=stddev, size=self.shape)
    
    def get_initializer(self, *args):
        match self.initializer_type.lower():
            case &#39;zeros&#39;: return self.zeros_initializer(*args)
            case &#39;ones&#39;: return self.ones_initializer(*args)
            case &#39;random_normal&#39;: return self.random_normal_initializer(*args)
            case &#39;random_uniform&#39;: return self.random_uniform_initializer(*args)
            case &#39;he_normal&#39;: return self.he_initializer(*args)
            case &#39;glorot_normal&#39;: return self.glorot_initializer(*args)
            case &#39;xavier_normal&#39;: return self.xavier_initializer()
            case &#39;lecun_normal&#39;: return self.lecun_normal_initializer(*args)
            case &#39;lecun_uniform&#39;: return self.lecun_uniform_initializer(*args)
            case &#39;glorot_uniform&#39;: return self.glorot_uniform_initializer(*args)
            case &#39;he_uniform&#39;: return self.he_uniform_initializer(*args)
            case &#39;xavier_uniform&#39;: return self.xavier_uniform_initializer(*args)
            case &#39;constant&#39;: return self.constant_initializer(*args)
            case &#39;orthogonal&#39;: return self.orthogonal_initializer(*args)
            case &#39;variance_scaling&#39;: return self.variance_scaling_initializer(*args)
            case &#39;truncated_normal&#39;: return self.truncated_normal_initializer(*args)
            case &#39;identity&#39;: return self.identity_initializer(*args)
            case &#39;uniform&#39;: return self.uniform_initializer(*args)
            case &#39;normal&#39;: return self.normal_initializer(*args)
            case _: raise ValueError(&#34;Valid initializer options are &#39;zeros&#39;, &#39;ones&#39;, &#39;random_normal&#39;, &#39;random_uniform&#39;, &#39;he_normal&#39;, &#39;xavier_normal&#39;, and &#39;glorot_normal&#39;, ...&#34;)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Neural.Rectifier.Weights_initializer.constant_initializer"><code class="name flex">
<span>def <span class="ident">constant_initializer</span></span>(<span>self, constant=0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def constant_initializer(self, constant=0):
    if self.seed is not None:
        np.random.seed(self.seed)
    return np.full(self.shape, constant)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.get_initializer"><code class="name flex">
<span>def <span class="ident">get_initializer</span></span>(<span>self, *args)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_initializer(self, *args):
    match self.initializer_type.lower():
        case &#39;zeros&#39;: return self.zeros_initializer(*args)
        case &#39;ones&#39;: return self.ones_initializer(*args)
        case &#39;random_normal&#39;: return self.random_normal_initializer(*args)
        case &#39;random_uniform&#39;: return self.random_uniform_initializer(*args)
        case &#39;he_normal&#39;: return self.he_initializer(*args)
        case &#39;glorot_normal&#39;: return self.glorot_initializer(*args)
        case &#39;xavier_normal&#39;: return self.xavier_initializer()
        case &#39;lecun_normal&#39;: return self.lecun_normal_initializer(*args)
        case &#39;lecun_uniform&#39;: return self.lecun_uniform_initializer(*args)
        case &#39;glorot_uniform&#39;: return self.glorot_uniform_initializer(*args)
        case &#39;he_uniform&#39;: return self.he_uniform_initializer(*args)
        case &#39;xavier_uniform&#39;: return self.xavier_uniform_initializer(*args)
        case &#39;constant&#39;: return self.constant_initializer(*args)
        case &#39;orthogonal&#39;: return self.orthogonal_initializer(*args)
        case &#39;variance_scaling&#39;: return self.variance_scaling_initializer(*args)
        case &#39;truncated_normal&#39;: return self.truncated_normal_initializer(*args)
        case &#39;identity&#39;: return self.identity_initializer(*args)
        case &#39;uniform&#39;: return self.uniform_initializer(*args)
        case &#39;normal&#39;: return self.normal_initializer(*args)
        case _: raise ValueError(&#34;Valid initializer options are &#39;zeros&#39;, &#39;ones&#39;, &#39;random_normal&#39;, &#39;random_uniform&#39;, &#39;he_normal&#39;, &#39;xavier_normal&#39;, and &#39;glorot_normal&#39;, ...&#34;)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.glorot_initializer"><code class="name flex">
<span>def <span class="ident">glorot_initializer</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>shape: Shape of the weight matrix.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def glorot_initializer(self):
    &#39;&#39;&#39;
    shape: Shape of the weight matrix.
    &#39;&#39;&#39;
    if self.seed is not None:
        np.random.seed(self.seed)
    try:
        F, kernelC, kernelH, kernelW = self.shape
    except:
        kernelH, kernelW = self.shape
    return np.random.randn(*self.shape) * np.sqrt(2/(kernelH+kernelW))</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.glorot_uniform_initializer"><code class="name flex">
<span>def <span class="ident">glorot_uniform_initializer</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def glorot_uniform_initializer(self):
    if self.seed is not None:
        np.random.seed(self.seed)
    limit = np.sqrt(6. / (self.shape[0] + self.shape[1]))
    return np.random.uniform(-limit, limit, size=self.shape)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.he_initializer"><code class="name flex">
<span>def <span class="ident">he_initializer</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def he_initializer(self):
    if self.seed is not None:
        np.random.seed(self.seed)
    try:
        F, kernelC, kernelH, kernelW = self.shape
    except:
        kernelH, kernelW = self.shape
    return np.random.randn(*self.shape) * np.sqrt(2/kernelH)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.he_uniform_initializer"><code class="name flex">
<span>def <span class="ident">he_uniform_initializer</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def he_uniform_initializer(self):
    if self.seed is not None:
        np.random.seed(self.seed)
    limit = np.sqrt(6. / self.shape[1])
    return np.random.uniform(-limit, limit, size=self.shape)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.identity_initializer"><code class="name flex">
<span>def <span class="ident">identity_initializer</span></span>(<span>self, gain=1.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def identity_initializer(self, gain=1.0):
    if self.seed is not None:
        np.random.seed(self.seed)
    if len(self.shape) != 2 or self.shape[0] != self.shape[1]:
        raise ValueError(&#39;Identity matrix initializer can only be used for 2D square matrices.&#39;)
    else:
        return np.eye(self.shape[0]) * gain</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.lecun_normal_initializer"><code class="name flex">
<span>def <span class="ident">lecun_normal_initializer</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lecun_normal_initializer(self):
    if self.seed is not None:
        np.random.seed(self.seed)
    return np.random.randn(*self.shape) * np.sqrt(1. / self.shape[1])</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.lecun_uniform_initializer"><code class="name flex">
<span>def <span class="ident">lecun_uniform_initializer</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def lecun_uniform_initializer(self):
    if self.seed is not None:
        np.random.seed(self.seed)
    limit = np.sqrt(3. / self.shape[1])
    return np.random.uniform(-limit, limit, size=self.shape)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.normal_initializer"><code class="name flex">
<span>def <span class="ident">normal_initializer</span></span>(<span>self, mean=0.0, stddev=1.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def normal_initializer(self, mean=0.0, stddev=1.0):
    if self.seed is not None:
        np.random.seed(self.seed)
    return np.random.normal(loc=mean, scale=stddev, size=self.shape)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.ones_initializer"><code class="name flex">
<span>def <span class="ident">ones_initializer</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def ones_initializer(self):
    if self.seed is not None:
        np.random.seed(self.seed)
    return np.ones(self.shape)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.orthogonal_initializer"><code class="name flex">
<span>def <span class="ident">orthogonal_initializer</span></span>(<span>self, gain=1.0)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def orthogonal_initializer(self, gain=1.0):
    if self.seed is not None:
        np.random.seed(self.seed)
    flat_shape = (self.shape[0], np.prod(self.shape[1:]))
    a = np.random.normal(0.0, 1.0, flat_shape)
    u, _, v = np.linalg.svd(a, full_matrices=False)
    q = u if u.shape == flat_shape else v
    return gain * q.reshape(self.shape)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.random_normal_initializer"><code class="name flex">
<span>def <span class="ident">random_normal_initializer</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_normal_initializer(self):
    if self.seed is not None:
        np.random.seed(self.seed)
    return np.random.normal(size=self.shape)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.random_uniform_initializer"><code class="name flex">
<span>def <span class="ident">random_uniform_initializer</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def random_uniform_initializer(self):
    if self.seed is not None:
        np.random.seed(self.seed)
    return np.random.uniform(size=self.shape)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.truncated_normal_initializer"><code class="name flex">
<span>def <span class="ident">truncated_normal_initializer</span></span>(<span>self, mean=0.0, stddev=0.05)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def truncated_normal_initializer(self, mean=0.0, stddev=0.05):
    if self.seed is not None:
        np.random.seed(self.seed)
    values = np.random.normal(loc=mean, scale=stddev, size=self.shape)
    return np.clip(values, mean - 2*stddev, mean + 2*stddev)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.uniform_initializer"><code class="name flex">
<span>def <span class="ident">uniform_initializer</span></span>(<span>self, minval=0, maxval=None)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def uniform_initializer(self, minval=0, maxval=None):
    if self.seed is not None:
        np.random.seed(self.seed)
    return np.random.uniform(minval, maxval, size=self.shape)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.variance_scaling_initializer"><code class="name flex">
<span>def <span class="ident">variance_scaling_initializer</span></span>(<span>self, scale=1.0, mode='fan_in', distribution='normal')</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def variance_scaling_initializer(self, scale=1.0, mode=&#39;fan_in&#39;, distribution=&#39;normal&#39;):
    if self.seed is not None:
        np.random.seed(self.seed)
    fan_in, fan_out = self.shape[0], self.shape[1]
    if mode == &#39;fan_avg&#39;:
        fan_avg = (fan_in + fan_out) / 2.0
        scale /= max(1., fan_avg)
    elif mode == &#39;fan_in&#39;:
        scale /= max(1., fan_in)
    elif mode == &#39;fan_out&#39;:
        scale /= max(1., fan_out)
    else:
        raise ValueError(&#39;Invalid mode for variance scaling initializer: %s.&#39; % mode)
    if distribution == &#39;normal&#39;:
        stddev = np.sqrt(scale)
        return np.random.normal(0., stddev, size=self.shape)
    elif distribution == &#39;uniform&#39;:
        limit = np.sqrt(3. * scale)
        return np.random.uniform(-limit, limit, size=self.shape)
    else:
        raise ValueError(&#39;Invalid distribution for variance scaling initializer: %s.&#39; % distribution)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.xavier_initializer"><code class="name flex">
<span>def <span class="ident">xavier_initializer</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"><p>shape: Shape of the Kernel matrix.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def xavier_initializer(self):
    &#39;&#39;&#39;
    shape: Shape of the Kernel matrix.
    &#39;&#39;&#39;
    if self.seed is not None:
        np.random.seed(self.seed)
    try:
        F, kernelC, kernelH, kernelW = self.shape
    except:
        kernelH, kernelW = self.shape
    return np.random.randn(*self.shape) * np.sqrt(1/kernelH)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.xavier_uniform_initializer"><code class="name flex">
<span>def <span class="ident">xavier_uniform_initializer</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def xavier_uniform_initializer(self):
    if self.seed is not None:
        np.random.seed(self.seed)
    limit = np.sqrt(3. / (self.shape[0] + self.shape[1]))
    return np.random.uniform(-limit, limit, size=self.shape)</code></pre>
</details>
</dd>
<dt id="Neural.Rectifier.Weights_initializer.zeros_initializer"><code class="name flex">
<span>def <span class="ident">zeros_initializer</span></span>(<span>self)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def zeros_initializer(self):
    if self.seed is not None:
        np.random.seed(self.seed)
    return np.zeros(self.shape)</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="Neural" href="index.html">Neural</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="Neural.Rectifier.recPicker" href="#Neural.Rectifier.recPicker">recPicker</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="Neural.Rectifier.Abs" href="#Neural.Rectifier.Abs">Abs</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.Abs.deriv" href="#Neural.Rectifier.Abs.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.Abs.func" href="#Neural.Rectifier.Abs.func">func</a></code></li>
<li><code><a title="Neural.Rectifier.Abs.intderiv" href="#Neural.Rectifier.Abs.intderiv">intderiv</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.Activation" href="#Neural.Rectifier.Activation">Activation</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.Activation.backpropagation" href="#Neural.Rectifier.Activation.backpropagation">backpropagation</a></code></li>
<li><code><a title="Neural.Rectifier.Activation.forward" href="#Neural.Rectifier.Activation.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.BSigmoid" href="#Neural.Rectifier.BSigmoid">BSigmoid</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.BSigmoid.deriv" href="#Neural.Rectifier.BSigmoid.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.BSigmoid.func" href="#Neural.Rectifier.BSigmoid.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.CosReLU" href="#Neural.Rectifier.CosReLU">CosReLU</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.CosReLU.deriv" href="#Neural.Rectifier.CosReLU.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.CosReLU.func" href="#Neural.Rectifier.CosReLU.func">func</a></code></li>
<li><code><a title="Neural.Rectifier.CosReLU.intderiv" href="#Neural.Rectifier.CosReLU.intderiv">intderiv</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.Cosinus" href="#Neural.Rectifier.Cosinus">Cosinus</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.Cosinus.deriv" href="#Neural.Rectifier.Cosinus.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.Cosinus.func" href="#Neural.Rectifier.Cosinus.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.Cost" href="#Neural.Rectifier.Cost">Cost</a></code></h4>
<ul class="two-column">
<li><code><a title="Neural.Rectifier.Cost.cross_entropy" href="#Neural.Rectifier.Cost.cross_entropy">cross_entropy</a></code></li>
<li><code><a title="Neural.Rectifier.Cost.d_cross_entropy" href="#Neural.Rectifier.Cost.d_cross_entropy">d_cross_entropy</a></code></li>
<li><code><a title="Neural.Rectifier.Cost.d_exp" href="#Neural.Rectifier.Cost.d_exp">d_exp</a></code></li>
<li><code><a title="Neural.Rectifier.Cost.d_hinge" href="#Neural.Rectifier.Cost.d_hinge">d_hinge</a></code></li>
<li><code><a title="Neural.Rectifier.Cost.d_huber" href="#Neural.Rectifier.Cost.d_huber">d_huber</a></code></li>
<li><code><a title="Neural.Rectifier.Cost.d_kl" href="#Neural.Rectifier.Cost.d_kl">d_kl</a></code></li>
<li><code><a title="Neural.Rectifier.Cost.d_log" href="#Neural.Rectifier.Cost.d_log">d_log</a></code></li>
<li><code><a title="Neural.Rectifier.Cost.d_mse" href="#Neural.Rectifier.Cost.d_mse">d_mse</a></code></li>
<li><code><a title="Neural.Rectifier.Cost.d_weighted" href="#Neural.Rectifier.Cost.d_weighted">d_weighted</a></code></li>
<li><code><a title="Neural.Rectifier.Cost.exp" href="#Neural.Rectifier.Cost.exp">exp</a></code></li>
<li><code><a title="Neural.Rectifier.Cost.get_cost" href="#Neural.Rectifier.Cost.get_cost">get_cost</a></code></li>
<li><code><a title="Neural.Rectifier.Cost.get_d_cost" href="#Neural.Rectifier.Cost.get_d_cost">get_d_cost</a></code></li>
<li><code><a title="Neural.Rectifier.Cost.hinge" href="#Neural.Rectifier.Cost.hinge">hinge</a></code></li>
<li><code><a title="Neural.Rectifier.Cost.huber" href="#Neural.Rectifier.Cost.huber">huber</a></code></li>
<li><code><a title="Neural.Rectifier.Cost.kl" href="#Neural.Rectifier.Cost.kl">kl</a></code></li>
<li><code><a title="Neural.Rectifier.Cost.log" href="#Neural.Rectifier.Cost.log">log</a></code></li>
<li><code><a title="Neural.Rectifier.Cost.mse" href="#Neural.Rectifier.Cost.mse">mse</a></code></li>
<li><code><a title="Neural.Rectifier.Cost.names" href="#Neural.Rectifier.Cost.names">names</a></code></li>
<li><code><a title="Neural.Rectifier.Cost.weighted" href="#Neural.Rectifier.Cost.weighted">weighted</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.Function" href="#Neural.Rectifier.Function">Function</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.Function.deriv" href="#Neural.Rectifier.Function.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.Function.func" href="#Neural.Rectifier.Function.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.Gaussche" href="#Neural.Rectifier.Gaussche">Gaussche</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.Gaussche.deriv" href="#Neural.Rectifier.Gaussche.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.Gaussche.func" href="#Neural.Rectifier.Gaussche.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.HTanh" href="#Neural.Rectifier.HTanh">HTanh</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.HTanh.deriv" href="#Neural.Rectifier.HTanh.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.HTanh.func" href="#Neural.Rectifier.HTanh.func">func</a></code></li>
<li><code><a title="Neural.Rectifier.HTanh.intderiv" href="#Neural.Rectifier.HTanh.intderiv">intderiv</a></code></li>
<li><code><a title="Neural.Rectifier.HTanh.intfunc" href="#Neural.Rectifier.HTanh.intfunc">intfunc</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.IMultiquatratisch" href="#Neural.Rectifier.IMultiquatratisch">IMultiquatratisch</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.IMultiquatratisch.deriv" href="#Neural.Rectifier.IMultiquatratisch.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.IMultiquatratisch.func" href="#Neural.Rectifier.IMultiquatratisch.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.KLL" href="#Neural.Rectifier.KLL">KLL</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.KLL.deriv" href="#Neural.Rectifier.KLL.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.KLL.func" href="#Neural.Rectifier.KLL.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.LCTanh" href="#Neural.Rectifier.LCTanh">LCTanh</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.LCTanh.deriv" href="#Neural.Rectifier.LCTanh.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.LCTanh.func" href="#Neural.Rectifier.LCTanh.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.LReLU" href="#Neural.Rectifier.LReLU">LReLU</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.LReLU.deriv" href="#Neural.Rectifier.LReLU.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.LReLU.func" href="#Neural.Rectifier.LReLU.func">func</a></code></li>
<li><code><a title="Neural.Rectifier.LReLU.intderiv" href="#Neural.Rectifier.LReLU.intderiv">intderiv</a></code></li>
<li><code><a title="Neural.Rectifier.LReLU.intfunc" href="#Neural.Rectifier.LReLU.intfunc">intfunc</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.LearningRateDecay" href="#Neural.Rectifier.LearningRateDecay">LearningRateDecay</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.LearningRateDecay.burnin_decay" href="#Neural.Rectifier.LearningRateDecay.burnin_decay">burnin_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.constant" href="#Neural.Rectifier.LearningRateDecay.constant">constant</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.cosine_annealing_decay" href="#Neural.Rectifier.LearningRateDecay.cosine_annealing_decay">cosine_annealing_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.cosine_decay" href="#Neural.Rectifier.LearningRateDecay.cosine_decay">cosine_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.cosine_decay_restarts" href="#Neural.Rectifier.LearningRateDecay.cosine_decay_restarts">cosine_decay_restarts</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.cyclic_decay" href="#Neural.Rectifier.LearningRateDecay.cyclic_decay">cyclic_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.decay_on_plateau" href="#Neural.Rectifier.LearningRateDecay.decay_on_plateau">decay_on_plateau</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.exp_range_decay" href="#Neural.Rectifier.LearningRateDecay.exp_range_decay">exp_range_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.exponential_decay" href="#Neural.Rectifier.LearningRateDecay.exponential_decay">exponential_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.inverse_time_decay" href="#Neural.Rectifier.LearningRateDecay.inverse_time_decay">inverse_time_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.linear_cosine_decay" href="#Neural.Rectifier.LearningRateDecay.linear_cosine_decay">linear_cosine_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.natural_exp_decay" href="#Neural.Rectifier.LearningRateDecay.natural_exp_decay">natural_exp_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.one_cycle_policy" href="#Neural.Rectifier.LearningRateDecay.one_cycle_policy">one_cycle_policy</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.piecewise_constant_decay" href="#Neural.Rectifier.LearningRateDecay.piecewise_constant_decay">piecewise_constant_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.polynomial_decay" href="#Neural.Rectifier.LearningRateDecay.polynomial_decay">polynomial_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.power_decay" href="#Neural.Rectifier.LearningRateDecay.power_decay">power_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.sqrt_decay" href="#Neural.Rectifier.LearningRateDecay.sqrt_decay">sqrt_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.staircase_decay" href="#Neural.Rectifier.LearningRateDecay.staircase_decay">staircase_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.step_decay" href="#Neural.Rectifier.LearningRateDecay.step_decay">step_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.time_decay" href="#Neural.Rectifier.LearningRateDecay.time_decay">time_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.triangular2_learning_rate" href="#Neural.Rectifier.LearningRateDecay.triangular2_learning_rate">triangular2_learning_rate</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.triangular2_lr_decay" href="#Neural.Rectifier.LearningRateDecay.triangular2_lr_decay">triangular2_lr_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.triangular_learning_rate" href="#Neural.Rectifier.LearningRateDecay.triangular_learning_rate">triangular_learning_rate</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.triangular_lr_decay" href="#Neural.Rectifier.LearningRateDecay.triangular_lr_decay">triangular_lr_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.warm_restart_decay" href="#Neural.Rectifier.LearningRateDecay.warm_restart_decay">warm_restart_decay</a></code></li>
<li><code><a title="Neural.Rectifier.LearningRateDecay.warmup_decay" href="#Neural.Rectifier.LearningRateDecay.warmup_decay">warmup_decay</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.Linear" href="#Neural.Rectifier.Linear">Linear</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.Linear.deriv" href="#Neural.Rectifier.Linear.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.Linear.func" href="#Neural.Rectifier.Linear.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.Linear2" href="#Neural.Rectifier.Linear2">Linear2</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.Linear2.deriv" href="#Neural.Rectifier.Linear2.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.Linear2.func" href="#Neural.Rectifier.Linear2.func">func</a></code></li>
<li><code><a title="Neural.Rectifier.Linear2.intderiv" href="#Neural.Rectifier.Linear2.intderiv">intderiv</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.Logit" href="#Neural.Rectifier.Logit">Logit</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.Logit.deriv" href="#Neural.Rectifier.Logit.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.Logit.func" href="#Neural.Rectifier.Logit.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.Multiquatratisch" href="#Neural.Rectifier.Multiquatratisch">Multiquatratisch</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.Multiquatratisch.deriv" href="#Neural.Rectifier.Multiquatratisch.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.Multiquatratisch.func" href="#Neural.Rectifier.Multiquatratisch.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.NoDeriv" href="#Neural.Rectifier.NoDeriv">NoDeriv</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.NoDeriv.change" href="#Neural.Rectifier.NoDeriv.change">change</a></code></li>
<li><code><a title="Neural.Rectifier.NoDeriv.deriv" href="#Neural.Rectifier.NoDeriv.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.NoDeriv.func" href="#Neural.Rectifier.NoDeriv.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.Normal" href="#Neural.Rectifier.Normal">Normal</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.Normal.deriv" href="#Neural.Rectifier.Normal.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.Normal.func" href="#Neural.Rectifier.Normal.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.Optimizer" href="#Neural.Rectifier.Optimizer">Optimizer</a></code></h4>
<ul class="two-column">
<li><code><a title="Neural.Rectifier.Optimizer.AdaMax" href="#Neural.Rectifier.Optimizer.AdaMax">AdaMax</a></code></li>
<li><code><a title="Neural.Rectifier.Optimizer.Adadelta" href="#Neural.Rectifier.Optimizer.Adadelta">Adadelta</a></code></li>
<li><code><a title="Neural.Rectifier.Optimizer.Adagrad" href="#Neural.Rectifier.Optimizer.Adagrad">Adagrad</a></code></li>
<li><code><a title="Neural.Rectifier.Optimizer.Adam" href="#Neural.Rectifier.Optimizer.Adam">Adam</a></code></li>
<li><code><a title="Neural.Rectifier.Optimizer.GD" href="#Neural.Rectifier.Optimizer.GD">GD</a></code></li>
<li><code><a title="Neural.Rectifier.Optimizer.NAG" href="#Neural.Rectifier.Optimizer.NAG">NAG</a></code></li>
<li><code><a title="Neural.Rectifier.Optimizer.Nadam" href="#Neural.Rectifier.Optimizer.Nadam">Nadam</a></code></li>
<li><code><a title="Neural.Rectifier.Optimizer.RMSProp" href="#Neural.Rectifier.Optimizer.RMSProp">RMSProp</a></code></li>
<li><code><a title="Neural.Rectifier.Optimizer.Rprop" href="#Neural.Rectifier.Optimizer.Rprop">Rprop</a></code></li>
<li><code><a title="Neural.Rectifier.Optimizer.SGD" href="#Neural.Rectifier.Optimizer.SGD">SGD</a></code></li>
<li><code><a title="Neural.Rectifier.Optimizer.get_optimization" href="#Neural.Rectifier.Optimizer.get_optimization">get_optimization</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.PwLinear" href="#Neural.Rectifier.PwLinear">PwLinear</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.PwLinear.deriv" href="#Neural.Rectifier.PwLinear.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.PwLinear.func" href="#Neural.Rectifier.PwLinear.func">func</a></code></li>
<li><code><a title="Neural.Rectifier.PwLinear.intderiv" href="#Neural.Rectifier.PwLinear.intderiv">intderiv</a></code></li>
<li><code><a title="Neural.Rectifier.PwLinear.intfunc" href="#Neural.Rectifier.PwLinear.intfunc">intfunc</a></code></li>
<li><code><a title="Neural.Rectifier.PwLinear.update" href="#Neural.Rectifier.PwLinear.update">update</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.ReLU" href="#Neural.Rectifier.ReLU">ReLU</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.ReLU.deriv" href="#Neural.Rectifier.ReLU.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.ReLU.func" href="#Neural.Rectifier.ReLU.func">func</a></code></li>
<li><code><a title="Neural.Rectifier.ReLU.intderiv" href="#Neural.Rectifier.ReLU.intderiv">intderiv</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.SReLU" href="#Neural.Rectifier.SReLU">SReLU</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.SReLU.deriv" href="#Neural.Rectifier.SReLU.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.SReLU.func" href="#Neural.Rectifier.SReLU.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.Sigmoid" href="#Neural.Rectifier.Sigmoid">Sigmoid</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.Sigmoid.deriv" href="#Neural.Rectifier.Sigmoid.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.Sigmoid.func" href="#Neural.Rectifier.Sigmoid.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.SinReLU" href="#Neural.Rectifier.SinReLU">SinReLU</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.SinReLU.deriv" href="#Neural.Rectifier.SinReLU.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.SinReLU.func" href="#Neural.Rectifier.SinReLU.func">func</a></code></li>
<li><code><a title="Neural.Rectifier.SinReLU.intderiv" href="#Neural.Rectifier.SinReLU.intderiv">intderiv</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.Sinus" href="#Neural.Rectifier.Sinus">Sinus</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.Sinus.deriv" href="#Neural.Rectifier.Sinus.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.Sinus.func" href="#Neural.Rectifier.Sinus.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.Softmax" href="#Neural.Rectifier.Softmax">Softmax</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.Softmax.deriv" href="#Neural.Rectifier.Softmax.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.Softmax.func" href="#Neural.Rectifier.Softmax.func">func</a></code></li>
<li><code><a title="Neural.Rectifier.Softmax.softmax_grad" href="#Neural.Rectifier.Softmax.softmax_grad">softmax_grad</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.Step" href="#Neural.Rectifier.Step">Step</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.Step.deriv" href="#Neural.Rectifier.Step.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.Step.func" href="#Neural.Rectifier.Step.func">func</a></code></li>
<li><code><a title="Neural.Rectifier.Step.intfunc" href="#Neural.Rectifier.Step.intfunc">intfunc</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.Tanh" href="#Neural.Rectifier.Tanh">Tanh</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.Tanh.deriv" href="#Neural.Rectifier.Tanh.deriv">deriv</a></code></li>
<li><code><a title="Neural.Rectifier.Tanh.func" href="#Neural.Rectifier.Tanh.func">func</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.Utility" href="#Neural.Rectifier.Utility">Utility</a></code></h4>
<ul class="two-column">
<li><code><a title="Neural.Rectifier.Utility.inv_standardize" href="#Neural.Rectifier.Utility.inv_standardize">inv_standardize</a></code></li>
<li><code><a title="Neural.Rectifier.Utility.label_encoding" href="#Neural.Rectifier.Utility.label_encoding">label_encoding</a></code></li>
<li><code><a title="Neural.Rectifier.Utility.minmax" href="#Neural.Rectifier.Utility.minmax">minmax</a></code></li>
<li><code><a title="Neural.Rectifier.Utility.onehot" href="#Neural.Rectifier.Utility.onehot">onehot</a></code></li>
<li><code><a title="Neural.Rectifier.Utility.standardize" href="#Neural.Rectifier.Utility.standardize">standardize</a></code></li>
<li><code><a title="Neural.Rectifier.Utility.train_test_split" href="#Neural.Rectifier.Utility.train_test_split">train_test_split</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Rectifier.Weights_initializer" href="#Neural.Rectifier.Weights_initializer">Weights_initializer</a></code></h4>
<ul class="">
<li><code><a title="Neural.Rectifier.Weights_initializer.constant_initializer" href="#Neural.Rectifier.Weights_initializer.constant_initializer">constant_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.get_initializer" href="#Neural.Rectifier.Weights_initializer.get_initializer">get_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.glorot_initializer" href="#Neural.Rectifier.Weights_initializer.glorot_initializer">glorot_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.glorot_uniform_initializer" href="#Neural.Rectifier.Weights_initializer.glorot_uniform_initializer">glorot_uniform_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.he_initializer" href="#Neural.Rectifier.Weights_initializer.he_initializer">he_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.he_uniform_initializer" href="#Neural.Rectifier.Weights_initializer.he_uniform_initializer">he_uniform_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.identity_initializer" href="#Neural.Rectifier.Weights_initializer.identity_initializer">identity_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.lecun_normal_initializer" href="#Neural.Rectifier.Weights_initializer.lecun_normal_initializer">lecun_normal_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.lecun_uniform_initializer" href="#Neural.Rectifier.Weights_initializer.lecun_uniform_initializer">lecun_uniform_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.normal_initializer" href="#Neural.Rectifier.Weights_initializer.normal_initializer">normal_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.ones_initializer" href="#Neural.Rectifier.Weights_initializer.ones_initializer">ones_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.orthogonal_initializer" href="#Neural.Rectifier.Weights_initializer.orthogonal_initializer">orthogonal_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.random_normal_initializer" href="#Neural.Rectifier.Weights_initializer.random_normal_initializer">random_normal_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.random_uniform_initializer" href="#Neural.Rectifier.Weights_initializer.random_uniform_initializer">random_uniform_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.truncated_normal_initializer" href="#Neural.Rectifier.Weights_initializer.truncated_normal_initializer">truncated_normal_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.uniform_initializer" href="#Neural.Rectifier.Weights_initializer.uniform_initializer">uniform_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.variance_scaling_initializer" href="#Neural.Rectifier.Weights_initializer.variance_scaling_initializer">variance_scaling_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.xavier_initializer" href="#Neural.Rectifier.Weights_initializer.xavier_initializer">xavier_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.xavier_uniform_initializer" href="#Neural.Rectifier.Weights_initializer.xavier_uniform_initializer">xavier_uniform_initializer</a></code></li>
<li><code><a title="Neural.Rectifier.Weights_initializer.zeros_initializer" href="#Neural.Rectifier.Weights_initializer.zeros_initializer">zeros_initializer</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>