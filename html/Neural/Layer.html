<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>Neural.Layer API documentation</title>
<meta name="description" content="Diese Modul Enthält die Klassen für dei Schichten eines Neuronalen Netzes.
definierte Schichten:
-Dense
Vollvermaschte/Dens Schicht
…" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>Neural.Layer</code></h1>
</header>
<section id="section-intro">
<p>Diese Modul Enthält die Klassen für dei Schichten eines Neuronalen Netzes.
definierte Schichten:
-Dense
Vollvermaschte/Dens Schicht
-Conv2D
Convolutional Layer
-Pooling2D
Pooling Layer (min/mean/max)
-Dropout
-Dropout Layer
-Flatten
- Flatten Layer
-Padding
-Padding Layer (used for Conv2D, Pooling2D)</p>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">&#39;&#39;&#39;
Diese Modul Enthält die Klassen für dei Schichten eines Neuronalen Netzes. 
definierte Schichten:
    -Dense
        Vollvermaschte/Dens Schicht
    -Conv2D
        Convolutional Layer
    -Pooling2D
        Pooling Layer (min/mean/max)
    -Dropout
        -Dropout Layer
    -Flatten
        - Flatten Layer
    -Padding
        -Padding Layer (used for Conv2D, Pooling2D)
&#39;&#39;&#39;


# numpy for linear algebra
import numpy as np

#Utility functions
from .Rectifier import *






# #### [Padding2D class]
#Klasse Padding2D: Fügt einen Ramen/Padding zu einem Bild hinzu
class Padding2D:
    &#34;&#34;&#34;
    Eine Klasse, die eine Padding-Schicht für eine Faltungsschicht implementiert.

    Attributes
    ----------
    padding : {&#39;same&#39;, &#39;valid&#39;} or int or tuple of int
        Der Padding-Typ. Erlaubte Typen sind nur &#39;same&#39;, &#39;valid&#39;, eine Ganzzahl oder ein Tupel der Länge 2.
    input_shape : tuple of int
        Die Form der Eingabe zur Padding2D-Schicht
    output_shape : tuple of int
        Die Form der Ausgabe nach dem Padding
    padT : int
        Die Menge an Padding oben
    padB : int
        Die Menge an Padding unten
    padL : int
        Die Menge an Padding links
    padR : int
        Die Menge an Padding rechts

    Methods
    -------
    __init__(padding=&#39;valid&#39;)
        Der Konstruktor der Padding2D-Klasse. Initialisiert den Padding-Typ.
    get_dimensions(input_shape, kernel_size=None, stride=(1,1))
        Eine Hilfsfunktion, die die Dimension der Ausgabe nach dem Padding bestimmt.
    forward(X, kernel_size, stride=(1,1))
        Eine Funktion, die die Vorwärtspropagation durchführt. Sie berechnet die Menge an Padding, die benötigt wird, und wendet das Padding auf die Eingabe X an.
    backpropagation(dXp)
        Führt die Rückwärtspropagation durch. Sie entfernt das Padding vom Gradienten der Ausgabe.
    &#34;&#34;&#34;

    def __init__(self, padding=&#39;valid&#39;):
        &#34;&#34;&#34;
        Der Konstruktor der Padding2D-Klasse. Initialisiert den Padding-Typ.

        Parameters
        ----------
        padding : {&#39;same&#39;, &#39;valid&#39;} or int or tuple of int, optional
            Der Padding-Typ. Erlaubte Typen sind nur &#39;same&#39;, &#39;valid&#39;, eine Ganzzahl oder ein Tupel der Länge 2. Standardwert ist &#39;valid&#39;.
        &#34;&#34;&#34;
        self.padding = padding

    def get_dimensions(self, input_shape, kernel_size = None, stride=(1,1)):
        &#34;&#34;&#34;
        Eine Hilfsfunktion, die die Dimension der Ausgabe nach dem Padding bestimmt.

        Parameters
        ----------
        input_shape : tuple of int
            Die Form der Eingabe zur Padding2D-Schicht
        kernel_size : tuple of int, optional
            Die Größe des Kernels. Standardwert ist None.
        stride : tuple of int, optional
            Die Schritte entlang der Höhe und Breite (strideH, strideW). Standardwert ist (1, 1).

        Returns
        -------
        output_shape : tuple of int
            Die Form der Ausgabe nach dem Padding
        (padT, padB, padL, padR) : tuple of int
            Ein Tupel, das die Menge an Padding in alle vier Richtungen (oben, unten, links, rechts) angibt
        &#34;&#34;&#34;
        if len(input_shape)==4:
            batch, inputC, inputH, inputW = input_shape
        elif len(input_shape)==3:
            inputC, inputH, inputW = input_shape

        
        strideH, strideW = stride
        padding = self.padding

        if type(padding)==int:
            padT, padB = padding, padding
            padL, padR = padding, padding

        if type(padding)==tuple:
            padH, padW = padding
            padT, padB = padH//2, (padH+1)//2
            padL, padR = padW//2, (padW+1)//2

        elif padding==&#39;valid&#39;:
            padT, padB = 0, 0
            padL, padR = 0, 0

        elif padding==&#39;same&#39;:
            # calculating how much padding is required in all 4 directions
            # (top, bottom, left and right)
            if kernel_size is None: raise ValueError(&#34;Kernel cannot be None if padding is same&#34;)
            kernelH, kernelW = kernel_size
            padH = (strideH-1)*inputH + kernelH - strideH
            padW = (strideW-1)*inputW + kernelW - strideW

            padT, padB = padH//2, (padH+1)//2
            padL, padR = padW//2, (padW+1)//2

        else:
            raise ValueError(&#34;Incorrect padding type. Allowed types are only &#39;same&#39;, &#39;valid&#39;, an integer or a tuple of length 2.&#34;)

        if len(input_shape)==4:
            output_shape = (batch, inputC, inputH+padT+padB, inputW+padL+padR)
        elif len(input_shape)==3:
            output_shape = (inputC, inputH+padT+padB, inputW+padL+padR)
        return output_shape, (padT, padB, padL, padR)

    def forward(self, X, kernel_size, stride=(1,1)):
        &#34;&#34;&#34;
        Eine Funktion, die die Vorwärtspropagation durchführt. Sie berechnet die Menge an Padding, die benötigt wird, und wendet das Padding auf die Eingabe X an.

        Parameters
        ----------
        X : numpy.ndarray
            Die Eingabe mit Form (batch, inputC, inputH, inputW)
        kernel_size : tuple of int
            Die Kernelgröße wie in Conv2D-Schicht angegeben
        stride : tuple of int, optional
            Die Schritte entlang der Höhe und Breite (strideH, strideW). Standardwert ist (1, 1).

        Returns
        -------
        Xp : numpy.ndarray
            Das gepaddete X mit Form (batch, inputC, inputH+padT+padB, inputW+padL+padR)
        &#34;&#34;&#34;
        self.input_shape = X.shape
        batch, inputC, inputH, inputW = self.input_shape

        self.output_shape, (self.padT, self.padB, self.padL, self.padR) = self.get_dimensions(self.input_shape,
                                                                                      kernel_size, stride=stride)

        zeros_r = np.zeros((batch, inputC, inputH, self.padR))
        zeros_l = np.zeros((batch, inputC, inputH, self.padL))
        zeros_t = np.zeros((batch, inputC, self.padT, inputW + self.padL + self.padR))
        zeros_b = np.zeros((batch, inputC, self.padB, inputW + self.padL + self.padR))

        Xp = np.concatenate((X, zeros_r), axis=3)
        Xp = np.concatenate((zeros_l, Xp), axis=3)
        Xp = np.concatenate((zeros_t, Xp), axis=2)
        Xp = np.concatenate((Xp, zeros_b), axis=2)

        return Xp

    def backpropagation(self, dXp):
        &#34;&#34;&#34;
        Führt die Rückwärtspropagation durch. Sie entfernt das Padding vom Gradienten der Ausgabe.

        Parameters
        ----------
        dXp : numpy.ndarray
            Der Backprop-Fehler von gepaddetem X (Xp) mit Form (batch, inputC, inputH+padT+padB, inputW+padL+padR)

        Returns
        -------
        dX : numpy.ndarray
            Der Backprop-Fehler von X mit Form (batch, inputC, inputH, inputW)

        Notes
        -----
        Diese Methode verwendet die Attribute `input_shape`, `padT`, `padB`, `padL` und `padR`, die von der `forward`-Methode gesetzt wurden, um den Gradienten der Ausgabe dXp zu schneiden und das Padding zu entfernen. Sie gibt den Gradienten der Eingabe dX zurück, der die gleiche Form wie die Eingabe hat.
        &#34;&#34;&#34;
        batch, inputC, inputH, inputW = self.input_shape
        dX = dXp[:, :, self.padT:self.padT+inputH, self.padL:self.padL+inputW]
        return dX


# #### [Convolution2D class]
class Conv2D:
    
    def __init__(self, filters, kernel_size, stride=(1, 1), padding=&#39;valid&#39;,
             activation_type=None, use_bias=True, weight_initializer_type=None,
             kernel_regularizer=None, seed=None, input_shape=None, bias = None):
            &#39;&#39;&#39;
            Der Konstruktor der Conv2D-Klasse. Initialisiert die Parameter der Faltungsschicht.

            Parameters
            ----------
            filters : int
                Anzahl der Ausgabefilter in der Faltung (F).
            kernel_size : int oder tuple of int
                Höhe und Breite des 2D-Faltungsfensters (kernelH, kernelW).
            stride : int oder tuple of int, optional
                Schritte entlang der Höhe und Breite (strideH, strideW). Standardwert ist (1, 1).
            padding : {&#39;same&#39;, &#39;valid&#39;} oder int oder tuple of int, optional
                Polsterungstyp. Standardwert ist &#39;valid&#39;.
            activation_type : {&#39;sigmoid&#39;, &#39;linear&#39;, &#39;tanh&#39;, &#39;softmax&#39;, &#39;prelu&#39;, &#39;relu&#39;} oder None, optional
                Art der Aktivierung. Standardwert ist None.
            use_bias : bool, optional
                Gibt an, ob eine Bias-Vektor verwendet wird. Standardwert ist True.
            weight_initializer_type : str oder None, optional
                Initialisierer für die Gewichtsmatrix des Kernels. Standardwert ist None.
            kernel_regularizer : tuple of (str, float) oder None, optional
                Regularisierungsfunktion für die Kernelmatrix. Standardwert ist None.
            seed : int oder None, optional
                Für reproduzierbare Ergebnisse. Standardwert ist None.
            input_shape : tuple of int oder None, optional
                Größe des Inputs (batch, inputC, inputH, inputW). Standardwert ist None.

            Notes
            -----
            Diese Klasse implementiert eine zweidimensionale Faltungsschicht, die eine lineare Transformation auf einem lokalen Bereich der Eingabe anwendet. Die Faltung wird durch eine Gewichtsmatrix (Kernel) und einen Bias-Vektor charakterisiert, die während des Trainings gelernt werden. Die Faltung kann auch eine Aktivierungsfunktion, eine Polsterung und eine Regularisierung enthalten, um die Leistung zu verbessern.

            
            
            &#39;&#39;&#39;

            # Verwendung von assert-Anweisungen zur Überprüfung der Eingabetypen
            #assert isinstance(filters, int), &#34;filters muss ein Integer sein&#34;
            assert isinstance(kernel_size, (int, tuple)), &#34;kernel_size muss ein Integer oder ein Tupel sein&#34;
            assert isinstance(stride, (int, tuple)), &#34;s muss ein Integer oder ein Tupel sein&#34;
            assert padding in [&#39;same&#39;, &#39;valid&#39;] or isinstance(padding, (int, tuple)), &#34;padding muss &#39;same&#39;, &#39;valid&#39;, ein Integer oder ein Tupel sein&#34;
            assert activation_type in [None, &#39;sigmoid&#39;, &#39;linear&#39;, &#39;tanh&#39;, &#39;softmax&#39;, &#39;prelu&#39;, &#39;relu&#39;], &#34;Ungültiger activation_type&#34;
            assert isinstance(use_bias, bool), &#34;use_bias muss ein Boolean sein&#34;
            assert kernel_regularizer is None or isinstance(kernel_regularizer, tuple), &#34;kernel_regularizer muss ein Tupel sein&#34;
            assert input_shape is None or isinstance(input_shape, tuple), &#34;input_shape muss ein Tupel sein&#34;

            self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size
            self.stride = (stride, stride) if isinstance(stride, int) else stride
            self.padding_type = padding
            self.padding = Padding2D(padding=padding)
            self.activation_type = activation_type
            self.activation = Activation(activation_type=activation_type)
            self.use_bias = use_bias
            self.weight_initializer_type = weight_initializer_type
            self.kernel_regularizer = kernel_regularizer if kernel_regularizer is not None else (&#39;L2&#39;, 0)
            self.seed = seed
            self.input_shape_x = input_shape
            
            #extract kernel:
            if isinstance(filters, int):
                self.F = filters
                self.Kernel = None
            elif isinstance(filters, np.ndarray):
                self.kernel_size = filters.shape[-2:]
                self.F = filters.shape[0]
                self.Kernel = filters
            else: raise ValueError(&#34;Given Filter has invalide type: &#34;, type(filters),&#34;.&#34;)
            
            if bias is None:
                self.bias = None
            elif isinstance(bias, np.ndarray):
                self.bias = bias
            else: raise ValueError(&#34;Given Filter has invalide type: &#34;, type(filters),&#34;.&#34;)
                

            # Extrahieren der Höhe und Breite aus kernel_size und stride
            self.kernelH, self.kernelW = self.kernel_size
            self.strideH, self.strideW = self.stride
            self.inputC, self.inputH, self.inputW = 0, 0, 0
    
    def get_dimensions(self, input_shape):
        &#34;&#34;&#34;
        Berechnet die Dimensionen des Outputs basierend auf dem Input-Shape.

        Parameters
        ----------
        input_shape : tuple of int
            Die Form des Inputs (3D oder 4D).

        Returns
        -------
        output_shape : tuple of int
            Die Form des Outputs (3D oder 4D).

        Notes
        -----
        Diese Funktion speichert das Input-Shape und das Output-Shape als Attribute der Klasse. Sie berücksichtigt auch das Padding, das für die Faltung verwendet wird, und berechnet die Höhe und Breite des Outputs anhand des Kernels und des Schritts. Sie passt das Output-Shape an die Länge des Input-Shapes an, indem sie die Batch- und Kanalgrößen beibehält oder hinzufügt.
        &#34;&#34;&#34;

        # Speichern des Input-Shapes
        self.input_shape_x = input_shape

        # Berechnung des Input-Shapes mit Padding, das tatsächlich für diese Conv2D verwendet wird
        self.input_shape, _ = self.padding.get_dimensions(self.input_shape_x, self.kernel_size, self.stride)

        # Entpacken des Input-Shapes in die entsprechenden Variablen
        *shape, self.inputC, self.inputH, self.inputW = self.input_shape

        # Berechnung der Output-Dimensionen
        self.outH = (self.inputH - self.kernelH) // self.strideH + 1
        self.outW = (self.inputW - self.kernelW) // self.strideW + 1

        # Zuweisung des Output-Shapes basierend auf der Länge des Input-Shapes
        self.output_shape = (*shape, self.inputC,  self.F, self.outH, self.outW) if len(input_shape) == 4 else (self.F, self.outH, self.outW)

    def initialize_parameters(self, input_shape, optimizer_type):
        &#34;&#34;&#34;
        Diese Funktion initialisiert die Parameter der Faltungsschicht.

        Parameters
        ----------
        input_shape : tuple of int
            Form der Eingabe zur Conv2D-Schicht
        optimizer_type : str
            Art des Optimierers

        Returns
        -------
        None
            Diese Funktion gibt nichts zurück, sondern setzt die Attribute der Klasse.

        Notes
        -----
        Diese Funktion ruft die Methode `get_dimensions` auf, um die Dimensionen des Outputs basierend auf dem Input-Shape zu berechnen. Sie erstellt dann die Gewichtsmatrix (Kernel) und den Bias-Vektor mit der Klasse `Weights_initializer` und dem angegebenen Initialisierer-Typ. Sie initialisiert auch den Optimizer mit der Klasse `Optimizer` und dem angegebenen Optimizer-Typ. Sie speichert alle diese Parameter als Attribute der Klasse.
        &#34;&#34;&#34;
        self.get_dimensions(input_shape)

        shapebias = (self.F, self.outH, self.outW)

        if self.Kernel is None:
            shape_Kernel = (self.F, self.inputC, self.kernelH, self.kernelW)

            initializer = Weights_initializer(shape=shape_Kernel, initializer_type=self.weight_initializer_type, seed=self.seed)

            self.Kernel = initializer.get_initializer()
        if self.bias is None:
            self.bias = np.zeros(shapebias)

        self.optimizer = Optimizer(optimizer_type=optimizer_type, shape_W=shape_Kernel, shape_b=shapebias)

    def dilate2D(self, X, Dr=(1,1)):
        &#34;&#34;&#34;
        Vergrößert die Eingabe X entlang der Höhe und Breite mit einem gegebenen Dilatationsfaktor.

        Parameters
        ----------
        X : numpy.ndarray
            Eingabedaten mit Form (batch, C, H, W)
        Dr : tuple of int, optional
            Dilatationsfaktor entlang der Höhe und Breite (dh, dw). Standardwert ist (1, 1).

        Returns
        -------
        Xd : numpy.ndarray
            Dilatierte Eingabe mit Form (batch, C, H*dh, W*dw)

        Notes
        -----
        Diese Funktion verwendet die numpy.repeat-Funktion, um die Eingabe entlang der letzten beiden Achsen zu wiederholen, die der Höhe und Breite entsprechen. Sie berechnet die Anzahl der Wiederholungen anhand des Dilatationsfaktors und der Form der Eingabe. Sie gibt eine neue Ansicht auf dem Eingabearray zurück, die keinen zusätzlichen Speicherplatz benötigt.
        &#34;&#34;&#34;
        dh, dw = Dr # Dilatationsfaktor
        batch, C, H, W = X.shape
        Xd = np.repeat(X, repeats=dw, axis=-1) # Wiederhole die Eingabe entlang der Breite
        Xd = np.repeat(Xd, repeats=dh, axis=-2) # Wiederhole die Eingabe entlang der Höhe
        return Xd # Gib die dilatierte Eingabe zurück

    def prepare_subMatrix(self, X, kernelH, kernelW, stride):
        batch, inputC, inputH, inputW = X.shape
        strideH, strideW = stride

        outH = (inputH-kernelH)//strideH + 1
        outW = (inputW-kernelW)//strideW + 1

        strides = (inputC*inputH*inputW, inputW*inputH, inputW*strideH, strideW, inputW, 1)
        strides = tuple(i * X.itemsize for i in strides)

        subM = np.lib.stride_tricks.as_strided(X,
                                               shape=(batch, inputC, outH, outW, kernelH, kernelW),
                                               strides=strides)

        return subM
        
    def convolve(self, X, kernel, stride=(1,1), mode=&#39;forward&#39;):
        &#34;&#34;&#34;
        Führt eine Faltung zwischen der Eingabe X und dem Kernel K aus.

        Parameters
        ----------
        X : numpy.ndarray
            Eingabedaten mit Form (batch, inputC, inputH, inputW)
        K : numpy.ndarray
            Kernelmatrix mit Form (F, inputC, kernelH, kernelW)
        s : tuple of int, optional
            Schritte entlang der Höhe und Breite (sH, sW). Standardwert ist (1, 1).
        mode : {&#39;forward&#39;, &#39;backpropagation&#39;, &#39;backParam&#39;} oder None, optional
            Modus der Faltung. Standardwert ist &#39;forward&#39;.

        Returns
        -------
        numpy.ndarray
            Ausgabedaten mit Form (batch, F, outH, outW) im &#39;forward&#39; Modus,
            (batch, inputC, inputH, inputW) im &#39;backpropagation&#39; Modus oder
            (F, inputC, kernelH, kernelW) im &#39;backParam&#39; Modus.

        Raises
        ------
        ValueError
            Wenn der gegebene Modus nicht erlaubt ist.

        Notes
        -----
        Diese Funktion verwendet die numpy.einsum-Funktion, um eine effiziente Faltung zwischen der Eingabe und dem Kernel durchzuführen. Sie bereitet eine Unter-Matrix der Eingabe vor, die zur Faltung verwendet wird, indem sie die Methode `prepare_subMatrix` aufruft. Sie wendet dann die Faltung an, indem sie die Einstein-Summenkonvention verwendet, die je nach Modus variiert. Sie gibt die gefaltete Ausgabe zurück, die je nach Modus eine andere Form hat.
        &#34;&#34;&#34;
        _, _, kernelH, kernelW = kernel.shape
        subM = self.prepare_subMatrix(X, kernelH, kernelW, stride)

        match mode:
            case &#39;forward&#39;: return np.einsum(&#39;fckl,mcijkl-&gt;mfij&#39;, kernel, subM)
            case &#39;backpropagation&#39;: return np.einsum(&#39;fdkl,mcijkl-&gt;mdij&#39;, kernel, subM)
            case &#39;backParam&#39;: return np.einsum(&#39;mfkl,mcijkl-&gt;fcij&#39;, kernel, subM)
            case _: raise ValueError(&#34;The given mode&#34;, mode, &#34; is not allowed, possible modes are: &#39;forward&#39;, &#39;backpropagation&#39; and &#39;backParam&#39;. &#34;)

    
    def dZ_D_dX(self, dZ_D, imputH, inputW):
    
        # Pad the dilated dZ (dZ_D -&gt; dZ_Dp)

        _, _, Hd, Wd = dZ_D.shape

        padH = imputH - Hd + self.kernelH - 1
        padW = inputW - Wd + self.kernelW - 1

        padding_back = Padding2D(padding=(padH, padW))

        dZ_Dp = padding_back.forward(dZ_D, self.kernel_size, self.stride)

        # Rotate K by 180 degrees

        K_rotated = self.Kernel[:, :, ::-1, ::-1]

        # convolve dZ_Dp with K_rotated

        dXp = self.convolve(dZ_Dp, K_rotated, mode=&#39;backpropagation&#39;)

        dX = self.padding.backpropagation(dXp)

        return dX

    def forward(self, X):
        # padding

        self.X = X

        Xp = self.padding.forward(X, self.kernel_size, self.stride)

        # convolve Xp with K
        Z = self.convolve(Xp, self.Kernel, self.stride, &#39;forward&#39;) + self.bias

        a = self.activation.forward(Z)

        return a
    
    def backpropagation(self, da):
        &#34;&#34;&#34;
        Berechnet die Gradienten der Eingabe X, des Kernels K und des Bias-Vektors b bezüglich der Faltung mit der Aktivierungsfunktion.

        Parameters
        ----------
        da : numpy.ndarray
            Gradient der Aktivierung mit Form (batch, F, outH, outW)

        Returns
        -------
        dX : numpy.ndarray
            Gradient der Eingabe mit Form (batch, inputC, inputH, inputW)

        Notes
        -----
        Diese Funktion verwendet die `padding`-Methode, um die Eingabe X zu polstern, so dass sie mit dem Kernel und dem Schritt kompatibel ist. Sie verwendet dann die `activation`-Methode, um den Gradienten der gefalteten Ausgabe Z zu berechnen. Sie verwendet dann die `dilate2D`-Methode, um den Gradienten der Ausgabe dZ zu vergrößern, so dass er mit dem Eingang kompatibel ist. Sie verwendet dann die `dZ_D_dX`-Methode, um den Gradienten der Eingabe dX zu berechnen. Sie verwendet dann die `numpy.pad`-Funktion, um den dilatierten Gradienten dZ_D zu polstern, so dass er mit dem Kernel kompatibel ist. Sie verwendet dann die `convolve`-Methode, um die Gradienten des Kernels dK und des Bias-Vektors db zu berechnen. Sie gibt den Gradienten der Eingabe dX zurück und speichert die Gradienten des Kernels dK und des Bias-Vektors db als Attribute der Klasse.
        &#34;&#34;&#34;
        # Polstern der Eingabe
        Xp = self.padding.forward(self.X, self.kernel_size, self.stride)
        batch, inputC, inputH, inputW = Xp.shape
        
        # Berechnung des Gradienten der gefalteten Ausgabe
        dZ = self.activation.backpropagation(da)

        # Vergrößerung des Gradienten der Ausgabe
        dZ_D = self.dilate2D(dZ, Dr=self.stride)

        # Berechnung des Gradienten der Eingabe
        dX = self.dZ_D_dX(dZ_D, inputH, inputW)

        # Berechnung der Polstergröße
        _, _, Hd, Wd = dZ_D.shape
        padH = self.inputH - Hd - self.kernelH + 1
        padW = self.inputW - Wd - self.kernelW + 1

        # Polstern des dilatierten Gradienten
        dZ_Dp = np.pad(dZ_D, pad_width=((0, 0), (0, 0), (padH // 2, padH - padH // 2), (padW // 2, padW - padW // 2)), mode=&#39;constant&#39;)

        # Berechnung des Gradienten des Kernels
        self.dK = self.convolve(Xp, dZ_Dp, mode=&#39;backParam&#39;)

        # Berechnung des Gradienten des Bias-Vektors
        self.db = np.sum(dZ, axis=0)

        # Rückgabe des Gradienten der Eingabe
        return dX
      
    def update(self, learnrate, batch, it):
        &#34;&#34;&#34;
        Aktualisiert die Parameter der Faltungsschicht (Kernel und Bias) anhand der Gradienten und des Optimierers.

        Parameters
        ----------
        learnrate : float
            Lernrate
        batch : int
            Batch-Größe (Anzahl der Samples im Batch)
        it : int
            Iterationsnummer

        Returns
        -------
        None
            Diese Funktion gibt nichts zurück, sondern aktualisiert die Attribute der Klasse.

        Notes
        -----
        Diese Funktion verwendet die `optimizer`-Methode, um die optimierten Gradienten zu erhalten, und die `kernel_regularizer`- und `weight_regularizer`-Attribute, um die Regularisierung anzuwenden. Sie aktualisiert dann die Parameter mit der Lernrate und der Batch-Größe, indem sie die `numpy.subtract`-Funktion verwendet, die eine vektorisierte Berechnung ermöglicht.
        &#34;&#34;&#34;
        # Erhalten der optimierten Gradienten
        dK, db = self.optimizer.get_optimization(self.dK, self.db, it)

        # Anwenden der Regularisierung
        if self.kernel_regularizer[0].casefold() == &#39;l2&#39;:
            dK += self.kernel_regularizer[1] * self.Kernel
        elif self.weight_regularizer[0].casefold() == &#39;l1&#39;:
            dK += self.kernel_regularizer[1] * np.sign(self.Kernel)

        # Aktualisieren der Parameter mit der Lernrate und der Batch-Größe
        self.Kernel = np.subtract(self.Kernel, dK * (learnrate / batch))
        if self.use_bias:
            self.bias = np.subtract(self.bias, db * (learnrate / batch))


# #### [Dropout class]

#Klasse für die Dropout Schichten/Layer
class Dropout:

    def __init__(self, p):
        &#39;&#39;&#39;
        Der Konstruktor der Dropout-Klasse. Initialisiert die Dropout-Wahrscheinlichkeit.
        
        Parameter:
        p: Dropout-Wahrscheinlichkeit
        
        Ausgabe:
        void
        &#39;&#39;&#39;
        self.p = p
        if self.p == 0:
            self.p += 1e-6
        if self.p == 1:
            self.p -= 1e-6

    def forward(self, X):
        &#39;&#39;&#39;
        Diese Funktion führt die Vorwärtspropagation durch. Sie erstellt eine Maske mit der gleichen Form wie X, 
        die zufällige Werte enthält, und wendet dann die Maske auf X an.
        
        Parameter:
        X: Eingabedaten
        
        Ausgabe:
        Z: Ausgabedaten nach Anwendung der Dropout-Maske
        &#39;&#39;&#39;
        self.mask = (np.random.rand(*X.shape) &lt; self.p) / self.p
        Z = X * self.mask
        return Z

    def backpropagation(self, dZ):
        &#39;&#39;&#39;
        Diese Funktion führt die Rückwärtspropagation durch. Sie wendet die Dropout-Maske auf den Gradienten an.
        
        Parameter:
        dZ: Gradient der Ausgabedaten
        
        Ausgabe:
        dX: Gradient der Eingabedaten nach Anwendung der Dropout-Maske
        &#39;&#39;&#39;
        dX = dZ * self.mask
        return dX





# #### [Maxpool2D class]
class Pooling2D:
    &#34;&#34;&#34;
    Eine Klasse, die eine Pooling-Schicht in einem künstlichen neuronalen Netzwerk implementiert.

    Attributes
    ----------
    padding : Padding2D
        Ein Objekt der Klasse Padding2D, das die Art des Padding für die Eingabedaten bestimmt.
    kernelSize : tuple of int
        Die Größe des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird ein quadratischer Kernel angenommen.
    stride : tuple of int
        Die Schrittgröße des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird eine gleiche Schrittgröße in beiden Richtungen angenommen.
    pool_type : str
        Die Art des Pooling, das angewendet werden soll. Mögliche Werte sind &#39;max&#39;, &#39;mean&#39; oder &#39;min&#39;.
    output_shape : tuple of int
        Die Form der Ausgabedaten nach dem Pooling.

    Methods
    -------
    get_dimensions(input_shape)
        Berechnet die Form der Ausgabedaten und das Padding für die Eingabedaten anhand der Kernel- und Schrittgröße.
    prepare_subMatrix(X, kernelSize, stride)
        Erstellt eine Matrix von Teilmatrizen, die den Pooling-Kerneln entsprechen, aus den Eingabedaten mit Hilfe von numpy-Strides.
    pooling(X, kernelSize, stride)
        Wendet das Pooling auf die Eingabedaten an und gibt die Ausgabedaten zurück.
    prepare_mask(subM, kernelH, kernelW, poolType)
        Erstellt eine Maske aus Einsen und Nullen, die angibt, welche Elemente der Teilmatrizen für das Pooling ausgewählt wurden.
    mask_dXp(mask, Xp, dZ, kernelH, kernelW)
        Multipliziert die Maske mit dem Fehlergradienten dZ, um den Fehlergradienten dXp für die gepaddeten Eingabedaten zu erhalten.
    maxmin_pool_backprop(dZ, X, type)
        Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ und der Maske für das Max- oder Min-Pooling.
    dZ_dZp(dZ)
        Erweitert den Fehlergradienten dZ, um ihn mit dem Fehlergradienten dXp in Übereinstimmung zu bringen, indem er ihn mit Nullen auffüllt und wickelt.
    averagepool_backprop(dZ, X)
        Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ für das Mittelwert-Pooling.
    forward(X)
        Führt das Pooling auf die Eingabedaten aus und gibt die Ausgabedaten zurück.
    backpropagation(dZ)
        Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ und des pool_type-Attributs.
    &#34;&#34;&#34;
    
    def __init__(self, kernelSize=(2,2), stride=(2,2), padding=&#39;valid&#39;, pool_type=&#39;mean&#39;):
        &#34;&#34;&#34;
        Initialisiert die Attribute der Pooling-Schicht.

        Parameters
        ----------
        kernelSize : tuple of int, optional
            Die Größe des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird ein quadratischer Kernel angenommen. Der Standardwert ist (2,2).
        stride : tuple of int, optional
            Die Schrittgröße des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird eine gleiche Schrittgröße in beiden Richtungen angenommen. Der Standardwert ist (2,2).
        padding : str, optional
            Die Art des Padding für die Eingabedaten. Mögliche Werte sind &#39;valid&#39;, &#39;same&#39; oder &#39;full&#39;. Der Standardwert ist &#39;valid&#39;.
        pool_type : str, optional
            Die Art des Pooling, das angewendet werden soll. Mögliche Werte sind &#39;max&#39;, &#39;mean&#39; oder &#39;min&#39;. Der Standardwert ist &#39;mean&#39;.

        Returns
        -------
        None
            Diese Methode gibt nichts zurück, sondern initialisiert die Attribute der Klasse.
        &#34;&#34;&#34;
        self.padding_type = padding
        self.padding = Padding2D(padding=padding)
        
        self.kernelSize = kernelSize
        if isinstance(kernelSize, int): self.kernel_size = (kernelSize, kernelSize) 
        self.kernelH, self.kernelW = self.kernelSize

        self.stride = stride
        if isinstance(stride, int): self.stride = (stride, stride)
        self.strideH, self.strideW = self.stride

        self.pool_type = pool_type

    def get_dimensions(self, input_shape):
        &#34;&#34;&#34;
        Berechnet die Form der Ausgabedaten und das Padding für die Eingabedaten anhand der Kernel- und Schrittgröße.

        Parameters
        ----------
        input_shape : tuple of int
            Die Form der Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.

        Returns
        -------
        None
            Diese Methode gibt nichts zurück, sondern aktualisiert die Attribute output_shape und padding der Klasse.
        &#34;&#34;&#34;
        #Die Größe der gepolsterten Eingabe ist die richtige input_shape
        input_shape,_ = self.padding.get_dimensions(input_shape, self.kernelSize, self.stride)
        
        *sh, inputH, inputW = input_shape

        outH = (inputH-self.kernelH)//self.strideH + 1
        outW = (inputW-self.kernelW)//self.strideW + 1
        
        self.output_shape = (*sh, outH, outW)
        
    # creates submatrixes for the pooling operation and returns them as numpy array (a view of the input matrix)
    def prepare_subMatrix(self, X, kernelSize, stride):
        &#34;&#34;&#34;
        Erstellt eine Matrix von Teilmatrizen, die den Pooling-Kerneln entsprechen, aus den Eingabedaten mit Hilfe von numpy-Strides.

        Parameters
        ----------
        X : numpy.ndarray
            Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.
        kernelSize : tuple of int
            Die Größe des Pooling-Kernels in Höhe und Breite.
        stride : tuple of int
            Die Schrittgröße des Pooling-Kernels in Höhe und Breite.

        Returns
        -------
        numpy.ndarray
            Die Matrix von Teilmatrizen in der Form (batch, inputC, Oh, Ow, kernelH, kernelW), wobei Oh die Ausgabehöhe, Ow die Ausgabebreite, kernelH die Kernelhöhe und kernelW die Kernelbreite ist.
        &#34;&#34;&#34;
        batch, inputC, inputH, inputW = X.shape #dimension of image (batch- Batch size, inputC - channel size, inputH - height, inputW- width)
        strideH, strideW = stride #strides(Schrittweite)
        kernelH, kernelW = kernelSize #size of filter

        Oh = (inputH-kernelH)//strideH + 1    #calc height of output Matrix
        Ow = (inputW-kernelW)//strideW + 1    #calc width of output Matrix

        strides = (inputC*inputH*inputW, inputH*inputW, inputW*strideH, strideW, inputW, 1) # tuple of strides of matrix (number of lement to pass) 
        strides = tuple(i * X.itemsize for i in strides) #x.itemsize - size of the item in Byte (INT =4), list of bytes to pass for each dimension

        subM = np.lib.stride_tricks.as_strided(X, shape=(batch, inputC, Oh, Ow, kernelH, kernelW), strides=strides) #create the submatrixes (Der Codeblock subM = np.lib.stride_tricks.as_strided(X, shape=(batch, inputC, Oh, Ow, kernelH, kernelW), strides=strides) verwendet die Funktion as_strided aus dem NumPy-Modul stride_tricks, um eine neue Ansicht der Eingabematrix X zu erstellen. Diese neue Ansicht, subM, repräsentiert eine Sammlung von Unter-Matrizen, die für die Pooling-Operation in einem Convolutional Neural Network (CNN) verwendet werden können. Hier ist eine detaillierte Erklärung: X: Die ursprüngliche Eingabematrix, die die Daten für das CNN enthält. shape=(m, inputC, Oh, Ow, kernelH, kernelW): Das neue Shape-Argument definiert die Form der resultierenden Unter-Matrix subM. m ist die Anzahl der Beispiele im Batch, inputC die Anzahl der Kanäle, Oh und Ow sind die Höhe und Breite der Ausgabe nach dem Pooling, und kernelH und kernelW sind die Höhe und Breite des Pooling-Kerns. strides=strides: Das Strides-Argument gibt an, wie viele Bytes in der ursprünglichen Matrix X übersprungen werden müssen, um zum nächsten Element in jeder Dimension zu gelangen. Die Funktion as_strided ermöglicht es, ohne zusätzlichen Speicherbedarf oder Kopieren von Daten, auf die verschiedenen Regionen der Eingabematrix X zuzugreifen, als wären sie separate Unter-Matrizen. Dies ist besonders nützlich für Pooling-Operationen, bei denen ein kleiner Bereich (der Pooling-Kern) über die gesamte Eingabematrix gleitet und Operationen wie Max-Pooling oder Average-Pooling durchführt. Die resultierende Unter-Matrix subM kann dann verwendet werden, um diese Pooling-Operationen effizient durchzuführen.)
        return subM 

    #perform the pooling operation
    def pooling(self, X, kernelSize=(2,2), stride=(2,2)):
        &#34;&#34;&#34;
        Wendet das Pooling auf die Eingabedaten an und gibt die Ausgabedaten zurück.

        Parameters
        ----------
        X : numpy.ndarray
            Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.
        kernelSize : tuple of int, optional
            Die Größe des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird ein quadratischer Kernel angenommen. Der Standardwert ist (2,2).
        stride : tuple of int, optional
            Die Schrittgröße des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird eine gleiche Schrittgröße in beiden Richtungen angenommen. Der Standardwert ist (2,2).

        Returns
        -------
        numpy.ndarray
            Die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.

        Raises
        ------
        ValueError
            Wenn der pool_type-Attributwert nicht &#39;max&#39;, &#39;mean&#39; oder &#39;min&#39; ist.
        &#34;&#34;&#34;
        subM = self.prepare_subMatrix(X, kernelSize, stride) #create view consisting of sub-matrices

        match self.pool_type:
            case &#39;max&#39;: return np.max(subM, axis=(-2,-1)) #Maxpooling
            case &#39;mean&#39;: return np.mean(subM, axis=(-2,-1)) #Average/Mean Pooling
            case &#39;min&#39; : return np.min(subM, axis = (-2,-1)) #Min Pooling
            case _: raise ValueError(&#34;Allowed pool types are only &#39;max&#39;, &#39;mean&#39; or &#39;min&#39; and not&#34;, str(self.pool_type))
            
    def prepare_mask(self, subM, kernelH, kernelW, poolType = &#39;max&#39;):
        &#34;&#34;&#34;
        Erstellt eine Maske aus Einsen und Nullen, die angibt, welche Elemente der Teilmatrizen für das Max- oder Min-Pooling ausgewählt wurden.

        Parameters
        ----------
        subM : numpy.ndarray
            Die Matrix von Teilmatrizen in der Form (batch, inputC, Oh, Ow, kernelH, kernelW), wobei Oh die Ausgabehöhe, Ow die Ausgabebreite, kernelH die Kernelhöhe und kernelW die Kernelbreite ist.
        kernelH : int
            Die Höhe des Pooling-Kernels.
        kernelW : int
            Die Breite des Pooling-Kernels.
        poolType : str
            Die Art des Pooling, das angewendet werden soll. Mögliche Werte sind &#39;max&#39; oder &#39;min&#39;.

        Returns
        -------
        numpy.ndarray
            Die Maske in der gleichen Form wie subM.
        &#34;&#34;&#34;
        batch, inputC, Oh, Ow, kernelH, kernelW = subM.shape # shape of submatrix (batch- Batchsize, inputC - channel size, inputH - output height, inputW- output width, kernelH - high pooling kernel, w - width pooling kernel)asd

        a = subM.reshape(-1,kernelH*kernelW)
        idx = np.where(poolType.lower() ==&#39;max&#39;, np.argmax(a, axis=1), np.argmin(a, axis=1))
        b = np.zeros(a.shape)
        b[np.arange(b.shape[0]), idx] = 1
        mask = b.reshape((batch, inputC, Oh, Ow, kernelH, kernelW))

        return mask

    def mask_dXp(self, mask, Xp, dZ, kernelH, kernelW):
        &#34;&#34;&#34;
        Multipliziert die Maske mit dem Fehlergradienten dZ, um den Fehlergradienten dXp für die gepaddeten Eingabedaten zu erhalten.

        Parameters
        ----------
        mask : numpy.ndarray
            Die Maske in der Form (batch, inputC, Oh, Ow, kernelH, kernelW), wobei Oh die Ausgabehöhe, Ow die Ausgabebreite, kernelH die Kernelhöhe und kernelW die Kernelbreite ist.
        Xp : numpy.ndarray
            Die gepaddeten Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei inputH die gepaddete Eingabehöhe und inputW die gepaddete Eingabebreite ist.
        dZ : numpy.ndarray
            Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.
        kernelH : int
            Die Höhe des Pooling-Kernels.
        kernelW : int
            Die Breite des Pooling-Kernels.

        Returns
        -------
        numpy.ndarray
            Der Fehlergradient für die gepaddeten Eingabedaten in der gleichen Form wie Xp.
        &#34;&#34;&#34;
        dA = np.einsum(&#39;i,ijk-&gt;ijk&#39;, dZ.reshape(-1), mask.reshape(-1,kernelH,kernelW)).reshape(mask.shape)
        batch, inputC, inputH, inputW = Xp.shape
        strides = (inputC*inputH*inputW, inputH*inputW, inputW, 1)
        strides = tuple(i * Xp.itemsize for i in strides)
        dXp = np.lib.stride_tricks.as_strided(dA, Xp.shape, strides)
        #dXp = np.broadcast_to(dA, Xp.shape)
        return dXp

    #backpropagation of max pooling layer
    def maxmin_pool_backprop(self, dZ, X, type = &#39;max&#39;):
        &#34;&#34;&#34;
        Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ und der Maske für das Max- oder Min-Pooling.

        Parameters
        ----------
        dZ : numpy.ndarray
            Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.
        X : numpy.ndarray
            Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei inputH die Eingabehöhe und inputW die Eingabebreite ist.
        type : str
            Die Art des Pooling, das angewendet werden soll. Mögliche Werte sind &#39;max&#39; oder &#39;min&#39;.

        Returns
        -------
        numpy.ndarray
            Der Fehlergradient für die Eingabedaten in der Form (batch, inputC, inputH, inputW).
        &#34;&#34;&#34;
        Xp = self.padding.forward(X, self.kernelSize, self.stride)

        subM = self.prepare_subMatrix(Xp, self.kernelSize, self.stride)

        _, _, _, _, kernelH, kernelW = subM.shape

        mask = self.prepare_mask(subM, kernelH, kernelW, type)

        dXp = self.mask_dXp(mask, Xp, dZ, kernelH, kernelW)

        return dXp
    
    
    def dZ_dZp(self, dZ):
        &#34;&#34;&#34;
        Erweitert den Fehlergradienten dZ, um ihn mit dem Fehlergradienten dXp in Übereinstimmung zu bringen, indem er ihn mit Nullen auffüllt und wickelt.

        Parameters
        ----------
        dZ : numpy.ndarray
            Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.

        Returns
        -------
        numpy.ndarray
            Der erweiterte Fehlergradient in der Form (batch, inputC, inputH, inputW), wobei inputH die gepaddete Eingabehöhe und inputW die gepaddete Eingabebreite ist.
        &#34;&#34;&#34;
        strideH, strideW = self.stride
        kernelH, kernelW = self.kernelSize
        dZp = np.pad(dZ, pad_width=((0, 0), (0, 0), (0, kernelH-1), (0, kernelW-1)), mode=&#39;constant&#39;)
        dZp = np.pad(dZp, pad_width=((0, 0), (0, 0), (0, -strideH+1), (0, -strideW+1)), mode=&#39;wrap&#39;)
        return dZp
    
    
    #backpropagation of the avarage pooling layer
    def averagepool_backprop(self, dZ, X):
        &#34;&#34;&#34;
        Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ für das Mittelwert-Pooling.

        Parameters
        ----------
        dZ : numpy.ndarray
            Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.
        X : numpy.ndarray
            Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei inputH die Eingabehöhe und inputW die Eingabebreite ist.

        Returns
        -------
        numpy.ndarray
            Der Fehlergradient für die Eingabedaten in der Form (batch, inputC, inputH, inputW).
        &#34;&#34;&#34;
        Xp = self.padding.forward(X, self.kernelSize, self.stride)

        batch, inputC, inputH, inputW = Xp.shape
        dZp = self.dZ_dZp(dZ)
        
        padH = inputH - dZp.shape[-2]
        padW = inputW - dZp.shape[-1]

        padding_back = Padding2D(padding=(padH, padW))

        dXp = padding_back.forward(dZp, s=self.stride, kernel_size=self.kernelSize)

        #return dXp /(inputH*inputW) # (self.strideH*self.strideW)
        return np.mean(dXp, axis=(-2, -1), keepdims=True)
    
    
    def forward(self, X):
        &#34;&#34;&#34;
        Führt das Pooling auf die Eingabedaten aus und gibt die Ausgabedaten zurück.

        Parameters
        ----------
        X : numpy.ndarray
            Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.

        Returns
        -------
        numpy.ndarray
            Die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.
        &#34;&#34;&#34;

        self.X = X
                        
        # padding
        Xp = self.padding.forward(X, self.kernelSize, self.stride)

        Z = self.pooling(Xp, self.kernelSize, self.stride)
        return Z

    def backpropagation(self, dZ):
        &#34;&#34;&#34;
        Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ und des pool_type-Attributs.

        Parameters
        ----------
        dZ : numpy.ndarray
            Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.

        Returns
        -------
        numpy.ndarray
            Der Fehlergradient für die Eingabedaten in der Form (batch, inputC, inputH, inputW).
        &#34;&#34;&#34;
        if self.pool_type==&#39;max&#39;:
            dXp = self.maxmin_pool_backprop(dZ, self.X)
        elif self.pool_type==&#39;mean&#39;:
            dXp = self.averagepool_backprop(dZ.copy(), self.X)
            #dXp2 = self.averagepool_backprop2(dZ.copy(), self.X) #optimized
        dX = self.padding.backpropagation(dXp)
        return dX


# #### Flatten class
#4D zu 2D


class Flatten:
    &#34;&#34;&#34;
    Eine Klasse, die eine Flattening-Schicht in einem künstlichen neuronalen Netzwerk implementiert.

    Attributes
    ----------
    batch : int
        Die Batch-Größe der Eingabedaten.
    inputC : int
        Die Anzahl der Kanäle der Eingabedaten.
    inputH : int
        Die Höhe der Eingabedaten.
    inputW : int
        Die Breite der Eingabedaten.
    output_shape : int
        Die Form der Ausgabedaten nach dem Flattening.

    Methods
    -------
    forward(X)
        Flacht die Eingabedaten ab und gibt die Ausgabedaten zurück.
    backpropagation(dZ)
        Formt den Fehlergradienten dZ zurück in die Form der Eingabedaten und gibt den Fehlergradienten dX zurück.
    get_dimensions(input_shape)
        Bestimmt die Form der Eingabe- und Ausgabedaten anhand der input_shape.
    &#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;
        Initialisiert die Attribute der Flattening-Schicht.

        Parameters
        ----------
        None
            Diese Methode nimmt keine Parameter entgegen.

        Returns
        -------
        None
            Diese Methode gibt nichts zurück, sondern initialisiert die Attribute der Klasse.
        &#34;&#34;&#34;
        pass

    def forward(self, X):
        &#34;&#34;&#34;
        Flacht die Eingabedaten ab und gibt die Ausgabedaten zurück.

        Parameters
        ----------
        X : numpy.ndarray
            Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.

        Returns
        -------
        numpy.ndarray
            Die Ausgabedaten in der Form (batch, inputC * inputH * inputW).
        &#34;&#34;&#34;
        self.batch, self.inputC, self.inputH, self.inputW = X.shape
        X_flat = X.reshape((self.batch, self.inputC * self.inputH * self.inputW))
        return X_flat

    def backpropagation(self, dZ):
        &#34;&#34;&#34;
        Formt den Fehlergradienten dZ zurück in die Form der Eingabedaten und gibt den Fehlergradienten dX zurück.

        Parameters
        ----------
        dZ : numpy.ndarray
            Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC * inputH * inputW).

        Returns
        -------
        numpy.ndarray
            Der Fehlergradient für die Eingabedaten in der Form (batch, inputC, inputH, inputW).
        &#34;&#34;&#34;
        dX = dZ.reshape((self.batch, self.inputC, self.inputH, self.inputW))
        return dX

    def get_dimensions(self, input_shape):
        &#34;&#34;&#34;
        Bestimmt die Form der Eingabe- und Ausgabedaten anhand der input_shape.

        Parameters
        ----------
        input_shape : tuple of int
            Die Form der Eingabedaten in der Form (batch, inputC, inputH, inputW) oder (inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.

        Returns
        -------
        None
            Diese Methode gibt nichts zurück, sondern aktualisiert die Attribute batch, inputC, inputH, inputW und output_shape der Klasse.
        &#34;&#34;&#34;
        if len(input_shape)==4:
            self.batch, self.inputC, self.inputH, self.inputW = input_shape
        elif len(input_shape)==3:
            self.inputC, self.inputH, self.inputW = input_shape

        self.output_shape = self.inputC * self.inputH * self.inputW
        


# Klasse für die vollvernetzten/Dens - Layer
class Dense:

    def __init__(self, neurons, activation_type=None, use_bias=True,
                 weight_initializer_type=None, weight_regularizer=None, seed=None, input_dim=None, weights = None, bias = None):

        &#39;&#39;&#39;
        Parameters:

        neurons: (positiver int) Anzahl der Neuronen

        activation_type: Art der Aktivierung, Standard: linear,  mögliche Optionen : &#39;sigmoid&#39;, &#39;linear&#39;, &#39;tanh&#39;, &#39;softmax&#39;, &#39;prelu&#39;, &#39;relu&#39; ...

        use_bias: (boolean) gibt an, ob der Bias genutz werden soll

        weight_initializer_type: (str) Initzialisierer für die Gewichte

        weight_regularizer: (tuple) Regularisierer, welcher auf die Matrix angewendet wird möglic: (&#39;L2&#39;, 0.01) or (&#39;L1&#39;, 2)

        seed: (seed) um das gleiche Netz zu repruduzieren

        input_dim: (int) Anzahl der Neuronen der Eingabeschicht
        &#39;&#39;&#39;
        self.neurons = neurons
        self.activation_type = activation_type
        self.activation = Activation(activation_type=activation_type) 
        self.use_bias = use_bias 
        self.weight_initializer_type = weight_initializer_type 
        if weight_regularizer is None:
            self.weight_regularizer = (&#39;L2&#39;, 0)
        else:
            self.weight_regularizer = weight_regularizer
        self.seed = seed
        self.input_dim = input_dim
        
        if weights is None:
            self.weight = None
        elif isinstance(weights, np.ndarray):
            self.weight = weights
            self.neurons = weights.shape[-1]
            self.input_shape = weights.shape[0]
        else: raise ValueError(&#34;The given weight type is invalide: &#34;, type(weights))

        if bias is None:
            self.bias = None
        elif isinstance(bias, np.ndarray):
            self.bias = bias
        else: raise ValueError(&#34;The given weight type is invalide: &#34;, type(bias))
        
        if isinstance(self.weight, np.ndarray) and isinstance(self.bias, np.ndarray) and self.bias.shape != (self.neurons, 1): raise ValueError(&#34;The bias Array has the wrong shape of: &#34;, self.bias.shape)

    def initialize_parameters(self, input_shape, optimizer_type):
        &#39;&#39;&#39;
        Diese Funktion initialisiert die Gewichte und den Bias des Neurons und den Optimierer.
        
        Parameter:
        input_shape: (tuple) gibt die Eingabegröße bzw. Neuronen des vorigen Layers an
        optimizer_type: (str) gibt den Typen des Optimizer an
        
        Ausgabe:
        Keine direkte Ausgabe, aber sie aktualisiert die Attribute self.W, self.b und self.optimizer der Klasse.
        &#39;&#39;&#39;
        shapeWeight = (input_shape, self.neurons) #Größe der Gewichte
        shapebias = (self.neurons, 1) #Größe (shape) der Bias-Werte
        if self.weight is None:
            initializer = Weights_initializer(shape=shapeWeight, initializer_type=self.weight_initializer_type, seed=self.seed) #Initialisierer für die Gewichte
            self.weight = initializer.get_initializer() #Initialiseren der Gewichte
        if self.bias is None:
            self.bias = np.zeros(shapebias) #Initialisieren der Bias-Werte

        self.optimizer = Optimizer(optimizer_type=optimizer_type, shape_W=shapeWeight, shape_b=shapebias)

    def forward(self, X):
        &#39;&#39;&#39;
        Diese Funktion führt die Vorwärtspropagation durch.
        
        Parameter:
        X: Eingabedaten
        
        Ausgabe:
        out: Aktivierungswerte nach Anwendung der Aktivierungsfunktion
        &#39;&#39;&#39;
        #out = f(x_i * wi + b)
        self.X = X
        d = np.dot(X, self.weight) # Multiplizieren der Eingaben mit den Gewichten jedes Neurons
        self.s = d + self.bias.T #addiern des Bias-Wertes
        out = self.activation.forward(self.s) #Akktivierungsfunktion anwenden
        return out

    def backpropagation(self, error):
        &#39;&#39;&#39;
        Diese Funktion führt die Rückwärtspropagation durch.
        
        Parameter:
        da: Fehler-Gradient der nächsten Schicht
        
        Ausgabe:
        dX: Fehldergradient nach der Eingabe, welche für die vorigen Schichten benötigt wird
        &#39;&#39;&#39;
        #dX = f&#39;(error_i) * w_i
        dz = self.activation.backpropagation(error) #Ableitung der Aktivierungsfunktion von dem Fehler der nächsten Schicht
        dr = dz.copy()
        self.dbias = np.sum(dz, axis=0).reshape(-1,1) #Änderung der Gewichte
        self.dweight = np.dot((self.X.T), dr) #Änderung der Bias-Werte
        dX = np.dot(dr, (self.weight.T))
        return dX

    def update(self, learnrate, batch, k):
        &#39;&#39;&#39;
        Diese Funktion aktualisiert die Gewichte und den Bias basierend auf den während der Rückwärtspropagation berechneten Ableitungen.
        
        Parameter:
        learnrate: Lernrate
        batch: Batch-Größe (Anzahl der Proben im Batch)
        k: Iterationsnummer
        
        Ausgabe:
        void
        &#39;&#39;&#39;

        dW, db = self.optimizer.get_optimization(self.dweight, self.dbias, k)
        #Anwenden des Geweichtsregularisierers
        if self.weight_regularizer[0].lower()==&#39;l2&#39;:
            dW += self.weight_regularizer[1] * self.weight
        elif self.weight_regularizer[0].lower()==&#39;l1&#39;:
            dW += self.weight_regularizer[1] * np.sign(self.weight)

        self.weight -= dW*(learnrate/batch)
        if self.use_bias:
            self.bias -= db*(learnrate/batch)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="Neural.Layer.Conv2D"><code class="flex name class">
<span>class <span class="ident">Conv2D</span></span>
<span>(</span><span>filters, kernel_size, stride=(1, 1), padding='valid', activation_type=None, use_bias=True, weight_initializer_type=None, kernel_regularizer=None, seed=None, input_shape=None, bias=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Der Konstruktor der Conv2D-Klasse. Initialisiert die Parameter der Faltungsschicht.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>filters</code></strong> :&ensp;<code>int</code></dt>
<dd>Anzahl der Ausgabefilter in der Faltung (F).</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>int oder tuple</code> of <code>int</code></dt>
<dd>Höhe und Breite des 2D-Faltungsfensters (kernelH, kernelW).</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>int oder tuple</code> of <code>int</code>, optional</dt>
<dd>Schritte entlang der Höhe und Breite (strideH, strideW). Standardwert ist (1, 1).</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>{'same', 'valid'} oder int oder tuple</code> of <code>int</code>, optional</dt>
<dd>Polsterungstyp. Standardwert ist 'valid'.</dd>
<dt><strong><code>activation_type</code></strong> :&ensp;<code>{'sigmoid', 'linear', 'tanh', 'softmax', 'prelu', 'relu'} oder None</code>, optional</dt>
<dd>Art der Aktivierung. Standardwert ist None.</dd>
<dt><strong><code>use_bias</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Gibt an, ob eine Bias-Vektor verwendet wird. Standardwert ist True.</dd>
<dt><strong><code>weight_initializer_type</code></strong> :&ensp;<code>str oder None</code>, optional</dt>
<dd>Initialisierer für die Gewichtsmatrix des Kernels. Standardwert ist None.</dd>
<dt><strong><code>kernel_regularizer</code></strong> :&ensp;<code>tuple</code> of <code>(str, float) oder None</code>, optional</dt>
<dd>Regularisierungsfunktion für die Kernelmatrix. Standardwert ist None.</dd>
<dt><strong><code>seed</code></strong> :&ensp;<code>int oder None</code>, optional</dt>
<dd>Für reproduzierbare Ergebnisse. Standardwert ist None.</dd>
<dt><strong><code>input_shape</code></strong> :&ensp;<code>tuple</code> of <code>int oder None</code>, optional</dt>
<dd>Größe des Inputs (batch, inputC, inputH, inputW). Standardwert ist None.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>Diese Klasse implementiert eine zweidimensionale Faltungsschicht, die eine lineare Transformation auf einem lokalen Bereich der Eingabe anwendet. Die Faltung wird durch eine Gewichtsmatrix (Kernel) und einen Bias-Vektor charakterisiert, die während des Trainings gelernt werden. Die Faltung kann auch eine Aktivierungsfunktion, eine Polsterung und eine Regularisierung enthalten, um die Leistung zu verbessern.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Conv2D:
    
    def __init__(self, filters, kernel_size, stride=(1, 1), padding=&#39;valid&#39;,
             activation_type=None, use_bias=True, weight_initializer_type=None,
             kernel_regularizer=None, seed=None, input_shape=None, bias = None):
            &#39;&#39;&#39;
            Der Konstruktor der Conv2D-Klasse. Initialisiert die Parameter der Faltungsschicht.

            Parameters
            ----------
            filters : int
                Anzahl der Ausgabefilter in der Faltung (F).
            kernel_size : int oder tuple of int
                Höhe und Breite des 2D-Faltungsfensters (kernelH, kernelW).
            stride : int oder tuple of int, optional
                Schritte entlang der Höhe und Breite (strideH, strideW). Standardwert ist (1, 1).
            padding : {&#39;same&#39;, &#39;valid&#39;} oder int oder tuple of int, optional
                Polsterungstyp. Standardwert ist &#39;valid&#39;.
            activation_type : {&#39;sigmoid&#39;, &#39;linear&#39;, &#39;tanh&#39;, &#39;softmax&#39;, &#39;prelu&#39;, &#39;relu&#39;} oder None, optional
                Art der Aktivierung. Standardwert ist None.
            use_bias : bool, optional
                Gibt an, ob eine Bias-Vektor verwendet wird. Standardwert ist True.
            weight_initializer_type : str oder None, optional
                Initialisierer für die Gewichtsmatrix des Kernels. Standardwert ist None.
            kernel_regularizer : tuple of (str, float) oder None, optional
                Regularisierungsfunktion für die Kernelmatrix. Standardwert ist None.
            seed : int oder None, optional
                Für reproduzierbare Ergebnisse. Standardwert ist None.
            input_shape : tuple of int oder None, optional
                Größe des Inputs (batch, inputC, inputH, inputW). Standardwert ist None.

            Notes
            -----
            Diese Klasse implementiert eine zweidimensionale Faltungsschicht, die eine lineare Transformation auf einem lokalen Bereich der Eingabe anwendet. Die Faltung wird durch eine Gewichtsmatrix (Kernel) und einen Bias-Vektor charakterisiert, die während des Trainings gelernt werden. Die Faltung kann auch eine Aktivierungsfunktion, eine Polsterung und eine Regularisierung enthalten, um die Leistung zu verbessern.

            
            
            &#39;&#39;&#39;

            # Verwendung von assert-Anweisungen zur Überprüfung der Eingabetypen
            #assert isinstance(filters, int), &#34;filters muss ein Integer sein&#34;
            assert isinstance(kernel_size, (int, tuple)), &#34;kernel_size muss ein Integer oder ein Tupel sein&#34;
            assert isinstance(stride, (int, tuple)), &#34;s muss ein Integer oder ein Tupel sein&#34;
            assert padding in [&#39;same&#39;, &#39;valid&#39;] or isinstance(padding, (int, tuple)), &#34;padding muss &#39;same&#39;, &#39;valid&#39;, ein Integer oder ein Tupel sein&#34;
            assert activation_type in [None, &#39;sigmoid&#39;, &#39;linear&#39;, &#39;tanh&#39;, &#39;softmax&#39;, &#39;prelu&#39;, &#39;relu&#39;], &#34;Ungültiger activation_type&#34;
            assert isinstance(use_bias, bool), &#34;use_bias muss ein Boolean sein&#34;
            assert kernel_regularizer is None or isinstance(kernel_regularizer, tuple), &#34;kernel_regularizer muss ein Tupel sein&#34;
            assert input_shape is None or isinstance(input_shape, tuple), &#34;input_shape muss ein Tupel sein&#34;

            self.kernel_size = (kernel_size, kernel_size) if isinstance(kernel_size, int) else kernel_size
            self.stride = (stride, stride) if isinstance(stride, int) else stride
            self.padding_type = padding
            self.padding = Padding2D(padding=padding)
            self.activation_type = activation_type
            self.activation = Activation(activation_type=activation_type)
            self.use_bias = use_bias
            self.weight_initializer_type = weight_initializer_type
            self.kernel_regularizer = kernel_regularizer if kernel_regularizer is not None else (&#39;L2&#39;, 0)
            self.seed = seed
            self.input_shape_x = input_shape
            
            #extract kernel:
            if isinstance(filters, int):
                self.F = filters
                self.Kernel = None
            elif isinstance(filters, np.ndarray):
                self.kernel_size = filters.shape[-2:]
                self.F = filters.shape[0]
                self.Kernel = filters
            else: raise ValueError(&#34;Given Filter has invalide type: &#34;, type(filters),&#34;.&#34;)
            
            if bias is None:
                self.bias = None
            elif isinstance(bias, np.ndarray):
                self.bias = bias
            else: raise ValueError(&#34;Given Filter has invalide type: &#34;, type(filters),&#34;.&#34;)
                

            # Extrahieren der Höhe und Breite aus kernel_size und stride
            self.kernelH, self.kernelW = self.kernel_size
            self.strideH, self.strideW = self.stride
            self.inputC, self.inputH, self.inputW = 0, 0, 0
    
    def get_dimensions(self, input_shape):
        &#34;&#34;&#34;
        Berechnet die Dimensionen des Outputs basierend auf dem Input-Shape.

        Parameters
        ----------
        input_shape : tuple of int
            Die Form des Inputs (3D oder 4D).

        Returns
        -------
        output_shape : tuple of int
            Die Form des Outputs (3D oder 4D).

        Notes
        -----
        Diese Funktion speichert das Input-Shape und das Output-Shape als Attribute der Klasse. Sie berücksichtigt auch das Padding, das für die Faltung verwendet wird, und berechnet die Höhe und Breite des Outputs anhand des Kernels und des Schritts. Sie passt das Output-Shape an die Länge des Input-Shapes an, indem sie die Batch- und Kanalgrößen beibehält oder hinzufügt.
        &#34;&#34;&#34;

        # Speichern des Input-Shapes
        self.input_shape_x = input_shape

        # Berechnung des Input-Shapes mit Padding, das tatsächlich für diese Conv2D verwendet wird
        self.input_shape, _ = self.padding.get_dimensions(self.input_shape_x, self.kernel_size, self.stride)

        # Entpacken des Input-Shapes in die entsprechenden Variablen
        *shape, self.inputC, self.inputH, self.inputW = self.input_shape

        # Berechnung der Output-Dimensionen
        self.outH = (self.inputH - self.kernelH) // self.strideH + 1
        self.outW = (self.inputW - self.kernelW) // self.strideW + 1

        # Zuweisung des Output-Shapes basierend auf der Länge des Input-Shapes
        self.output_shape = (*shape, self.inputC,  self.F, self.outH, self.outW) if len(input_shape) == 4 else (self.F, self.outH, self.outW)

    def initialize_parameters(self, input_shape, optimizer_type):
        &#34;&#34;&#34;
        Diese Funktion initialisiert die Parameter der Faltungsschicht.

        Parameters
        ----------
        input_shape : tuple of int
            Form der Eingabe zur Conv2D-Schicht
        optimizer_type : str
            Art des Optimierers

        Returns
        -------
        None
            Diese Funktion gibt nichts zurück, sondern setzt die Attribute der Klasse.

        Notes
        -----
        Diese Funktion ruft die Methode `get_dimensions` auf, um die Dimensionen des Outputs basierend auf dem Input-Shape zu berechnen. Sie erstellt dann die Gewichtsmatrix (Kernel) und den Bias-Vektor mit der Klasse `Weights_initializer` und dem angegebenen Initialisierer-Typ. Sie initialisiert auch den Optimizer mit der Klasse `Optimizer` und dem angegebenen Optimizer-Typ. Sie speichert alle diese Parameter als Attribute der Klasse.
        &#34;&#34;&#34;
        self.get_dimensions(input_shape)

        shapebias = (self.F, self.outH, self.outW)

        if self.Kernel is None:
            shape_Kernel = (self.F, self.inputC, self.kernelH, self.kernelW)

            initializer = Weights_initializer(shape=shape_Kernel, initializer_type=self.weight_initializer_type, seed=self.seed)

            self.Kernel = initializer.get_initializer()
        if self.bias is None:
            self.bias = np.zeros(shapebias)

        self.optimizer = Optimizer(optimizer_type=optimizer_type, shape_W=shape_Kernel, shape_b=shapebias)

    def dilate2D(self, X, Dr=(1,1)):
        &#34;&#34;&#34;
        Vergrößert die Eingabe X entlang der Höhe und Breite mit einem gegebenen Dilatationsfaktor.

        Parameters
        ----------
        X : numpy.ndarray
            Eingabedaten mit Form (batch, C, H, W)
        Dr : tuple of int, optional
            Dilatationsfaktor entlang der Höhe und Breite (dh, dw). Standardwert ist (1, 1).

        Returns
        -------
        Xd : numpy.ndarray
            Dilatierte Eingabe mit Form (batch, C, H*dh, W*dw)

        Notes
        -----
        Diese Funktion verwendet die numpy.repeat-Funktion, um die Eingabe entlang der letzten beiden Achsen zu wiederholen, die der Höhe und Breite entsprechen. Sie berechnet die Anzahl der Wiederholungen anhand des Dilatationsfaktors und der Form der Eingabe. Sie gibt eine neue Ansicht auf dem Eingabearray zurück, die keinen zusätzlichen Speicherplatz benötigt.
        &#34;&#34;&#34;
        dh, dw = Dr # Dilatationsfaktor
        batch, C, H, W = X.shape
        Xd = np.repeat(X, repeats=dw, axis=-1) # Wiederhole die Eingabe entlang der Breite
        Xd = np.repeat(Xd, repeats=dh, axis=-2) # Wiederhole die Eingabe entlang der Höhe
        return Xd # Gib die dilatierte Eingabe zurück

    def prepare_subMatrix(self, X, kernelH, kernelW, stride):
        batch, inputC, inputH, inputW = X.shape
        strideH, strideW = stride

        outH = (inputH-kernelH)//strideH + 1
        outW = (inputW-kernelW)//strideW + 1

        strides = (inputC*inputH*inputW, inputW*inputH, inputW*strideH, strideW, inputW, 1)
        strides = tuple(i * X.itemsize for i in strides)

        subM = np.lib.stride_tricks.as_strided(X,
                                               shape=(batch, inputC, outH, outW, kernelH, kernelW),
                                               strides=strides)

        return subM
        
    def convolve(self, X, kernel, stride=(1,1), mode=&#39;forward&#39;):
        &#34;&#34;&#34;
        Führt eine Faltung zwischen der Eingabe X und dem Kernel K aus.

        Parameters
        ----------
        X : numpy.ndarray
            Eingabedaten mit Form (batch, inputC, inputH, inputW)
        K : numpy.ndarray
            Kernelmatrix mit Form (F, inputC, kernelH, kernelW)
        s : tuple of int, optional
            Schritte entlang der Höhe und Breite (sH, sW). Standardwert ist (1, 1).
        mode : {&#39;forward&#39;, &#39;backpropagation&#39;, &#39;backParam&#39;} oder None, optional
            Modus der Faltung. Standardwert ist &#39;forward&#39;.

        Returns
        -------
        numpy.ndarray
            Ausgabedaten mit Form (batch, F, outH, outW) im &#39;forward&#39; Modus,
            (batch, inputC, inputH, inputW) im &#39;backpropagation&#39; Modus oder
            (F, inputC, kernelH, kernelW) im &#39;backParam&#39; Modus.

        Raises
        ------
        ValueError
            Wenn der gegebene Modus nicht erlaubt ist.

        Notes
        -----
        Diese Funktion verwendet die numpy.einsum-Funktion, um eine effiziente Faltung zwischen der Eingabe und dem Kernel durchzuführen. Sie bereitet eine Unter-Matrix der Eingabe vor, die zur Faltung verwendet wird, indem sie die Methode `prepare_subMatrix` aufruft. Sie wendet dann die Faltung an, indem sie die Einstein-Summenkonvention verwendet, die je nach Modus variiert. Sie gibt die gefaltete Ausgabe zurück, die je nach Modus eine andere Form hat.
        &#34;&#34;&#34;
        _, _, kernelH, kernelW = kernel.shape
        subM = self.prepare_subMatrix(X, kernelH, kernelW, stride)

        match mode:
            case &#39;forward&#39;: return np.einsum(&#39;fckl,mcijkl-&gt;mfij&#39;, kernel, subM)
            case &#39;backpropagation&#39;: return np.einsum(&#39;fdkl,mcijkl-&gt;mdij&#39;, kernel, subM)
            case &#39;backParam&#39;: return np.einsum(&#39;mfkl,mcijkl-&gt;fcij&#39;, kernel, subM)
            case _: raise ValueError(&#34;The given mode&#34;, mode, &#34; is not allowed, possible modes are: &#39;forward&#39;, &#39;backpropagation&#39; and &#39;backParam&#39;. &#34;)

    
    def dZ_D_dX(self, dZ_D, imputH, inputW):
    
        # Pad the dilated dZ (dZ_D -&gt; dZ_Dp)

        _, _, Hd, Wd = dZ_D.shape

        padH = imputH - Hd + self.kernelH - 1
        padW = inputW - Wd + self.kernelW - 1

        padding_back = Padding2D(padding=(padH, padW))

        dZ_Dp = padding_back.forward(dZ_D, self.kernel_size, self.stride)

        # Rotate K by 180 degrees

        K_rotated = self.Kernel[:, :, ::-1, ::-1]

        # convolve dZ_Dp with K_rotated

        dXp = self.convolve(dZ_Dp, K_rotated, mode=&#39;backpropagation&#39;)

        dX = self.padding.backpropagation(dXp)

        return dX

    def forward(self, X):
        # padding

        self.X = X

        Xp = self.padding.forward(X, self.kernel_size, self.stride)

        # convolve Xp with K
        Z = self.convolve(Xp, self.Kernel, self.stride, &#39;forward&#39;) + self.bias

        a = self.activation.forward(Z)

        return a
    
    def backpropagation(self, da):
        &#34;&#34;&#34;
        Berechnet die Gradienten der Eingabe X, des Kernels K und des Bias-Vektors b bezüglich der Faltung mit der Aktivierungsfunktion.

        Parameters
        ----------
        da : numpy.ndarray
            Gradient der Aktivierung mit Form (batch, F, outH, outW)

        Returns
        -------
        dX : numpy.ndarray
            Gradient der Eingabe mit Form (batch, inputC, inputH, inputW)

        Notes
        -----
        Diese Funktion verwendet die `padding`-Methode, um die Eingabe X zu polstern, so dass sie mit dem Kernel und dem Schritt kompatibel ist. Sie verwendet dann die `activation`-Methode, um den Gradienten der gefalteten Ausgabe Z zu berechnen. Sie verwendet dann die `dilate2D`-Methode, um den Gradienten der Ausgabe dZ zu vergrößern, so dass er mit dem Eingang kompatibel ist. Sie verwendet dann die `dZ_D_dX`-Methode, um den Gradienten der Eingabe dX zu berechnen. Sie verwendet dann die `numpy.pad`-Funktion, um den dilatierten Gradienten dZ_D zu polstern, so dass er mit dem Kernel kompatibel ist. Sie verwendet dann die `convolve`-Methode, um die Gradienten des Kernels dK und des Bias-Vektors db zu berechnen. Sie gibt den Gradienten der Eingabe dX zurück und speichert die Gradienten des Kernels dK und des Bias-Vektors db als Attribute der Klasse.
        &#34;&#34;&#34;
        # Polstern der Eingabe
        Xp = self.padding.forward(self.X, self.kernel_size, self.stride)
        batch, inputC, inputH, inputW = Xp.shape
        
        # Berechnung des Gradienten der gefalteten Ausgabe
        dZ = self.activation.backpropagation(da)

        # Vergrößerung des Gradienten der Ausgabe
        dZ_D = self.dilate2D(dZ, Dr=self.stride)

        # Berechnung des Gradienten der Eingabe
        dX = self.dZ_D_dX(dZ_D, inputH, inputW)

        # Berechnung der Polstergröße
        _, _, Hd, Wd = dZ_D.shape
        padH = self.inputH - Hd - self.kernelH + 1
        padW = self.inputW - Wd - self.kernelW + 1

        # Polstern des dilatierten Gradienten
        dZ_Dp = np.pad(dZ_D, pad_width=((0, 0), (0, 0), (padH // 2, padH - padH // 2), (padW // 2, padW - padW // 2)), mode=&#39;constant&#39;)

        # Berechnung des Gradienten des Kernels
        self.dK = self.convolve(Xp, dZ_Dp, mode=&#39;backParam&#39;)

        # Berechnung des Gradienten des Bias-Vektors
        self.db = np.sum(dZ, axis=0)

        # Rückgabe des Gradienten der Eingabe
        return dX
      
    def update(self, learnrate, batch, it):
        &#34;&#34;&#34;
        Aktualisiert die Parameter der Faltungsschicht (Kernel und Bias) anhand der Gradienten und des Optimierers.

        Parameters
        ----------
        learnrate : float
            Lernrate
        batch : int
            Batch-Größe (Anzahl der Samples im Batch)
        it : int
            Iterationsnummer

        Returns
        -------
        None
            Diese Funktion gibt nichts zurück, sondern aktualisiert die Attribute der Klasse.

        Notes
        -----
        Diese Funktion verwendet die `optimizer`-Methode, um die optimierten Gradienten zu erhalten, und die `kernel_regularizer`- und `weight_regularizer`-Attribute, um die Regularisierung anzuwenden. Sie aktualisiert dann die Parameter mit der Lernrate und der Batch-Größe, indem sie die `numpy.subtract`-Funktion verwendet, die eine vektorisierte Berechnung ermöglicht.
        &#34;&#34;&#34;
        # Erhalten der optimierten Gradienten
        dK, db = self.optimizer.get_optimization(self.dK, self.db, it)

        # Anwenden der Regularisierung
        if self.kernel_regularizer[0].casefold() == &#39;l2&#39;:
            dK += self.kernel_regularizer[1] * self.Kernel
        elif self.weight_regularizer[0].casefold() == &#39;l1&#39;:
            dK += self.kernel_regularizer[1] * np.sign(self.Kernel)

        # Aktualisieren der Parameter mit der Lernrate und der Batch-Größe
        self.Kernel = np.subtract(self.Kernel, dK * (learnrate / batch))
        if self.use_bias:
            self.bias = np.subtract(self.bias, db * (learnrate / batch))</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Neural.Layer.Conv2D.backpropagation"><code class="name flex">
<span>def <span class="ident">backpropagation</span></span>(<span>self, da)</span>
</code></dt>
<dd>
<div class="desc"><p>Berechnet die Gradienten der Eingabe X, des Kernels K und des Bias-Vektors b bezüglich der Faltung mit der Aktivierungsfunktion.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>da</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Gradient der Aktivierung mit Form (batch, F, outH, outW)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dX</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Gradient der Eingabe mit Form (batch, inputC, inputH, inputW)</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>Diese Funktion verwendet die <code>padding</code>-Methode, um die Eingabe X zu polstern, so dass sie mit dem Kernel und dem Schritt kompatibel ist. Sie verwendet dann die <code>activation</code>-Methode, um den Gradienten der gefalteten Ausgabe Z zu berechnen. Sie verwendet dann die <code>dilate2D</code>-Methode, um den Gradienten der Ausgabe dZ zu vergrößern, so dass er mit dem Eingang kompatibel ist. Sie verwendet dann die <code>dZ_D_dX</code>-Methode, um den Gradienten der Eingabe dX zu berechnen. Sie verwendet dann die <code>numpy.pad</code>-Funktion, um den dilatierten Gradienten dZ_D zu polstern, so dass er mit dem Kernel kompatibel ist. Sie verwendet dann die <code>convolve</code>-Methode, um die Gradienten des Kernels dK und des Bias-Vektors db zu berechnen. Sie gibt den Gradienten der Eingabe dX zurück und speichert die Gradienten des Kernels dK und des Bias-Vektors db als Attribute der Klasse.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backpropagation(self, da):
    &#34;&#34;&#34;
    Berechnet die Gradienten der Eingabe X, des Kernels K und des Bias-Vektors b bezüglich der Faltung mit der Aktivierungsfunktion.

    Parameters
    ----------
    da : numpy.ndarray
        Gradient der Aktivierung mit Form (batch, F, outH, outW)

    Returns
    -------
    dX : numpy.ndarray
        Gradient der Eingabe mit Form (batch, inputC, inputH, inputW)

    Notes
    -----
    Diese Funktion verwendet die `padding`-Methode, um die Eingabe X zu polstern, so dass sie mit dem Kernel und dem Schritt kompatibel ist. Sie verwendet dann die `activation`-Methode, um den Gradienten der gefalteten Ausgabe Z zu berechnen. Sie verwendet dann die `dilate2D`-Methode, um den Gradienten der Ausgabe dZ zu vergrößern, so dass er mit dem Eingang kompatibel ist. Sie verwendet dann die `dZ_D_dX`-Methode, um den Gradienten der Eingabe dX zu berechnen. Sie verwendet dann die `numpy.pad`-Funktion, um den dilatierten Gradienten dZ_D zu polstern, so dass er mit dem Kernel kompatibel ist. Sie verwendet dann die `convolve`-Methode, um die Gradienten des Kernels dK und des Bias-Vektors db zu berechnen. Sie gibt den Gradienten der Eingabe dX zurück und speichert die Gradienten des Kernels dK und des Bias-Vektors db als Attribute der Klasse.
    &#34;&#34;&#34;
    # Polstern der Eingabe
    Xp = self.padding.forward(self.X, self.kernel_size, self.stride)
    batch, inputC, inputH, inputW = Xp.shape
    
    # Berechnung des Gradienten der gefalteten Ausgabe
    dZ = self.activation.backpropagation(da)

    # Vergrößerung des Gradienten der Ausgabe
    dZ_D = self.dilate2D(dZ, Dr=self.stride)

    # Berechnung des Gradienten der Eingabe
    dX = self.dZ_D_dX(dZ_D, inputH, inputW)

    # Berechnung der Polstergröße
    _, _, Hd, Wd = dZ_D.shape
    padH = self.inputH - Hd - self.kernelH + 1
    padW = self.inputW - Wd - self.kernelW + 1

    # Polstern des dilatierten Gradienten
    dZ_Dp = np.pad(dZ_D, pad_width=((0, 0), (0, 0), (padH // 2, padH - padH // 2), (padW // 2, padW - padW // 2)), mode=&#39;constant&#39;)

    # Berechnung des Gradienten des Kernels
    self.dK = self.convolve(Xp, dZ_Dp, mode=&#39;backParam&#39;)

    # Berechnung des Gradienten des Bias-Vektors
    self.db = np.sum(dZ, axis=0)

    # Rückgabe des Gradienten der Eingabe
    return dX</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Conv2D.convolve"><code class="name flex">
<span>def <span class="ident">convolve</span></span>(<span>self, X, kernel, stride=(1, 1), mode='forward')</span>
</code></dt>
<dd>
<div class="desc"><p>Führt eine Faltung zwischen der Eingabe X und dem Kernel K aus.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Eingabedaten mit Form (batch, inputC, inputH, inputW)</dd>
<dt><strong><code>K</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Kernelmatrix mit Form (F, inputC, kernelH, kernelW)</dd>
<dt><strong><code>s</code></strong> :&ensp;<code>tuple</code> of <code>int</code>, optional</dt>
<dd>Schritte entlang der Höhe und Breite (sH, sW). Standardwert ist (1, 1).</dd>
<dt><strong><code>mode</code></strong> :&ensp;<code>{'forward', 'backpropagation', 'backParam'} oder None</code>, optional</dt>
<dd>Modus der Faltung. Standardwert ist 'forward'.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Ausgabedaten mit Form (batch, F, outH, outW) im 'forward' Modus,
(batch, inputC, inputH, inputW) im 'backpropagation' Modus oder
(F, inputC, kernelH, kernelW) im 'backParam' Modus.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>Wenn der gegebene Modus nicht erlaubt ist.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>Diese Funktion verwendet die numpy.einsum-Funktion, um eine effiziente Faltung zwischen der Eingabe und dem Kernel durchzuführen. Sie bereitet eine Unter-Matrix der Eingabe vor, die zur Faltung verwendet wird, indem sie die Methode <code>prepare_subMatrix</code> aufruft. Sie wendet dann die Faltung an, indem sie die Einstein-Summenkonvention verwendet, die je nach Modus variiert. Sie gibt die gefaltete Ausgabe zurück, die je nach Modus eine andere Form hat.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def convolve(self, X, kernel, stride=(1,1), mode=&#39;forward&#39;):
    &#34;&#34;&#34;
    Führt eine Faltung zwischen der Eingabe X und dem Kernel K aus.

    Parameters
    ----------
    X : numpy.ndarray
        Eingabedaten mit Form (batch, inputC, inputH, inputW)
    K : numpy.ndarray
        Kernelmatrix mit Form (F, inputC, kernelH, kernelW)
    s : tuple of int, optional
        Schritte entlang der Höhe und Breite (sH, sW). Standardwert ist (1, 1).
    mode : {&#39;forward&#39;, &#39;backpropagation&#39;, &#39;backParam&#39;} oder None, optional
        Modus der Faltung. Standardwert ist &#39;forward&#39;.

    Returns
    -------
    numpy.ndarray
        Ausgabedaten mit Form (batch, F, outH, outW) im &#39;forward&#39; Modus,
        (batch, inputC, inputH, inputW) im &#39;backpropagation&#39; Modus oder
        (F, inputC, kernelH, kernelW) im &#39;backParam&#39; Modus.

    Raises
    ------
    ValueError
        Wenn der gegebene Modus nicht erlaubt ist.

    Notes
    -----
    Diese Funktion verwendet die numpy.einsum-Funktion, um eine effiziente Faltung zwischen der Eingabe und dem Kernel durchzuführen. Sie bereitet eine Unter-Matrix der Eingabe vor, die zur Faltung verwendet wird, indem sie die Methode `prepare_subMatrix` aufruft. Sie wendet dann die Faltung an, indem sie die Einstein-Summenkonvention verwendet, die je nach Modus variiert. Sie gibt die gefaltete Ausgabe zurück, die je nach Modus eine andere Form hat.
    &#34;&#34;&#34;
    _, _, kernelH, kernelW = kernel.shape
    subM = self.prepare_subMatrix(X, kernelH, kernelW, stride)

    match mode:
        case &#39;forward&#39;: return np.einsum(&#39;fckl,mcijkl-&gt;mfij&#39;, kernel, subM)
        case &#39;backpropagation&#39;: return np.einsum(&#39;fdkl,mcijkl-&gt;mdij&#39;, kernel, subM)
        case &#39;backParam&#39;: return np.einsum(&#39;mfkl,mcijkl-&gt;fcij&#39;, kernel, subM)
        case _: raise ValueError(&#34;The given mode&#34;, mode, &#34; is not allowed, possible modes are: &#39;forward&#39;, &#39;backpropagation&#39; and &#39;backParam&#39;. &#34;)</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Conv2D.dZ_D_dX"><code class="name flex">
<span>def <span class="ident">dZ_D_dX</span></span>(<span>self, dZ_D, imputH, inputW)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dZ_D_dX(self, dZ_D, imputH, inputW):

    # Pad the dilated dZ (dZ_D -&gt; dZ_Dp)

    _, _, Hd, Wd = dZ_D.shape

    padH = imputH - Hd + self.kernelH - 1
    padW = inputW - Wd + self.kernelW - 1

    padding_back = Padding2D(padding=(padH, padW))

    dZ_Dp = padding_back.forward(dZ_D, self.kernel_size, self.stride)

    # Rotate K by 180 degrees

    K_rotated = self.Kernel[:, :, ::-1, ::-1]

    # convolve dZ_Dp with K_rotated

    dXp = self.convolve(dZ_Dp, K_rotated, mode=&#39;backpropagation&#39;)

    dX = self.padding.backpropagation(dXp)

    return dX</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Conv2D.dilate2D"><code class="name flex">
<span>def <span class="ident">dilate2D</span></span>(<span>self, X, Dr=(1, 1))</span>
</code></dt>
<dd>
<div class="desc"><p>Vergrößert die Eingabe X entlang der Höhe und Breite mit einem gegebenen Dilatationsfaktor.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Eingabedaten mit Form (batch, C, H, W)</dd>
<dt><strong><code>Dr</code></strong> :&ensp;<code>tuple</code> of <code>int</code>, optional</dt>
<dd>Dilatationsfaktor entlang der Höhe und Breite (dh, dw). Standardwert ist (1, 1).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Xd</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Dilatierte Eingabe mit Form (batch, C, H<em>dh, W</em>dw)</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>Diese Funktion verwendet die numpy.repeat-Funktion, um die Eingabe entlang der letzten beiden Achsen zu wiederholen, die der Höhe und Breite entsprechen. Sie berechnet die Anzahl der Wiederholungen anhand des Dilatationsfaktors und der Form der Eingabe. Sie gibt eine neue Ansicht auf dem Eingabearray zurück, die keinen zusätzlichen Speicherplatz benötigt.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dilate2D(self, X, Dr=(1,1)):
    &#34;&#34;&#34;
    Vergrößert die Eingabe X entlang der Höhe und Breite mit einem gegebenen Dilatationsfaktor.

    Parameters
    ----------
    X : numpy.ndarray
        Eingabedaten mit Form (batch, C, H, W)
    Dr : tuple of int, optional
        Dilatationsfaktor entlang der Höhe und Breite (dh, dw). Standardwert ist (1, 1).

    Returns
    -------
    Xd : numpy.ndarray
        Dilatierte Eingabe mit Form (batch, C, H*dh, W*dw)

    Notes
    -----
    Diese Funktion verwendet die numpy.repeat-Funktion, um die Eingabe entlang der letzten beiden Achsen zu wiederholen, die der Höhe und Breite entsprechen. Sie berechnet die Anzahl der Wiederholungen anhand des Dilatationsfaktors und der Form der Eingabe. Sie gibt eine neue Ansicht auf dem Eingabearray zurück, die keinen zusätzlichen Speicherplatz benötigt.
    &#34;&#34;&#34;
    dh, dw = Dr # Dilatationsfaktor
    batch, C, H, W = X.shape
    Xd = np.repeat(X, repeats=dw, axis=-1) # Wiederhole die Eingabe entlang der Breite
    Xd = np.repeat(Xd, repeats=dh, axis=-2) # Wiederhole die Eingabe entlang der Höhe
    return Xd # Gib die dilatierte Eingabe zurück</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Conv2D.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X):
    # padding

    self.X = X

    Xp = self.padding.forward(X, self.kernel_size, self.stride)

    # convolve Xp with K
    Z = self.convolve(Xp, self.Kernel, self.stride, &#39;forward&#39;) + self.bias

    a = self.activation.forward(Z)

    return a</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Conv2D.get_dimensions"><code class="name flex">
<span>def <span class="ident">get_dimensions</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Berechnet die Dimensionen des Outputs basierend auf dem Input-Shape.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>input_shape</code></strong> :&ensp;<code>tuple</code> of <code>int</code></dt>
<dd>Die Form des Inputs (3D oder 4D).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>output_shape</code></strong> :&ensp;<code>tuple</code> of <code>int</code></dt>
<dd>Die Form des Outputs (3D oder 4D).</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>Diese Funktion speichert das Input-Shape und das Output-Shape als Attribute der Klasse. Sie berücksichtigt auch das Padding, das für die Faltung verwendet wird, und berechnet die Höhe und Breite des Outputs anhand des Kernels und des Schritts. Sie passt das Output-Shape an die Länge des Input-Shapes an, indem sie die Batch- und Kanalgrößen beibehält oder hinzufügt.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dimensions(self, input_shape):
    &#34;&#34;&#34;
    Berechnet die Dimensionen des Outputs basierend auf dem Input-Shape.

    Parameters
    ----------
    input_shape : tuple of int
        Die Form des Inputs (3D oder 4D).

    Returns
    -------
    output_shape : tuple of int
        Die Form des Outputs (3D oder 4D).

    Notes
    -----
    Diese Funktion speichert das Input-Shape und das Output-Shape als Attribute der Klasse. Sie berücksichtigt auch das Padding, das für die Faltung verwendet wird, und berechnet die Höhe und Breite des Outputs anhand des Kernels und des Schritts. Sie passt das Output-Shape an die Länge des Input-Shapes an, indem sie die Batch- und Kanalgrößen beibehält oder hinzufügt.
    &#34;&#34;&#34;

    # Speichern des Input-Shapes
    self.input_shape_x = input_shape

    # Berechnung des Input-Shapes mit Padding, das tatsächlich für diese Conv2D verwendet wird
    self.input_shape, _ = self.padding.get_dimensions(self.input_shape_x, self.kernel_size, self.stride)

    # Entpacken des Input-Shapes in die entsprechenden Variablen
    *shape, self.inputC, self.inputH, self.inputW = self.input_shape

    # Berechnung der Output-Dimensionen
    self.outH = (self.inputH - self.kernelH) // self.strideH + 1
    self.outW = (self.inputW - self.kernelW) // self.strideW + 1

    # Zuweisung des Output-Shapes basierend auf der Länge des Input-Shapes
    self.output_shape = (*shape, self.inputC,  self.F, self.outH, self.outW) if len(input_shape) == 4 else (self.F, self.outH, self.outW)</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Conv2D.initialize_parameters"><code class="name flex">
<span>def <span class="ident">initialize_parameters</span></span>(<span>self, input_shape, optimizer_type)</span>
</code></dt>
<dd>
<div class="desc"><p>Diese Funktion initialisiert die Parameter der Faltungsschicht.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>input_shape</code></strong> :&ensp;<code>tuple</code> of <code>int</code></dt>
<dd>Form der Eingabe zur Conv2D-Schicht</dd>
<dt><strong><code>optimizer_type</code></strong> :&ensp;<code>str</code></dt>
<dd>Art des Optimierers</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>Diese Funktion gibt nichts zurück, sondern setzt die Attribute der Klasse.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>Diese Funktion ruft die Methode <code>get_dimensions</code> auf, um die Dimensionen des Outputs basierend auf dem Input-Shape zu berechnen. Sie erstellt dann die Gewichtsmatrix (Kernel) und den Bias-Vektor mit der Klasse <code>Weights_initializer</code> und dem angegebenen Initialisierer-Typ. Sie initialisiert auch den Optimizer mit der Klasse <code>Optimizer</code> und dem angegebenen Optimizer-Typ. Sie speichert alle diese Parameter als Attribute der Klasse.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize_parameters(self, input_shape, optimizer_type):
    &#34;&#34;&#34;
    Diese Funktion initialisiert die Parameter der Faltungsschicht.

    Parameters
    ----------
    input_shape : tuple of int
        Form der Eingabe zur Conv2D-Schicht
    optimizer_type : str
        Art des Optimierers

    Returns
    -------
    None
        Diese Funktion gibt nichts zurück, sondern setzt die Attribute der Klasse.

    Notes
    -----
    Diese Funktion ruft die Methode `get_dimensions` auf, um die Dimensionen des Outputs basierend auf dem Input-Shape zu berechnen. Sie erstellt dann die Gewichtsmatrix (Kernel) und den Bias-Vektor mit der Klasse `Weights_initializer` und dem angegebenen Initialisierer-Typ. Sie initialisiert auch den Optimizer mit der Klasse `Optimizer` und dem angegebenen Optimizer-Typ. Sie speichert alle diese Parameter als Attribute der Klasse.
    &#34;&#34;&#34;
    self.get_dimensions(input_shape)

    shapebias = (self.F, self.outH, self.outW)

    if self.Kernel is None:
        shape_Kernel = (self.F, self.inputC, self.kernelH, self.kernelW)

        initializer = Weights_initializer(shape=shape_Kernel, initializer_type=self.weight_initializer_type, seed=self.seed)

        self.Kernel = initializer.get_initializer()
    if self.bias is None:
        self.bias = np.zeros(shapebias)

    self.optimizer = Optimizer(optimizer_type=optimizer_type, shape_W=shape_Kernel, shape_b=shapebias)</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Conv2D.prepare_subMatrix"><code class="name flex">
<span>def <span class="ident">prepare_subMatrix</span></span>(<span>self, X, kernelH, kernelW, stride)</span>
</code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_subMatrix(self, X, kernelH, kernelW, stride):
    batch, inputC, inputH, inputW = X.shape
    strideH, strideW = stride

    outH = (inputH-kernelH)//strideH + 1
    outW = (inputW-kernelW)//strideW + 1

    strides = (inputC*inputH*inputW, inputW*inputH, inputW*strideH, strideW, inputW, 1)
    strides = tuple(i * X.itemsize for i in strides)

    subM = np.lib.stride_tricks.as_strided(X,
                                           shape=(batch, inputC, outH, outW, kernelH, kernelW),
                                           strides=strides)

    return subM</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Conv2D.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, learnrate, batch, it)</span>
</code></dt>
<dd>
<div class="desc"><p>Aktualisiert die Parameter der Faltungsschicht (Kernel und Bias) anhand der Gradienten und des Optimierers.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>learnrate</code></strong> :&ensp;<code>float</code></dt>
<dd>Lernrate</dd>
<dt><strong><code>batch</code></strong> :&ensp;<code>int</code></dt>
<dd>Batch-Größe (Anzahl der Samples im Batch)</dd>
<dt><strong><code>it</code></strong> :&ensp;<code>int</code></dt>
<dd>Iterationsnummer</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>Diese Funktion gibt nichts zurück, sondern aktualisiert die Attribute der Klasse.</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>Diese Funktion verwendet die <code>optimizer</code>-Methode, um die optimierten Gradienten zu erhalten, und die <code>kernel_regularizer</code>- und <code>weight_regularizer</code>-Attribute, um die Regularisierung anzuwenden. Sie aktualisiert dann die Parameter mit der Lernrate und der Batch-Größe, indem sie die <code>numpy.subtract</code>-Funktion verwendet, die eine vektorisierte Berechnung ermöglicht.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self, learnrate, batch, it):
    &#34;&#34;&#34;
    Aktualisiert die Parameter der Faltungsschicht (Kernel und Bias) anhand der Gradienten und des Optimierers.

    Parameters
    ----------
    learnrate : float
        Lernrate
    batch : int
        Batch-Größe (Anzahl der Samples im Batch)
    it : int
        Iterationsnummer

    Returns
    -------
    None
        Diese Funktion gibt nichts zurück, sondern aktualisiert die Attribute der Klasse.

    Notes
    -----
    Diese Funktion verwendet die `optimizer`-Methode, um die optimierten Gradienten zu erhalten, und die `kernel_regularizer`- und `weight_regularizer`-Attribute, um die Regularisierung anzuwenden. Sie aktualisiert dann die Parameter mit der Lernrate und der Batch-Größe, indem sie die `numpy.subtract`-Funktion verwendet, die eine vektorisierte Berechnung ermöglicht.
    &#34;&#34;&#34;
    # Erhalten der optimierten Gradienten
    dK, db = self.optimizer.get_optimization(self.dK, self.db, it)

    # Anwenden der Regularisierung
    if self.kernel_regularizer[0].casefold() == &#39;l2&#39;:
        dK += self.kernel_regularizer[1] * self.Kernel
    elif self.weight_regularizer[0].casefold() == &#39;l1&#39;:
        dK += self.kernel_regularizer[1] * np.sign(self.Kernel)

    # Aktualisieren der Parameter mit der Lernrate und der Batch-Größe
    self.Kernel = np.subtract(self.Kernel, dK * (learnrate / batch))
    if self.use_bias:
        self.bias = np.subtract(self.bias, db * (learnrate / batch))</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Layer.Dense"><code class="flex name class">
<span>class <span class="ident">Dense</span></span>
<span>(</span><span>neurons, activation_type=None, use_bias=True, weight_initializer_type=None, weight_regularizer=None, seed=None, input_dim=None, weights=None, bias=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Parameters:</p>
<p>neurons: (positiver int) Anzahl der Neuronen</p>
<p>activation_type: Art der Aktivierung, Standard: linear,
mögliche Optionen : 'sigmoid', 'linear', 'tanh', 'softmax', 'prelu', 'relu' &hellip;</p>
<p>use_bias: (boolean) gibt an, ob der Bias genutz werden soll</p>
<p>weight_initializer_type: (str) Initzialisierer für die Gewichte</p>
<p>weight_regularizer: (tuple) Regularisierer, welcher auf die Matrix angewendet wird möglic: ('L2', 0.01) or ('L1', 2)</p>
<p>seed: (seed) um das gleiche Netz zu repruduzieren</p>
<p>input_dim: (int) Anzahl der Neuronen der Eingabeschicht</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Dense:

    def __init__(self, neurons, activation_type=None, use_bias=True,
                 weight_initializer_type=None, weight_regularizer=None, seed=None, input_dim=None, weights = None, bias = None):

        &#39;&#39;&#39;
        Parameters:

        neurons: (positiver int) Anzahl der Neuronen

        activation_type: Art der Aktivierung, Standard: linear,  mögliche Optionen : &#39;sigmoid&#39;, &#39;linear&#39;, &#39;tanh&#39;, &#39;softmax&#39;, &#39;prelu&#39;, &#39;relu&#39; ...

        use_bias: (boolean) gibt an, ob der Bias genutz werden soll

        weight_initializer_type: (str) Initzialisierer für die Gewichte

        weight_regularizer: (tuple) Regularisierer, welcher auf die Matrix angewendet wird möglic: (&#39;L2&#39;, 0.01) or (&#39;L1&#39;, 2)

        seed: (seed) um das gleiche Netz zu repruduzieren

        input_dim: (int) Anzahl der Neuronen der Eingabeschicht
        &#39;&#39;&#39;
        self.neurons = neurons
        self.activation_type = activation_type
        self.activation = Activation(activation_type=activation_type) 
        self.use_bias = use_bias 
        self.weight_initializer_type = weight_initializer_type 
        if weight_regularizer is None:
            self.weight_regularizer = (&#39;L2&#39;, 0)
        else:
            self.weight_regularizer = weight_regularizer
        self.seed = seed
        self.input_dim = input_dim
        
        if weights is None:
            self.weight = None
        elif isinstance(weights, np.ndarray):
            self.weight = weights
            self.neurons = weights.shape[-1]
            self.input_shape = weights.shape[0]
        else: raise ValueError(&#34;The given weight type is invalide: &#34;, type(weights))

        if bias is None:
            self.bias = None
        elif isinstance(bias, np.ndarray):
            self.bias = bias
        else: raise ValueError(&#34;The given weight type is invalide: &#34;, type(bias))
        
        if isinstance(self.weight, np.ndarray) and isinstance(self.bias, np.ndarray) and self.bias.shape != (self.neurons, 1): raise ValueError(&#34;The bias Array has the wrong shape of: &#34;, self.bias.shape)

    def initialize_parameters(self, input_shape, optimizer_type):
        &#39;&#39;&#39;
        Diese Funktion initialisiert die Gewichte und den Bias des Neurons und den Optimierer.
        
        Parameter:
        input_shape: (tuple) gibt die Eingabegröße bzw. Neuronen des vorigen Layers an
        optimizer_type: (str) gibt den Typen des Optimizer an
        
        Ausgabe:
        Keine direkte Ausgabe, aber sie aktualisiert die Attribute self.W, self.b und self.optimizer der Klasse.
        &#39;&#39;&#39;
        shapeWeight = (input_shape, self.neurons) #Größe der Gewichte
        shapebias = (self.neurons, 1) #Größe (shape) der Bias-Werte
        if self.weight is None:
            initializer = Weights_initializer(shape=shapeWeight, initializer_type=self.weight_initializer_type, seed=self.seed) #Initialisierer für die Gewichte
            self.weight = initializer.get_initializer() #Initialiseren der Gewichte
        if self.bias is None:
            self.bias = np.zeros(shapebias) #Initialisieren der Bias-Werte

        self.optimizer = Optimizer(optimizer_type=optimizer_type, shape_W=shapeWeight, shape_b=shapebias)

    def forward(self, X):
        &#39;&#39;&#39;
        Diese Funktion führt die Vorwärtspropagation durch.
        
        Parameter:
        X: Eingabedaten
        
        Ausgabe:
        out: Aktivierungswerte nach Anwendung der Aktivierungsfunktion
        &#39;&#39;&#39;
        #out = f(x_i * wi + b)
        self.X = X
        d = np.dot(X, self.weight) # Multiplizieren der Eingaben mit den Gewichten jedes Neurons
        self.s = d + self.bias.T #addiern des Bias-Wertes
        out = self.activation.forward(self.s) #Akktivierungsfunktion anwenden
        return out

    def backpropagation(self, error):
        &#39;&#39;&#39;
        Diese Funktion führt die Rückwärtspropagation durch.
        
        Parameter:
        da: Fehler-Gradient der nächsten Schicht
        
        Ausgabe:
        dX: Fehldergradient nach der Eingabe, welche für die vorigen Schichten benötigt wird
        &#39;&#39;&#39;
        #dX = f&#39;(error_i) * w_i
        dz = self.activation.backpropagation(error) #Ableitung der Aktivierungsfunktion von dem Fehler der nächsten Schicht
        dr = dz.copy()
        self.dbias = np.sum(dz, axis=0).reshape(-1,1) #Änderung der Gewichte
        self.dweight = np.dot((self.X.T), dr) #Änderung der Bias-Werte
        dX = np.dot(dr, (self.weight.T))
        return dX

    def update(self, learnrate, batch, k):
        &#39;&#39;&#39;
        Diese Funktion aktualisiert die Gewichte und den Bias basierend auf den während der Rückwärtspropagation berechneten Ableitungen.
        
        Parameter:
        learnrate: Lernrate
        batch: Batch-Größe (Anzahl der Proben im Batch)
        k: Iterationsnummer
        
        Ausgabe:
        void
        &#39;&#39;&#39;

        dW, db = self.optimizer.get_optimization(self.dweight, self.dbias, k)
        #Anwenden des Geweichtsregularisierers
        if self.weight_regularizer[0].lower()==&#39;l2&#39;:
            dW += self.weight_regularizer[1] * self.weight
        elif self.weight_regularizer[0].lower()==&#39;l1&#39;:
            dW += self.weight_regularizer[1] * np.sign(self.weight)

        self.weight -= dW*(learnrate/batch)
        if self.use_bias:
            self.bias -= db*(learnrate/batch)</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Neural.Layer.Dense.backpropagation"><code class="name flex">
<span>def <span class="ident">backpropagation</span></span>(<span>self, error)</span>
</code></dt>
<dd>
<div class="desc"><p>Diese Funktion führt die Rückwärtspropagation durch.</p>
<p>Parameter:
da: Fehler-Gradient der nächsten Schicht</p>
<p>Ausgabe:
dX: Fehldergradient nach der Eingabe, welche für die vorigen Schichten benötigt wird</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backpropagation(self, error):
    &#39;&#39;&#39;
    Diese Funktion führt die Rückwärtspropagation durch.
    
    Parameter:
    da: Fehler-Gradient der nächsten Schicht
    
    Ausgabe:
    dX: Fehldergradient nach der Eingabe, welche für die vorigen Schichten benötigt wird
    &#39;&#39;&#39;
    #dX = f&#39;(error_i) * w_i
    dz = self.activation.backpropagation(error) #Ableitung der Aktivierungsfunktion von dem Fehler der nächsten Schicht
    dr = dz.copy()
    self.dbias = np.sum(dz, axis=0).reshape(-1,1) #Änderung der Gewichte
    self.dweight = np.dot((self.X.T), dr) #Änderung der Bias-Werte
    dX = np.dot(dr, (self.weight.T))
    return dX</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Dense.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Diese Funktion führt die Vorwärtspropagation durch.</p>
<p>Parameter:
X: Eingabedaten</p>
<p>Ausgabe:
out: Aktivierungswerte nach Anwendung der Aktivierungsfunktion</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X):
    &#39;&#39;&#39;
    Diese Funktion führt die Vorwärtspropagation durch.
    
    Parameter:
    X: Eingabedaten
    
    Ausgabe:
    out: Aktivierungswerte nach Anwendung der Aktivierungsfunktion
    &#39;&#39;&#39;
    #out = f(x_i * wi + b)
    self.X = X
    d = np.dot(X, self.weight) # Multiplizieren der Eingaben mit den Gewichten jedes Neurons
    self.s = d + self.bias.T #addiern des Bias-Wertes
    out = self.activation.forward(self.s) #Akktivierungsfunktion anwenden
    return out</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Dense.initialize_parameters"><code class="name flex">
<span>def <span class="ident">initialize_parameters</span></span>(<span>self, input_shape, optimizer_type)</span>
</code></dt>
<dd>
<div class="desc"><p>Diese Funktion initialisiert die Gewichte und den Bias des Neurons und den Optimierer.</p>
<p>Parameter:
input_shape: (tuple) gibt die Eingabegröße bzw. Neuronen des vorigen Layers an
optimizer_type: (str) gibt den Typen des Optimizer an</p>
<p>Ausgabe:
Keine direkte Ausgabe, aber sie aktualisiert die Attribute self.W, self.b und self.optimizer der Klasse.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def initialize_parameters(self, input_shape, optimizer_type):
    &#39;&#39;&#39;
    Diese Funktion initialisiert die Gewichte und den Bias des Neurons und den Optimierer.
    
    Parameter:
    input_shape: (tuple) gibt die Eingabegröße bzw. Neuronen des vorigen Layers an
    optimizer_type: (str) gibt den Typen des Optimizer an
    
    Ausgabe:
    Keine direkte Ausgabe, aber sie aktualisiert die Attribute self.W, self.b und self.optimizer der Klasse.
    &#39;&#39;&#39;
    shapeWeight = (input_shape, self.neurons) #Größe der Gewichte
    shapebias = (self.neurons, 1) #Größe (shape) der Bias-Werte
    if self.weight is None:
        initializer = Weights_initializer(shape=shapeWeight, initializer_type=self.weight_initializer_type, seed=self.seed) #Initialisierer für die Gewichte
        self.weight = initializer.get_initializer() #Initialiseren der Gewichte
    if self.bias is None:
        self.bias = np.zeros(shapebias) #Initialisieren der Bias-Werte

    self.optimizer = Optimizer(optimizer_type=optimizer_type, shape_W=shapeWeight, shape_b=shapebias)</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Dense.update"><code class="name flex">
<span>def <span class="ident">update</span></span>(<span>self, learnrate, batch, k)</span>
</code></dt>
<dd>
<div class="desc"><p>Diese Funktion aktualisiert die Gewichte und den Bias basierend auf den während der Rückwärtspropagation berechneten Ableitungen.</p>
<p>Parameter:
learnrate: Lernrate
batch: Batch-Größe (Anzahl der Proben im Batch)
k: Iterationsnummer</p>
<p>Ausgabe:
void</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def update(self, learnrate, batch, k):
    &#39;&#39;&#39;
    Diese Funktion aktualisiert die Gewichte und den Bias basierend auf den während der Rückwärtspropagation berechneten Ableitungen.
    
    Parameter:
    learnrate: Lernrate
    batch: Batch-Größe (Anzahl der Proben im Batch)
    k: Iterationsnummer
    
    Ausgabe:
    void
    &#39;&#39;&#39;

    dW, db = self.optimizer.get_optimization(self.dweight, self.dbias, k)
    #Anwenden des Geweichtsregularisierers
    if self.weight_regularizer[0].lower()==&#39;l2&#39;:
        dW += self.weight_regularizer[1] * self.weight
    elif self.weight_regularizer[0].lower()==&#39;l1&#39;:
        dW += self.weight_regularizer[1] * np.sign(self.weight)

    self.weight -= dW*(learnrate/batch)
    if self.use_bias:
        self.bias -= db*(learnrate/batch)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Layer.Dropout"><code class="flex name class">
<span>class <span class="ident">Dropout</span></span>
<span>(</span><span>p)</span>
</code></dt>
<dd>
<div class="desc"><p>Der Konstruktor der Dropout-Klasse. Initialisiert die Dropout-Wahrscheinlichkeit.</p>
<p>Parameter:
p: Dropout-Wahrscheinlichkeit</p>
<p>Ausgabe:
void</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Dropout:

    def __init__(self, p):
        &#39;&#39;&#39;
        Der Konstruktor der Dropout-Klasse. Initialisiert die Dropout-Wahrscheinlichkeit.
        
        Parameter:
        p: Dropout-Wahrscheinlichkeit
        
        Ausgabe:
        void
        &#39;&#39;&#39;
        self.p = p
        if self.p == 0:
            self.p += 1e-6
        if self.p == 1:
            self.p -= 1e-6

    def forward(self, X):
        &#39;&#39;&#39;
        Diese Funktion führt die Vorwärtspropagation durch. Sie erstellt eine Maske mit der gleichen Form wie X, 
        die zufällige Werte enthält, und wendet dann die Maske auf X an.
        
        Parameter:
        X: Eingabedaten
        
        Ausgabe:
        Z: Ausgabedaten nach Anwendung der Dropout-Maske
        &#39;&#39;&#39;
        self.mask = (np.random.rand(*X.shape) &lt; self.p) / self.p
        Z = X * self.mask
        return Z

    def backpropagation(self, dZ):
        &#39;&#39;&#39;
        Diese Funktion führt die Rückwärtspropagation durch. Sie wendet die Dropout-Maske auf den Gradienten an.
        
        Parameter:
        dZ: Gradient der Ausgabedaten
        
        Ausgabe:
        dX: Gradient der Eingabedaten nach Anwendung der Dropout-Maske
        &#39;&#39;&#39;
        dX = dZ * self.mask
        return dX</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Neural.Layer.Dropout.backpropagation"><code class="name flex">
<span>def <span class="ident">backpropagation</span></span>(<span>self, dZ)</span>
</code></dt>
<dd>
<div class="desc"><p>Diese Funktion führt die Rückwärtspropagation durch. Sie wendet die Dropout-Maske auf den Gradienten an.</p>
<p>Parameter:
dZ: Gradient der Ausgabedaten</p>
<p>Ausgabe:
dX: Gradient der Eingabedaten nach Anwendung der Dropout-Maske</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backpropagation(self, dZ):
    &#39;&#39;&#39;
    Diese Funktion führt die Rückwärtspropagation durch. Sie wendet die Dropout-Maske auf den Gradienten an.
    
    Parameter:
    dZ: Gradient der Ausgabedaten
    
    Ausgabe:
    dX: Gradient der Eingabedaten nach Anwendung der Dropout-Maske
    &#39;&#39;&#39;
    dX = dZ * self.mask
    return dX</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Dropout.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Diese Funktion führt die Vorwärtspropagation durch. Sie erstellt eine Maske mit der gleichen Form wie X,
die zufällige Werte enthält, und wendet dann die Maske auf X an.</p>
<p>Parameter:
X: Eingabedaten</p>
<p>Ausgabe:
Z: Ausgabedaten nach Anwendung der Dropout-Maske</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X):
    &#39;&#39;&#39;
    Diese Funktion führt die Vorwärtspropagation durch. Sie erstellt eine Maske mit der gleichen Form wie X, 
    die zufällige Werte enthält, und wendet dann die Maske auf X an.
    
    Parameter:
    X: Eingabedaten
    
    Ausgabe:
    Z: Ausgabedaten nach Anwendung der Dropout-Maske
    &#39;&#39;&#39;
    self.mask = (np.random.rand(*X.shape) &lt; self.p) / self.p
    Z = X * self.mask
    return Z</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Layer.Flatten"><code class="flex name class">
<span>class <span class="ident">Flatten</span></span>
</code></dt>
<dd>
<div class="desc"><p>Eine Klasse, die eine Flattening-Schicht in einem künstlichen neuronalen Netzwerk implementiert.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>batch</code></strong> :&ensp;<code>int</code></dt>
<dd>Die Batch-Größe der Eingabedaten.</dd>
<dt><strong><code>inputC</code></strong> :&ensp;<code>int</code></dt>
<dd>Die Anzahl der Kanäle der Eingabedaten.</dd>
<dt><strong><code>inputH</code></strong> :&ensp;<code>int</code></dt>
<dd>Die Höhe der Eingabedaten.</dd>
<dt><strong><code>inputW</code></strong> :&ensp;<code>int</code></dt>
<dd>Die Breite der Eingabedaten.</dd>
<dt><strong><code>output_shape</code></strong> :&ensp;<code>int</code></dt>
<dd>Die Form der Ausgabedaten nach dem Flattening.</dd>
</dl>
<h2 id="methods">Methods</h2>
<p>forward(X)
Flacht die Eingabedaten ab und gibt die Ausgabedaten zurück.
backpropagation(dZ)
Formt den Fehlergradienten dZ zurück in die Form der Eingabedaten und gibt den Fehlergradienten dX zurück.
get_dimensions(input_shape)
Bestimmt die Form der Eingabe- und Ausgabedaten anhand der input_shape.</p>
<p>Initialisiert die Attribute der Flattening-Schicht.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>None</code></strong></dt>
<dd>Diese Methode nimmt keine Parameter entgegen.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>Diese Methode gibt nichts zurück, sondern initialisiert die Attribute der Klasse.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Flatten:
    &#34;&#34;&#34;
    Eine Klasse, die eine Flattening-Schicht in einem künstlichen neuronalen Netzwerk implementiert.

    Attributes
    ----------
    batch : int
        Die Batch-Größe der Eingabedaten.
    inputC : int
        Die Anzahl der Kanäle der Eingabedaten.
    inputH : int
        Die Höhe der Eingabedaten.
    inputW : int
        Die Breite der Eingabedaten.
    output_shape : int
        Die Form der Ausgabedaten nach dem Flattening.

    Methods
    -------
    forward(X)
        Flacht die Eingabedaten ab und gibt die Ausgabedaten zurück.
    backpropagation(dZ)
        Formt den Fehlergradienten dZ zurück in die Form der Eingabedaten und gibt den Fehlergradienten dX zurück.
    get_dimensions(input_shape)
        Bestimmt die Form der Eingabe- und Ausgabedaten anhand der input_shape.
    &#34;&#34;&#34;

    def __init__(self):
        &#34;&#34;&#34;
        Initialisiert die Attribute der Flattening-Schicht.

        Parameters
        ----------
        None
            Diese Methode nimmt keine Parameter entgegen.

        Returns
        -------
        None
            Diese Methode gibt nichts zurück, sondern initialisiert die Attribute der Klasse.
        &#34;&#34;&#34;
        pass

    def forward(self, X):
        &#34;&#34;&#34;
        Flacht die Eingabedaten ab und gibt die Ausgabedaten zurück.

        Parameters
        ----------
        X : numpy.ndarray
            Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.

        Returns
        -------
        numpy.ndarray
            Die Ausgabedaten in der Form (batch, inputC * inputH * inputW).
        &#34;&#34;&#34;
        self.batch, self.inputC, self.inputH, self.inputW = X.shape
        X_flat = X.reshape((self.batch, self.inputC * self.inputH * self.inputW))
        return X_flat

    def backpropagation(self, dZ):
        &#34;&#34;&#34;
        Formt den Fehlergradienten dZ zurück in die Form der Eingabedaten und gibt den Fehlergradienten dX zurück.

        Parameters
        ----------
        dZ : numpy.ndarray
            Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC * inputH * inputW).

        Returns
        -------
        numpy.ndarray
            Der Fehlergradient für die Eingabedaten in der Form (batch, inputC, inputH, inputW).
        &#34;&#34;&#34;
        dX = dZ.reshape((self.batch, self.inputC, self.inputH, self.inputW))
        return dX

    def get_dimensions(self, input_shape):
        &#34;&#34;&#34;
        Bestimmt die Form der Eingabe- und Ausgabedaten anhand der input_shape.

        Parameters
        ----------
        input_shape : tuple of int
            Die Form der Eingabedaten in der Form (batch, inputC, inputH, inputW) oder (inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.

        Returns
        -------
        None
            Diese Methode gibt nichts zurück, sondern aktualisiert die Attribute batch, inputC, inputH, inputW und output_shape der Klasse.
        &#34;&#34;&#34;
        if len(input_shape)==4:
            self.batch, self.inputC, self.inputH, self.inputW = input_shape
        elif len(input_shape)==3:
            self.inputC, self.inputH, self.inputW = input_shape

        self.output_shape = self.inputC * self.inputH * self.inputW</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Neural.Layer.Flatten.backpropagation"><code class="name flex">
<span>def <span class="ident">backpropagation</span></span>(<span>self, dZ)</span>
</code></dt>
<dd>
<div class="desc"><p>Formt den Fehlergradienten dZ zurück in die Form der Eingabedaten und gibt den Fehlergradienten dX zurück.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dZ</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC * inputH * inputW).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Der Fehlergradient für die Eingabedaten in der Form (batch, inputC, inputH, inputW).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backpropagation(self, dZ):
    &#34;&#34;&#34;
    Formt den Fehlergradienten dZ zurück in die Form der Eingabedaten und gibt den Fehlergradienten dX zurück.

    Parameters
    ----------
    dZ : numpy.ndarray
        Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC * inputH * inputW).

    Returns
    -------
    numpy.ndarray
        Der Fehlergradient für die Eingabedaten in der Form (batch, inputC, inputH, inputW).
    &#34;&#34;&#34;
    dX = dZ.reshape((self.batch, self.inputC, self.inputH, self.inputW))
    return dX</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Flatten.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Flacht die Eingabedaten ab und gibt die Ausgabedaten zurück.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Die Ausgabedaten in der Form (batch, inputC * inputH * inputW).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X):
    &#34;&#34;&#34;
    Flacht die Eingabedaten ab und gibt die Ausgabedaten zurück.

    Parameters
    ----------
    X : numpy.ndarray
        Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.

    Returns
    -------
    numpy.ndarray
        Die Ausgabedaten in der Form (batch, inputC * inputH * inputW).
    &#34;&#34;&#34;
    self.batch, self.inputC, self.inputH, self.inputW = X.shape
    X_flat = X.reshape((self.batch, self.inputC * self.inputH * self.inputW))
    return X_flat</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Flatten.get_dimensions"><code class="name flex">
<span>def <span class="ident">get_dimensions</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Bestimmt die Form der Eingabe- und Ausgabedaten anhand der input_shape.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>input_shape</code></strong> :&ensp;<code>tuple</code> of <code>int</code></dt>
<dd>Die Form der Eingabedaten in der Form (batch, inputC, inputH, inputW) oder (inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>Diese Methode gibt nichts zurück, sondern aktualisiert die Attribute batch, inputC, inputH, inputW und output_shape der Klasse.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dimensions(self, input_shape):
    &#34;&#34;&#34;
    Bestimmt die Form der Eingabe- und Ausgabedaten anhand der input_shape.

    Parameters
    ----------
    input_shape : tuple of int
        Die Form der Eingabedaten in der Form (batch, inputC, inputH, inputW) oder (inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.

    Returns
    -------
    None
        Diese Methode gibt nichts zurück, sondern aktualisiert die Attribute batch, inputC, inputH, inputW und output_shape der Klasse.
    &#34;&#34;&#34;
    if len(input_shape)==4:
        self.batch, self.inputC, self.inputH, self.inputW = input_shape
    elif len(input_shape)==3:
        self.inputC, self.inputH, self.inputW = input_shape

    self.output_shape = self.inputC * self.inputH * self.inputW</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Layer.Padding2D"><code class="flex name class">
<span>class <span class="ident">Padding2D</span></span>
<span>(</span><span>padding='valid')</span>
</code></dt>
<dd>
<div class="desc"><p>Eine Klasse, die eine Padding-Schicht für eine Faltungsschicht implementiert.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>padding</code></strong> :&ensp;<code>{'same', 'valid'}</code> or <code>int</code> or <code>tuple</code> of <code>int</code></dt>
<dd>Der Padding-Typ. Erlaubte Typen sind nur 'same', 'valid', eine Ganzzahl oder ein Tupel der Länge 2.</dd>
<dt><strong><code>input_shape</code></strong> :&ensp;<code>tuple</code> of <code>int</code></dt>
<dd>Die Form der Eingabe zur Padding2D-Schicht</dd>
<dt><strong><code>output_shape</code></strong> :&ensp;<code>tuple</code> of <code>int</code></dt>
<dd>Die Form der Ausgabe nach dem Padding</dd>
<dt><strong><code>padT</code></strong> :&ensp;<code>int</code></dt>
<dd>Die Menge an Padding oben</dd>
<dt><strong><code>padB</code></strong> :&ensp;<code>int</code></dt>
<dd>Die Menge an Padding unten</dd>
<dt><strong><code>padL</code></strong> :&ensp;<code>int</code></dt>
<dd>Die Menge an Padding links</dd>
<dt><strong><code>padR</code></strong> :&ensp;<code>int</code></dt>
<dd>Die Menge an Padding rechts</dd>
</dl>
<h2 id="methods">Methods</h2>
<p><strong>init</strong>(padding='valid')
Der Konstruktor der Padding2D-Klasse. Initialisiert den Padding-Typ.
get_dimensions(input_shape, kernel_size=None, stride=(1,1))
Eine Hilfsfunktion, die die Dimension der Ausgabe nach dem Padding bestimmt.
forward(X, kernel_size, stride=(1,1))
Eine Funktion, die die Vorwärtspropagation durchführt. Sie berechnet die Menge an Padding, die benötigt wird, und wendet das Padding auf die Eingabe X an.
backpropagation(dXp)
Führt die Rückwärtspropagation durch. Sie entfernt das Padding vom Gradienten der Ausgabe.</p>
<p>Der Konstruktor der Padding2D-Klasse. Initialisiert den Padding-Typ.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>padding</code></strong> :&ensp;<code>{'same', 'valid'}</code> or <code>int</code> or <code>tuple</code> of <code>int</code>, optional</dt>
<dd>Der Padding-Typ. Erlaubte Typen sind nur 'same', 'valid', eine Ganzzahl oder ein Tupel der Länge 2. Standardwert ist 'valid'.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Padding2D:
    &#34;&#34;&#34;
    Eine Klasse, die eine Padding-Schicht für eine Faltungsschicht implementiert.

    Attributes
    ----------
    padding : {&#39;same&#39;, &#39;valid&#39;} or int or tuple of int
        Der Padding-Typ. Erlaubte Typen sind nur &#39;same&#39;, &#39;valid&#39;, eine Ganzzahl oder ein Tupel der Länge 2.
    input_shape : tuple of int
        Die Form der Eingabe zur Padding2D-Schicht
    output_shape : tuple of int
        Die Form der Ausgabe nach dem Padding
    padT : int
        Die Menge an Padding oben
    padB : int
        Die Menge an Padding unten
    padL : int
        Die Menge an Padding links
    padR : int
        Die Menge an Padding rechts

    Methods
    -------
    __init__(padding=&#39;valid&#39;)
        Der Konstruktor der Padding2D-Klasse. Initialisiert den Padding-Typ.
    get_dimensions(input_shape, kernel_size=None, stride=(1,1))
        Eine Hilfsfunktion, die die Dimension der Ausgabe nach dem Padding bestimmt.
    forward(X, kernel_size, stride=(1,1))
        Eine Funktion, die die Vorwärtspropagation durchführt. Sie berechnet die Menge an Padding, die benötigt wird, und wendet das Padding auf die Eingabe X an.
    backpropagation(dXp)
        Führt die Rückwärtspropagation durch. Sie entfernt das Padding vom Gradienten der Ausgabe.
    &#34;&#34;&#34;

    def __init__(self, padding=&#39;valid&#39;):
        &#34;&#34;&#34;
        Der Konstruktor der Padding2D-Klasse. Initialisiert den Padding-Typ.

        Parameters
        ----------
        padding : {&#39;same&#39;, &#39;valid&#39;} or int or tuple of int, optional
            Der Padding-Typ. Erlaubte Typen sind nur &#39;same&#39;, &#39;valid&#39;, eine Ganzzahl oder ein Tupel der Länge 2. Standardwert ist &#39;valid&#39;.
        &#34;&#34;&#34;
        self.padding = padding

    def get_dimensions(self, input_shape, kernel_size = None, stride=(1,1)):
        &#34;&#34;&#34;
        Eine Hilfsfunktion, die die Dimension der Ausgabe nach dem Padding bestimmt.

        Parameters
        ----------
        input_shape : tuple of int
            Die Form der Eingabe zur Padding2D-Schicht
        kernel_size : tuple of int, optional
            Die Größe des Kernels. Standardwert ist None.
        stride : tuple of int, optional
            Die Schritte entlang der Höhe und Breite (strideH, strideW). Standardwert ist (1, 1).

        Returns
        -------
        output_shape : tuple of int
            Die Form der Ausgabe nach dem Padding
        (padT, padB, padL, padR) : tuple of int
            Ein Tupel, das die Menge an Padding in alle vier Richtungen (oben, unten, links, rechts) angibt
        &#34;&#34;&#34;
        if len(input_shape)==4:
            batch, inputC, inputH, inputW = input_shape
        elif len(input_shape)==3:
            inputC, inputH, inputW = input_shape

        
        strideH, strideW = stride
        padding = self.padding

        if type(padding)==int:
            padT, padB = padding, padding
            padL, padR = padding, padding

        if type(padding)==tuple:
            padH, padW = padding
            padT, padB = padH//2, (padH+1)//2
            padL, padR = padW//2, (padW+1)//2

        elif padding==&#39;valid&#39;:
            padT, padB = 0, 0
            padL, padR = 0, 0

        elif padding==&#39;same&#39;:
            # calculating how much padding is required in all 4 directions
            # (top, bottom, left and right)
            if kernel_size is None: raise ValueError(&#34;Kernel cannot be None if padding is same&#34;)
            kernelH, kernelW = kernel_size
            padH = (strideH-1)*inputH + kernelH - strideH
            padW = (strideW-1)*inputW + kernelW - strideW

            padT, padB = padH//2, (padH+1)//2
            padL, padR = padW//2, (padW+1)//2

        else:
            raise ValueError(&#34;Incorrect padding type. Allowed types are only &#39;same&#39;, &#39;valid&#39;, an integer or a tuple of length 2.&#34;)

        if len(input_shape)==4:
            output_shape = (batch, inputC, inputH+padT+padB, inputW+padL+padR)
        elif len(input_shape)==3:
            output_shape = (inputC, inputH+padT+padB, inputW+padL+padR)
        return output_shape, (padT, padB, padL, padR)

    def forward(self, X, kernel_size, stride=(1,1)):
        &#34;&#34;&#34;
        Eine Funktion, die die Vorwärtspropagation durchführt. Sie berechnet die Menge an Padding, die benötigt wird, und wendet das Padding auf die Eingabe X an.

        Parameters
        ----------
        X : numpy.ndarray
            Die Eingabe mit Form (batch, inputC, inputH, inputW)
        kernel_size : tuple of int
            Die Kernelgröße wie in Conv2D-Schicht angegeben
        stride : tuple of int, optional
            Die Schritte entlang der Höhe und Breite (strideH, strideW). Standardwert ist (1, 1).

        Returns
        -------
        Xp : numpy.ndarray
            Das gepaddete X mit Form (batch, inputC, inputH+padT+padB, inputW+padL+padR)
        &#34;&#34;&#34;
        self.input_shape = X.shape
        batch, inputC, inputH, inputW = self.input_shape

        self.output_shape, (self.padT, self.padB, self.padL, self.padR) = self.get_dimensions(self.input_shape,
                                                                                      kernel_size, stride=stride)

        zeros_r = np.zeros((batch, inputC, inputH, self.padR))
        zeros_l = np.zeros((batch, inputC, inputH, self.padL))
        zeros_t = np.zeros((batch, inputC, self.padT, inputW + self.padL + self.padR))
        zeros_b = np.zeros((batch, inputC, self.padB, inputW + self.padL + self.padR))

        Xp = np.concatenate((X, zeros_r), axis=3)
        Xp = np.concatenate((zeros_l, Xp), axis=3)
        Xp = np.concatenate((zeros_t, Xp), axis=2)
        Xp = np.concatenate((Xp, zeros_b), axis=2)

        return Xp

    def backpropagation(self, dXp):
        &#34;&#34;&#34;
        Führt die Rückwärtspropagation durch. Sie entfernt das Padding vom Gradienten der Ausgabe.

        Parameters
        ----------
        dXp : numpy.ndarray
            Der Backprop-Fehler von gepaddetem X (Xp) mit Form (batch, inputC, inputH+padT+padB, inputW+padL+padR)

        Returns
        -------
        dX : numpy.ndarray
            Der Backprop-Fehler von X mit Form (batch, inputC, inputH, inputW)

        Notes
        -----
        Diese Methode verwendet die Attribute `input_shape`, `padT`, `padB`, `padL` und `padR`, die von der `forward`-Methode gesetzt wurden, um den Gradienten der Ausgabe dXp zu schneiden und das Padding zu entfernen. Sie gibt den Gradienten der Eingabe dX zurück, der die gleiche Form wie die Eingabe hat.
        &#34;&#34;&#34;
        batch, inputC, inputH, inputW = self.input_shape
        dX = dXp[:, :, self.padT:self.padT+inputH, self.padL:self.padL+inputW]
        return dX</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Neural.Layer.Padding2D.backpropagation"><code class="name flex">
<span>def <span class="ident">backpropagation</span></span>(<span>self, dXp)</span>
</code></dt>
<dd>
<div class="desc"><p>Führt die Rückwärtspropagation durch. Sie entfernt das Padding vom Gradienten der Ausgabe.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dXp</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Der Backprop-Fehler von gepaddetem X (Xp) mit Form (batch, inputC, inputH+padT+padB, inputW+padL+padR)</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>dX</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Der Backprop-Fehler von X mit Form (batch, inputC, inputH, inputW)</dd>
</dl>
<h2 id="notes">Notes</h2>
<p>Diese Methode verwendet die Attribute <code>input_shape</code>, <code>padT</code>, <code>padB</code>, <code>padL</code> und <code>padR</code>, die von der <code>forward</code>-Methode gesetzt wurden, um den Gradienten der Ausgabe dXp zu schneiden und das Padding zu entfernen. Sie gibt den Gradienten der Eingabe dX zurück, der die gleiche Form wie die Eingabe hat.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backpropagation(self, dXp):
    &#34;&#34;&#34;
    Führt die Rückwärtspropagation durch. Sie entfernt das Padding vom Gradienten der Ausgabe.

    Parameters
    ----------
    dXp : numpy.ndarray
        Der Backprop-Fehler von gepaddetem X (Xp) mit Form (batch, inputC, inputH+padT+padB, inputW+padL+padR)

    Returns
    -------
    dX : numpy.ndarray
        Der Backprop-Fehler von X mit Form (batch, inputC, inputH, inputW)

    Notes
    -----
    Diese Methode verwendet die Attribute `input_shape`, `padT`, `padB`, `padL` und `padR`, die von der `forward`-Methode gesetzt wurden, um den Gradienten der Ausgabe dXp zu schneiden und das Padding zu entfernen. Sie gibt den Gradienten der Eingabe dX zurück, der die gleiche Form wie die Eingabe hat.
    &#34;&#34;&#34;
    batch, inputC, inputH, inputW = self.input_shape
    dX = dXp[:, :, self.padT:self.padT+inputH, self.padL:self.padL+inputW]
    return dX</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Padding2D.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X, kernel_size, stride=(1, 1))</span>
</code></dt>
<dd>
<div class="desc"><p>Eine Funktion, die die Vorwärtspropagation durchführt. Sie berechnet die Menge an Padding, die benötigt wird, und wendet das Padding auf die Eingabe X an.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Die Eingabe mit Form (batch, inputC, inputH, inputW)</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>tuple</code> of <code>int</code></dt>
<dd>Die Kernelgröße wie in Conv2D-Schicht angegeben</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>tuple</code> of <code>int</code>, optional</dt>
<dd>Die Schritte entlang der Höhe und Breite (strideH, strideW). Standardwert ist (1, 1).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>Xp</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Das gepaddete X mit Form (batch, inputC, inputH+padT+padB, inputW+padL+padR)</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X, kernel_size, stride=(1,1)):
    &#34;&#34;&#34;
    Eine Funktion, die die Vorwärtspropagation durchführt. Sie berechnet die Menge an Padding, die benötigt wird, und wendet das Padding auf die Eingabe X an.

    Parameters
    ----------
    X : numpy.ndarray
        Die Eingabe mit Form (batch, inputC, inputH, inputW)
    kernel_size : tuple of int
        Die Kernelgröße wie in Conv2D-Schicht angegeben
    stride : tuple of int, optional
        Die Schritte entlang der Höhe und Breite (strideH, strideW). Standardwert ist (1, 1).

    Returns
    -------
    Xp : numpy.ndarray
        Das gepaddete X mit Form (batch, inputC, inputH+padT+padB, inputW+padL+padR)
    &#34;&#34;&#34;
    self.input_shape = X.shape
    batch, inputC, inputH, inputW = self.input_shape

    self.output_shape, (self.padT, self.padB, self.padL, self.padR) = self.get_dimensions(self.input_shape,
                                                                                  kernel_size, stride=stride)

    zeros_r = np.zeros((batch, inputC, inputH, self.padR))
    zeros_l = np.zeros((batch, inputC, inputH, self.padL))
    zeros_t = np.zeros((batch, inputC, self.padT, inputW + self.padL + self.padR))
    zeros_b = np.zeros((batch, inputC, self.padB, inputW + self.padL + self.padR))

    Xp = np.concatenate((X, zeros_r), axis=3)
    Xp = np.concatenate((zeros_l, Xp), axis=3)
    Xp = np.concatenate((zeros_t, Xp), axis=2)
    Xp = np.concatenate((Xp, zeros_b), axis=2)

    return Xp</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Padding2D.get_dimensions"><code class="name flex">
<span>def <span class="ident">get_dimensions</span></span>(<span>self, input_shape, kernel_size=None, stride=(1, 1))</span>
</code></dt>
<dd>
<div class="desc"><p>Eine Hilfsfunktion, die die Dimension der Ausgabe nach dem Padding bestimmt.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>input_shape</code></strong> :&ensp;<code>tuple</code> of <code>int</code></dt>
<dd>Die Form der Eingabe zur Padding2D-Schicht</dd>
<dt><strong><code>kernel_size</code></strong> :&ensp;<code>tuple</code> of <code>int</code>, optional</dt>
<dd>Die Größe des Kernels. Standardwert ist None.</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>tuple</code> of <code>int</code>, optional</dt>
<dd>Die Schritte entlang der Höhe und Breite (strideH, strideW). Standardwert ist (1, 1).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><strong><code>output_shape</code></strong> :&ensp;<code>tuple</code> of <code>int</code></dt>
<dd>Die Form der Ausgabe nach dem Padding</dd>
</dl>
<p>(padT, padB, padL, padR) : tuple of int
Ein Tupel, das die Menge an Padding in alle vier Richtungen (oben, unten, links, rechts) angibt</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dimensions(self, input_shape, kernel_size = None, stride=(1,1)):
    &#34;&#34;&#34;
    Eine Hilfsfunktion, die die Dimension der Ausgabe nach dem Padding bestimmt.

    Parameters
    ----------
    input_shape : tuple of int
        Die Form der Eingabe zur Padding2D-Schicht
    kernel_size : tuple of int, optional
        Die Größe des Kernels. Standardwert ist None.
    stride : tuple of int, optional
        Die Schritte entlang der Höhe und Breite (strideH, strideW). Standardwert ist (1, 1).

    Returns
    -------
    output_shape : tuple of int
        Die Form der Ausgabe nach dem Padding
    (padT, padB, padL, padR) : tuple of int
        Ein Tupel, das die Menge an Padding in alle vier Richtungen (oben, unten, links, rechts) angibt
    &#34;&#34;&#34;
    if len(input_shape)==4:
        batch, inputC, inputH, inputW = input_shape
    elif len(input_shape)==3:
        inputC, inputH, inputW = input_shape

    
    strideH, strideW = stride
    padding = self.padding

    if type(padding)==int:
        padT, padB = padding, padding
        padL, padR = padding, padding

    if type(padding)==tuple:
        padH, padW = padding
        padT, padB = padH//2, (padH+1)//2
        padL, padR = padW//2, (padW+1)//2

    elif padding==&#39;valid&#39;:
        padT, padB = 0, 0
        padL, padR = 0, 0

    elif padding==&#39;same&#39;:
        # calculating how much padding is required in all 4 directions
        # (top, bottom, left and right)
        if kernel_size is None: raise ValueError(&#34;Kernel cannot be None if padding is same&#34;)
        kernelH, kernelW = kernel_size
        padH = (strideH-1)*inputH + kernelH - strideH
        padW = (strideW-1)*inputW + kernelW - strideW

        padT, padB = padH//2, (padH+1)//2
        padL, padR = padW//2, (padW+1)//2

    else:
        raise ValueError(&#34;Incorrect padding type. Allowed types are only &#39;same&#39;, &#39;valid&#39;, an integer or a tuple of length 2.&#34;)

    if len(input_shape)==4:
        output_shape = (batch, inputC, inputH+padT+padB, inputW+padL+padR)
    elif len(input_shape)==3:
        output_shape = (inputC, inputH+padT+padB, inputW+padL+padR)
    return output_shape, (padT, padB, padL, padR)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="Neural.Layer.Pooling2D"><code class="flex name class">
<span>class <span class="ident">Pooling2D</span></span>
<span>(</span><span>kernelSize=(2, 2), stride=(2, 2), padding='valid', pool_type='mean')</span>
</code></dt>
<dd>
<div class="desc"><p>Eine Klasse, die eine Pooling-Schicht in einem künstlichen neuronalen Netzwerk implementiert.</p>
<h2 id="attributes">Attributes</h2>
<dl>
<dt><strong><code>padding</code></strong> :&ensp;<code><a title="Neural.Layer.Padding2D" href="#Neural.Layer.Padding2D">Padding2D</a></code></dt>
<dd>Ein Objekt der Klasse Padding2D, das die Art des Padding für die Eingabedaten bestimmt.</dd>
<dt><strong><code>kernelSize</code></strong> :&ensp;<code>tuple</code> of <code>int</code></dt>
<dd>Die Größe des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird ein quadratischer Kernel angenommen.</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>tuple</code> of <code>int</code></dt>
<dd>Die Schrittgröße des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird eine gleiche Schrittgröße in beiden Richtungen angenommen.</dd>
<dt><strong><code>pool_type</code></strong> :&ensp;<code>str</code></dt>
<dd>Die Art des Pooling, das angewendet werden soll. Mögliche Werte sind 'max', 'mean' oder 'min'.</dd>
<dt><strong><code>output_shape</code></strong> :&ensp;<code>tuple</code> of <code>int</code></dt>
<dd>Die Form der Ausgabedaten nach dem Pooling.</dd>
</dl>
<h2 id="methods">Methods</h2>
<p>get_dimensions(input_shape)
Berechnet die Form der Ausgabedaten und das Padding für die Eingabedaten anhand der Kernel- und Schrittgröße.
prepare_subMatrix(X, kernelSize, stride)
Erstellt eine Matrix von Teilmatrizen, die den Pooling-Kerneln entsprechen, aus den Eingabedaten mit Hilfe von numpy-Strides.
pooling(X, kernelSize, stride)
Wendet das Pooling auf die Eingabedaten an und gibt die Ausgabedaten zurück.
prepare_mask(subM, kernelH, kernelW, poolType)
Erstellt eine Maske aus Einsen und Nullen, die angibt, welche Elemente der Teilmatrizen für das Pooling ausgewählt wurden.
mask_dXp(mask, Xp, dZ, kernelH, kernelW)
Multipliziert die Maske mit dem Fehlergradienten dZ, um den Fehlergradienten dXp für die gepaddeten Eingabedaten zu erhalten.
maxmin_pool_backprop(dZ, X, type)
Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ und der Maske für das Max- oder Min-Pooling.
dZ_dZp(dZ)
Erweitert den Fehlergradienten dZ, um ihn mit dem Fehlergradienten dXp in Übereinstimmung zu bringen, indem er ihn mit Nullen auffüllt und wickelt.
averagepool_backprop(dZ, X)
Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ für das Mittelwert-Pooling.
forward(X)
Führt das Pooling auf die Eingabedaten aus und gibt die Ausgabedaten zurück.
backpropagation(dZ)
Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ und des pool_type-Attributs.</p>
<p>Initialisiert die Attribute der Pooling-Schicht.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>kernelSize</code></strong> :&ensp;<code>tuple</code> of <code>int</code>, optional</dt>
<dd>Die Größe des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird ein quadratischer Kernel angenommen. Der Standardwert ist (2,2).</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>tuple</code> of <code>int</code>, optional</dt>
<dd>Die Schrittgröße des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird eine gleiche Schrittgröße in beiden Richtungen angenommen. Der Standardwert ist (2,2).</dd>
<dt><strong><code>padding</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Die Art des Padding für die Eingabedaten. Mögliche Werte sind 'valid', 'same' oder 'full'. Der Standardwert ist 'valid'.</dd>
<dt><strong><code>pool_type</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Die Art des Pooling, das angewendet werden soll. Mögliche Werte sind 'max', 'mean' oder 'min'. Der Standardwert ist 'mean'.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>Diese Methode gibt nichts zurück, sondern initialisiert die Attribute der Klasse.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Pooling2D:
    &#34;&#34;&#34;
    Eine Klasse, die eine Pooling-Schicht in einem künstlichen neuronalen Netzwerk implementiert.

    Attributes
    ----------
    padding : Padding2D
        Ein Objekt der Klasse Padding2D, das die Art des Padding für die Eingabedaten bestimmt.
    kernelSize : tuple of int
        Die Größe des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird ein quadratischer Kernel angenommen.
    stride : tuple of int
        Die Schrittgröße des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird eine gleiche Schrittgröße in beiden Richtungen angenommen.
    pool_type : str
        Die Art des Pooling, das angewendet werden soll. Mögliche Werte sind &#39;max&#39;, &#39;mean&#39; oder &#39;min&#39;.
    output_shape : tuple of int
        Die Form der Ausgabedaten nach dem Pooling.

    Methods
    -------
    get_dimensions(input_shape)
        Berechnet die Form der Ausgabedaten und das Padding für die Eingabedaten anhand der Kernel- und Schrittgröße.
    prepare_subMatrix(X, kernelSize, stride)
        Erstellt eine Matrix von Teilmatrizen, die den Pooling-Kerneln entsprechen, aus den Eingabedaten mit Hilfe von numpy-Strides.
    pooling(X, kernelSize, stride)
        Wendet das Pooling auf die Eingabedaten an und gibt die Ausgabedaten zurück.
    prepare_mask(subM, kernelH, kernelW, poolType)
        Erstellt eine Maske aus Einsen und Nullen, die angibt, welche Elemente der Teilmatrizen für das Pooling ausgewählt wurden.
    mask_dXp(mask, Xp, dZ, kernelH, kernelW)
        Multipliziert die Maske mit dem Fehlergradienten dZ, um den Fehlergradienten dXp für die gepaddeten Eingabedaten zu erhalten.
    maxmin_pool_backprop(dZ, X, type)
        Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ und der Maske für das Max- oder Min-Pooling.
    dZ_dZp(dZ)
        Erweitert den Fehlergradienten dZ, um ihn mit dem Fehlergradienten dXp in Übereinstimmung zu bringen, indem er ihn mit Nullen auffüllt und wickelt.
    averagepool_backprop(dZ, X)
        Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ für das Mittelwert-Pooling.
    forward(X)
        Führt das Pooling auf die Eingabedaten aus und gibt die Ausgabedaten zurück.
    backpropagation(dZ)
        Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ und des pool_type-Attributs.
    &#34;&#34;&#34;
    
    def __init__(self, kernelSize=(2,2), stride=(2,2), padding=&#39;valid&#39;, pool_type=&#39;mean&#39;):
        &#34;&#34;&#34;
        Initialisiert die Attribute der Pooling-Schicht.

        Parameters
        ----------
        kernelSize : tuple of int, optional
            Die Größe des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird ein quadratischer Kernel angenommen. Der Standardwert ist (2,2).
        stride : tuple of int, optional
            Die Schrittgröße des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird eine gleiche Schrittgröße in beiden Richtungen angenommen. Der Standardwert ist (2,2).
        padding : str, optional
            Die Art des Padding für die Eingabedaten. Mögliche Werte sind &#39;valid&#39;, &#39;same&#39; oder &#39;full&#39;. Der Standardwert ist &#39;valid&#39;.
        pool_type : str, optional
            Die Art des Pooling, das angewendet werden soll. Mögliche Werte sind &#39;max&#39;, &#39;mean&#39; oder &#39;min&#39;. Der Standardwert ist &#39;mean&#39;.

        Returns
        -------
        None
            Diese Methode gibt nichts zurück, sondern initialisiert die Attribute der Klasse.
        &#34;&#34;&#34;
        self.padding_type = padding
        self.padding = Padding2D(padding=padding)
        
        self.kernelSize = kernelSize
        if isinstance(kernelSize, int): self.kernel_size = (kernelSize, kernelSize) 
        self.kernelH, self.kernelW = self.kernelSize

        self.stride = stride
        if isinstance(stride, int): self.stride = (stride, stride)
        self.strideH, self.strideW = self.stride

        self.pool_type = pool_type

    def get_dimensions(self, input_shape):
        &#34;&#34;&#34;
        Berechnet die Form der Ausgabedaten und das Padding für die Eingabedaten anhand der Kernel- und Schrittgröße.

        Parameters
        ----------
        input_shape : tuple of int
            Die Form der Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.

        Returns
        -------
        None
            Diese Methode gibt nichts zurück, sondern aktualisiert die Attribute output_shape und padding der Klasse.
        &#34;&#34;&#34;
        #Die Größe der gepolsterten Eingabe ist die richtige input_shape
        input_shape,_ = self.padding.get_dimensions(input_shape, self.kernelSize, self.stride)
        
        *sh, inputH, inputW = input_shape

        outH = (inputH-self.kernelH)//self.strideH + 1
        outW = (inputW-self.kernelW)//self.strideW + 1
        
        self.output_shape = (*sh, outH, outW)
        
    # creates submatrixes for the pooling operation and returns them as numpy array (a view of the input matrix)
    def prepare_subMatrix(self, X, kernelSize, stride):
        &#34;&#34;&#34;
        Erstellt eine Matrix von Teilmatrizen, die den Pooling-Kerneln entsprechen, aus den Eingabedaten mit Hilfe von numpy-Strides.

        Parameters
        ----------
        X : numpy.ndarray
            Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.
        kernelSize : tuple of int
            Die Größe des Pooling-Kernels in Höhe und Breite.
        stride : tuple of int
            Die Schrittgröße des Pooling-Kernels in Höhe und Breite.

        Returns
        -------
        numpy.ndarray
            Die Matrix von Teilmatrizen in der Form (batch, inputC, Oh, Ow, kernelH, kernelW), wobei Oh die Ausgabehöhe, Ow die Ausgabebreite, kernelH die Kernelhöhe und kernelW die Kernelbreite ist.
        &#34;&#34;&#34;
        batch, inputC, inputH, inputW = X.shape #dimension of image (batch- Batch size, inputC - channel size, inputH - height, inputW- width)
        strideH, strideW = stride #strides(Schrittweite)
        kernelH, kernelW = kernelSize #size of filter

        Oh = (inputH-kernelH)//strideH + 1    #calc height of output Matrix
        Ow = (inputW-kernelW)//strideW + 1    #calc width of output Matrix

        strides = (inputC*inputH*inputW, inputH*inputW, inputW*strideH, strideW, inputW, 1) # tuple of strides of matrix (number of lement to pass) 
        strides = tuple(i * X.itemsize for i in strides) #x.itemsize - size of the item in Byte (INT =4), list of bytes to pass for each dimension

        subM = np.lib.stride_tricks.as_strided(X, shape=(batch, inputC, Oh, Ow, kernelH, kernelW), strides=strides) #create the submatrixes (Der Codeblock subM = np.lib.stride_tricks.as_strided(X, shape=(batch, inputC, Oh, Ow, kernelH, kernelW), strides=strides) verwendet die Funktion as_strided aus dem NumPy-Modul stride_tricks, um eine neue Ansicht der Eingabematrix X zu erstellen. Diese neue Ansicht, subM, repräsentiert eine Sammlung von Unter-Matrizen, die für die Pooling-Operation in einem Convolutional Neural Network (CNN) verwendet werden können. Hier ist eine detaillierte Erklärung: X: Die ursprüngliche Eingabematrix, die die Daten für das CNN enthält. shape=(m, inputC, Oh, Ow, kernelH, kernelW): Das neue Shape-Argument definiert die Form der resultierenden Unter-Matrix subM. m ist die Anzahl der Beispiele im Batch, inputC die Anzahl der Kanäle, Oh und Ow sind die Höhe und Breite der Ausgabe nach dem Pooling, und kernelH und kernelW sind die Höhe und Breite des Pooling-Kerns. strides=strides: Das Strides-Argument gibt an, wie viele Bytes in der ursprünglichen Matrix X übersprungen werden müssen, um zum nächsten Element in jeder Dimension zu gelangen. Die Funktion as_strided ermöglicht es, ohne zusätzlichen Speicherbedarf oder Kopieren von Daten, auf die verschiedenen Regionen der Eingabematrix X zuzugreifen, als wären sie separate Unter-Matrizen. Dies ist besonders nützlich für Pooling-Operationen, bei denen ein kleiner Bereich (der Pooling-Kern) über die gesamte Eingabematrix gleitet und Operationen wie Max-Pooling oder Average-Pooling durchführt. Die resultierende Unter-Matrix subM kann dann verwendet werden, um diese Pooling-Operationen effizient durchzuführen.)
        return subM 

    #perform the pooling operation
    def pooling(self, X, kernelSize=(2,2), stride=(2,2)):
        &#34;&#34;&#34;
        Wendet das Pooling auf die Eingabedaten an und gibt die Ausgabedaten zurück.

        Parameters
        ----------
        X : numpy.ndarray
            Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.
        kernelSize : tuple of int, optional
            Die Größe des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird ein quadratischer Kernel angenommen. Der Standardwert ist (2,2).
        stride : tuple of int, optional
            Die Schrittgröße des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird eine gleiche Schrittgröße in beiden Richtungen angenommen. Der Standardwert ist (2,2).

        Returns
        -------
        numpy.ndarray
            Die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.

        Raises
        ------
        ValueError
            Wenn der pool_type-Attributwert nicht &#39;max&#39;, &#39;mean&#39; oder &#39;min&#39; ist.
        &#34;&#34;&#34;
        subM = self.prepare_subMatrix(X, kernelSize, stride) #create view consisting of sub-matrices

        match self.pool_type:
            case &#39;max&#39;: return np.max(subM, axis=(-2,-1)) #Maxpooling
            case &#39;mean&#39;: return np.mean(subM, axis=(-2,-1)) #Average/Mean Pooling
            case &#39;min&#39; : return np.min(subM, axis = (-2,-1)) #Min Pooling
            case _: raise ValueError(&#34;Allowed pool types are only &#39;max&#39;, &#39;mean&#39; or &#39;min&#39; and not&#34;, str(self.pool_type))
            
    def prepare_mask(self, subM, kernelH, kernelW, poolType = &#39;max&#39;):
        &#34;&#34;&#34;
        Erstellt eine Maske aus Einsen und Nullen, die angibt, welche Elemente der Teilmatrizen für das Max- oder Min-Pooling ausgewählt wurden.

        Parameters
        ----------
        subM : numpy.ndarray
            Die Matrix von Teilmatrizen in der Form (batch, inputC, Oh, Ow, kernelH, kernelW), wobei Oh die Ausgabehöhe, Ow die Ausgabebreite, kernelH die Kernelhöhe und kernelW die Kernelbreite ist.
        kernelH : int
            Die Höhe des Pooling-Kernels.
        kernelW : int
            Die Breite des Pooling-Kernels.
        poolType : str
            Die Art des Pooling, das angewendet werden soll. Mögliche Werte sind &#39;max&#39; oder &#39;min&#39;.

        Returns
        -------
        numpy.ndarray
            Die Maske in der gleichen Form wie subM.
        &#34;&#34;&#34;
        batch, inputC, Oh, Ow, kernelH, kernelW = subM.shape # shape of submatrix (batch- Batchsize, inputC - channel size, inputH - output height, inputW- output width, kernelH - high pooling kernel, w - width pooling kernel)asd

        a = subM.reshape(-1,kernelH*kernelW)
        idx = np.where(poolType.lower() ==&#39;max&#39;, np.argmax(a, axis=1), np.argmin(a, axis=1))
        b = np.zeros(a.shape)
        b[np.arange(b.shape[0]), idx] = 1
        mask = b.reshape((batch, inputC, Oh, Ow, kernelH, kernelW))

        return mask

    def mask_dXp(self, mask, Xp, dZ, kernelH, kernelW):
        &#34;&#34;&#34;
        Multipliziert die Maske mit dem Fehlergradienten dZ, um den Fehlergradienten dXp für die gepaddeten Eingabedaten zu erhalten.

        Parameters
        ----------
        mask : numpy.ndarray
            Die Maske in der Form (batch, inputC, Oh, Ow, kernelH, kernelW), wobei Oh die Ausgabehöhe, Ow die Ausgabebreite, kernelH die Kernelhöhe und kernelW die Kernelbreite ist.
        Xp : numpy.ndarray
            Die gepaddeten Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei inputH die gepaddete Eingabehöhe und inputW die gepaddete Eingabebreite ist.
        dZ : numpy.ndarray
            Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.
        kernelH : int
            Die Höhe des Pooling-Kernels.
        kernelW : int
            Die Breite des Pooling-Kernels.

        Returns
        -------
        numpy.ndarray
            Der Fehlergradient für die gepaddeten Eingabedaten in der gleichen Form wie Xp.
        &#34;&#34;&#34;
        dA = np.einsum(&#39;i,ijk-&gt;ijk&#39;, dZ.reshape(-1), mask.reshape(-1,kernelH,kernelW)).reshape(mask.shape)
        batch, inputC, inputH, inputW = Xp.shape
        strides = (inputC*inputH*inputW, inputH*inputW, inputW, 1)
        strides = tuple(i * Xp.itemsize for i in strides)
        dXp = np.lib.stride_tricks.as_strided(dA, Xp.shape, strides)
        #dXp = np.broadcast_to(dA, Xp.shape)
        return dXp

    #backpropagation of max pooling layer
    def maxmin_pool_backprop(self, dZ, X, type = &#39;max&#39;):
        &#34;&#34;&#34;
        Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ und der Maske für das Max- oder Min-Pooling.

        Parameters
        ----------
        dZ : numpy.ndarray
            Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.
        X : numpy.ndarray
            Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei inputH die Eingabehöhe und inputW die Eingabebreite ist.
        type : str
            Die Art des Pooling, das angewendet werden soll. Mögliche Werte sind &#39;max&#39; oder &#39;min&#39;.

        Returns
        -------
        numpy.ndarray
            Der Fehlergradient für die Eingabedaten in der Form (batch, inputC, inputH, inputW).
        &#34;&#34;&#34;
        Xp = self.padding.forward(X, self.kernelSize, self.stride)

        subM = self.prepare_subMatrix(Xp, self.kernelSize, self.stride)

        _, _, _, _, kernelH, kernelW = subM.shape

        mask = self.prepare_mask(subM, kernelH, kernelW, type)

        dXp = self.mask_dXp(mask, Xp, dZ, kernelH, kernelW)

        return dXp
    
    
    def dZ_dZp(self, dZ):
        &#34;&#34;&#34;
        Erweitert den Fehlergradienten dZ, um ihn mit dem Fehlergradienten dXp in Übereinstimmung zu bringen, indem er ihn mit Nullen auffüllt und wickelt.

        Parameters
        ----------
        dZ : numpy.ndarray
            Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.

        Returns
        -------
        numpy.ndarray
            Der erweiterte Fehlergradient in der Form (batch, inputC, inputH, inputW), wobei inputH die gepaddete Eingabehöhe und inputW die gepaddete Eingabebreite ist.
        &#34;&#34;&#34;
        strideH, strideW = self.stride
        kernelH, kernelW = self.kernelSize
        dZp = np.pad(dZ, pad_width=((0, 0), (0, 0), (0, kernelH-1), (0, kernelW-1)), mode=&#39;constant&#39;)
        dZp = np.pad(dZp, pad_width=((0, 0), (0, 0), (0, -strideH+1), (0, -strideW+1)), mode=&#39;wrap&#39;)
        return dZp
    
    
    #backpropagation of the avarage pooling layer
    def averagepool_backprop(self, dZ, X):
        &#34;&#34;&#34;
        Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ für das Mittelwert-Pooling.

        Parameters
        ----------
        dZ : numpy.ndarray
            Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.
        X : numpy.ndarray
            Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei inputH die Eingabehöhe und inputW die Eingabebreite ist.

        Returns
        -------
        numpy.ndarray
            Der Fehlergradient für die Eingabedaten in der Form (batch, inputC, inputH, inputW).
        &#34;&#34;&#34;
        Xp = self.padding.forward(X, self.kernelSize, self.stride)

        batch, inputC, inputH, inputW = Xp.shape
        dZp = self.dZ_dZp(dZ)
        
        padH = inputH - dZp.shape[-2]
        padW = inputW - dZp.shape[-1]

        padding_back = Padding2D(padding=(padH, padW))

        dXp = padding_back.forward(dZp, s=self.stride, kernel_size=self.kernelSize)

        #return dXp /(inputH*inputW) # (self.strideH*self.strideW)
        return np.mean(dXp, axis=(-2, -1), keepdims=True)
    
    
    def forward(self, X):
        &#34;&#34;&#34;
        Führt das Pooling auf die Eingabedaten aus und gibt die Ausgabedaten zurück.

        Parameters
        ----------
        X : numpy.ndarray
            Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.

        Returns
        -------
        numpy.ndarray
            Die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.
        &#34;&#34;&#34;

        self.X = X
                        
        # padding
        Xp = self.padding.forward(X, self.kernelSize, self.stride)

        Z = self.pooling(Xp, self.kernelSize, self.stride)
        return Z

    def backpropagation(self, dZ):
        &#34;&#34;&#34;
        Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ und des pool_type-Attributs.

        Parameters
        ----------
        dZ : numpy.ndarray
            Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.

        Returns
        -------
        numpy.ndarray
            Der Fehlergradient für die Eingabedaten in der Form (batch, inputC, inputH, inputW).
        &#34;&#34;&#34;
        if self.pool_type==&#39;max&#39;:
            dXp = self.maxmin_pool_backprop(dZ, self.X)
        elif self.pool_type==&#39;mean&#39;:
            dXp = self.averagepool_backprop(dZ.copy(), self.X)
            #dXp2 = self.averagepool_backprop2(dZ.copy(), self.X) #optimized
        dX = self.padding.backpropagation(dXp)
        return dX</code></pre>
</details>
<h3>Methods</h3>
<dl>
<dt id="Neural.Layer.Pooling2D.averagepool_backprop"><code class="name flex">
<span>def <span class="ident">averagepool_backprop</span></span>(<span>self, dZ, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ für das Mittelwert-Pooling.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dZ</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.</dd>
<dt><strong><code>X</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei inputH die Eingabehöhe und inputW die Eingabebreite ist.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Der Fehlergradient für die Eingabedaten in der Form (batch, inputC, inputH, inputW).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def averagepool_backprop(self, dZ, X):
    &#34;&#34;&#34;
    Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ für das Mittelwert-Pooling.

    Parameters
    ----------
    dZ : numpy.ndarray
        Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.
    X : numpy.ndarray
        Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei inputH die Eingabehöhe und inputW die Eingabebreite ist.

    Returns
    -------
    numpy.ndarray
        Der Fehlergradient für die Eingabedaten in der Form (batch, inputC, inputH, inputW).
    &#34;&#34;&#34;
    Xp = self.padding.forward(X, self.kernelSize, self.stride)

    batch, inputC, inputH, inputW = Xp.shape
    dZp = self.dZ_dZp(dZ)
    
    padH = inputH - dZp.shape[-2]
    padW = inputW - dZp.shape[-1]

    padding_back = Padding2D(padding=(padH, padW))

    dXp = padding_back.forward(dZp, s=self.stride, kernel_size=self.kernelSize)

    #return dXp /(inputH*inputW) # (self.strideH*self.strideW)
    return np.mean(dXp, axis=(-2, -1), keepdims=True)</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Pooling2D.backpropagation"><code class="name flex">
<span>def <span class="ident">backpropagation</span></span>(<span>self, dZ)</span>
</code></dt>
<dd>
<div class="desc"><p>Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ und des pool_type-Attributs.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dZ</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Der Fehlergradient für die Eingabedaten in der Form (batch, inputC, inputH, inputW).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def backpropagation(self, dZ):
    &#34;&#34;&#34;
    Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ und des pool_type-Attributs.

    Parameters
    ----------
    dZ : numpy.ndarray
        Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.

    Returns
    -------
    numpy.ndarray
        Der Fehlergradient für die Eingabedaten in der Form (batch, inputC, inputH, inputW).
    &#34;&#34;&#34;
    if self.pool_type==&#39;max&#39;:
        dXp = self.maxmin_pool_backprop(dZ, self.X)
    elif self.pool_type==&#39;mean&#39;:
        dXp = self.averagepool_backprop(dZ.copy(), self.X)
        #dXp2 = self.averagepool_backprop2(dZ.copy(), self.X) #optimized
    dX = self.padding.backpropagation(dXp)
    return dX</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Pooling2D.dZ_dZp"><code class="name flex">
<span>def <span class="ident">dZ_dZp</span></span>(<span>self, dZ)</span>
</code></dt>
<dd>
<div class="desc"><p>Erweitert den Fehlergradienten dZ, um ihn mit dem Fehlergradienten dXp in Übereinstimmung zu bringen, indem er ihn mit Nullen auffüllt und wickelt.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dZ</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Der erweiterte Fehlergradient in der Form (batch, inputC, inputH, inputW), wobei inputH die gepaddete Eingabehöhe und inputW die gepaddete Eingabebreite ist.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def dZ_dZp(self, dZ):
    &#34;&#34;&#34;
    Erweitert den Fehlergradienten dZ, um ihn mit dem Fehlergradienten dXp in Übereinstimmung zu bringen, indem er ihn mit Nullen auffüllt und wickelt.

    Parameters
    ----------
    dZ : numpy.ndarray
        Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.

    Returns
    -------
    numpy.ndarray
        Der erweiterte Fehlergradient in der Form (batch, inputC, inputH, inputW), wobei inputH die gepaddete Eingabehöhe und inputW die gepaddete Eingabebreite ist.
    &#34;&#34;&#34;
    strideH, strideW = self.stride
    kernelH, kernelW = self.kernelSize
    dZp = np.pad(dZ, pad_width=((0, 0), (0, 0), (0, kernelH-1), (0, kernelW-1)), mode=&#39;constant&#39;)
    dZp = np.pad(dZp, pad_width=((0, 0), (0, 0), (0, -strideH+1), (0, -strideW+1)), mode=&#39;wrap&#39;)
    return dZp</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Pooling2D.forward"><code class="name flex">
<span>def <span class="ident">forward</span></span>(<span>self, X)</span>
</code></dt>
<dd>
<div class="desc"><p>Führt das Pooling auf die Eingabedaten aus und gibt die Ausgabedaten zurück.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def forward(self, X):
    &#34;&#34;&#34;
    Führt das Pooling auf die Eingabedaten aus und gibt die Ausgabedaten zurück.

    Parameters
    ----------
    X : numpy.ndarray
        Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.

    Returns
    -------
    numpy.ndarray
        Die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.
    &#34;&#34;&#34;

    self.X = X
                    
    # padding
    Xp = self.padding.forward(X, self.kernelSize, self.stride)

    Z = self.pooling(Xp, self.kernelSize, self.stride)
    return Z</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Pooling2D.get_dimensions"><code class="name flex">
<span>def <span class="ident">get_dimensions</span></span>(<span>self, input_shape)</span>
</code></dt>
<dd>
<div class="desc"><p>Berechnet die Form der Ausgabedaten und das Padding für die Eingabedaten anhand der Kernel- und Schrittgröße.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>input_shape</code></strong> :&ensp;<code>tuple</code> of <code>int</code></dt>
<dd>Die Form der Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>None</code></dt>
<dd>Diese Methode gibt nichts zurück, sondern aktualisiert die Attribute output_shape und padding der Klasse.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_dimensions(self, input_shape):
    &#34;&#34;&#34;
    Berechnet die Form der Ausgabedaten und das Padding für die Eingabedaten anhand der Kernel- und Schrittgröße.

    Parameters
    ----------
    input_shape : tuple of int
        Die Form der Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.

    Returns
    -------
    None
        Diese Methode gibt nichts zurück, sondern aktualisiert die Attribute output_shape und padding der Klasse.
    &#34;&#34;&#34;
    #Die Größe der gepolsterten Eingabe ist die richtige input_shape
    input_shape,_ = self.padding.get_dimensions(input_shape, self.kernelSize, self.stride)
    
    *sh, inputH, inputW = input_shape

    outH = (inputH-self.kernelH)//self.strideH + 1
    outW = (inputW-self.kernelW)//self.strideW + 1
    
    self.output_shape = (*sh, outH, outW)</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Pooling2D.mask_dXp"><code class="name flex">
<span>def <span class="ident">mask_dXp</span></span>(<span>self, mask, Xp, dZ, kernelH, kernelW)</span>
</code></dt>
<dd>
<div class="desc"><p>Multipliziert die Maske mit dem Fehlergradienten dZ, um den Fehlergradienten dXp für die gepaddeten Eingabedaten zu erhalten.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>mask</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Die Maske in der Form (batch, inputC, Oh, Ow, kernelH, kernelW), wobei Oh die Ausgabehöhe, Ow die Ausgabebreite, kernelH die Kernelhöhe und kernelW die Kernelbreite ist.</dd>
<dt><strong><code>Xp</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Die gepaddeten Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei inputH die gepaddete Eingabehöhe und inputW die gepaddete Eingabebreite ist.</dd>
<dt><strong><code>dZ</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.</dd>
<dt><strong><code>kernelH</code></strong> :&ensp;<code>int</code></dt>
<dd>Die Höhe des Pooling-Kernels.</dd>
<dt><strong><code>kernelW</code></strong> :&ensp;<code>int</code></dt>
<dd>Die Breite des Pooling-Kernels.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Der Fehlergradient für die gepaddeten Eingabedaten in der gleichen Form wie Xp.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def mask_dXp(self, mask, Xp, dZ, kernelH, kernelW):
    &#34;&#34;&#34;
    Multipliziert die Maske mit dem Fehlergradienten dZ, um den Fehlergradienten dXp für die gepaddeten Eingabedaten zu erhalten.

    Parameters
    ----------
    mask : numpy.ndarray
        Die Maske in der Form (batch, inputC, Oh, Ow, kernelH, kernelW), wobei Oh die Ausgabehöhe, Ow die Ausgabebreite, kernelH die Kernelhöhe und kernelW die Kernelbreite ist.
    Xp : numpy.ndarray
        Die gepaddeten Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei inputH die gepaddete Eingabehöhe und inputW die gepaddete Eingabebreite ist.
    dZ : numpy.ndarray
        Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.
    kernelH : int
        Die Höhe des Pooling-Kernels.
    kernelW : int
        Die Breite des Pooling-Kernels.

    Returns
    -------
    numpy.ndarray
        Der Fehlergradient für die gepaddeten Eingabedaten in der gleichen Form wie Xp.
    &#34;&#34;&#34;
    dA = np.einsum(&#39;i,ijk-&gt;ijk&#39;, dZ.reshape(-1), mask.reshape(-1,kernelH,kernelW)).reshape(mask.shape)
    batch, inputC, inputH, inputW = Xp.shape
    strides = (inputC*inputH*inputW, inputH*inputW, inputW, 1)
    strides = tuple(i * Xp.itemsize for i in strides)
    dXp = np.lib.stride_tricks.as_strided(dA, Xp.shape, strides)
    #dXp = np.broadcast_to(dA, Xp.shape)
    return dXp</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Pooling2D.maxmin_pool_backprop"><code class="name flex">
<span>def <span class="ident">maxmin_pool_backprop</span></span>(<span>self, dZ, X, type='max')</span>
</code></dt>
<dd>
<div class="desc"><p>Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ und der Maske für das Max- oder Min-Pooling.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>dZ</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.</dd>
<dt><strong><code>X</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei inputH die Eingabehöhe und inputW die Eingabebreite ist.</dd>
<dt><strong><code>type</code></strong> :&ensp;<code>str</code></dt>
<dd>Die Art des Pooling, das angewendet werden soll. Mögliche Werte sind 'max' oder 'min'.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Der Fehlergradient für die Eingabedaten in der Form (batch, inputC, inputH, inputW).</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def maxmin_pool_backprop(self, dZ, X, type = &#39;max&#39;):
    &#34;&#34;&#34;
    Berechnet den Fehlergradienten dX für die Eingabedaten anhand des Fehlergradienten dZ und der Maske für das Max- oder Min-Pooling.

    Parameters
    ----------
    dZ : numpy.ndarray
        Der Fehlergradient für die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.
    X : numpy.ndarray
        Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei inputH die Eingabehöhe und inputW die Eingabebreite ist.
    type : str
        Die Art des Pooling, das angewendet werden soll. Mögliche Werte sind &#39;max&#39; oder &#39;min&#39;.

    Returns
    -------
    numpy.ndarray
        Der Fehlergradient für die Eingabedaten in der Form (batch, inputC, inputH, inputW).
    &#34;&#34;&#34;
    Xp = self.padding.forward(X, self.kernelSize, self.stride)

    subM = self.prepare_subMatrix(Xp, self.kernelSize, self.stride)

    _, _, _, _, kernelH, kernelW = subM.shape

    mask = self.prepare_mask(subM, kernelH, kernelW, type)

    dXp = self.mask_dXp(mask, Xp, dZ, kernelH, kernelW)

    return dXp</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Pooling2D.pooling"><code class="name flex">
<span>def <span class="ident">pooling</span></span>(<span>self, X, kernelSize=(2, 2), stride=(2, 2))</span>
</code></dt>
<dd>
<div class="desc"><p>Wendet das Pooling auf die Eingabedaten an und gibt die Ausgabedaten zurück.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.</dd>
<dt><strong><code>kernelSize</code></strong> :&ensp;<code>tuple</code> of <code>int</code>, optional</dt>
<dd>Die Größe des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird ein quadratischer Kernel angenommen. Der Standardwert ist (2,2).</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>tuple</code> of <code>int</code>, optional</dt>
<dd>Die Schrittgröße des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird eine gleiche Schrittgröße in beiden Richtungen angenommen. Der Standardwert ist (2,2).</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.</dd>
</dl>
<h2 id="raises">Raises</h2>
<dl>
<dt><code>ValueError</code></dt>
<dd>Wenn der pool_type-Attributwert nicht 'max', 'mean' oder 'min' ist.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def pooling(self, X, kernelSize=(2,2), stride=(2,2)):
    &#34;&#34;&#34;
    Wendet das Pooling auf die Eingabedaten an und gibt die Ausgabedaten zurück.

    Parameters
    ----------
    X : numpy.ndarray
        Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.
    kernelSize : tuple of int, optional
        Die Größe des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird ein quadratischer Kernel angenommen. Der Standardwert ist (2,2).
    stride : tuple of int, optional
        Die Schrittgröße des Pooling-Kernels in Höhe und Breite. Wenn ein einzelner int-Wert angegeben wird, wird eine gleiche Schrittgröße in beiden Richtungen angenommen. Der Standardwert ist (2,2).

    Returns
    -------
    numpy.ndarray
        Die Ausgabedaten in der Form (batch, inputC, Oh, Ow), wobei Oh die Ausgabehöhe und Ow die Ausgabebreite ist.

    Raises
    ------
    ValueError
        Wenn der pool_type-Attributwert nicht &#39;max&#39;, &#39;mean&#39; oder &#39;min&#39; ist.
    &#34;&#34;&#34;
    subM = self.prepare_subMatrix(X, kernelSize, stride) #create view consisting of sub-matrices

    match self.pool_type:
        case &#39;max&#39;: return np.max(subM, axis=(-2,-1)) #Maxpooling
        case &#39;mean&#39;: return np.mean(subM, axis=(-2,-1)) #Average/Mean Pooling
        case &#39;min&#39; : return np.min(subM, axis = (-2,-1)) #Min Pooling
        case _: raise ValueError(&#34;Allowed pool types are only &#39;max&#39;, &#39;mean&#39; or &#39;min&#39; and not&#34;, str(self.pool_type))</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Pooling2D.prepare_mask"><code class="name flex">
<span>def <span class="ident">prepare_mask</span></span>(<span>self, subM, kernelH, kernelW, poolType='max')</span>
</code></dt>
<dd>
<div class="desc"><p>Erstellt eine Maske aus Einsen und Nullen, die angibt, welche Elemente der Teilmatrizen für das Max- oder Min-Pooling ausgewählt wurden.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>subM</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Die Matrix von Teilmatrizen in der Form (batch, inputC, Oh, Ow, kernelH, kernelW), wobei Oh die Ausgabehöhe, Ow die Ausgabebreite, kernelH die Kernelhöhe und kernelW die Kernelbreite ist.</dd>
<dt><strong><code>kernelH</code></strong> :&ensp;<code>int</code></dt>
<dd>Die Höhe des Pooling-Kernels.</dd>
<dt><strong><code>kernelW</code></strong> :&ensp;<code>int</code></dt>
<dd>Die Breite des Pooling-Kernels.</dd>
<dt><strong><code>poolType</code></strong> :&ensp;<code>str</code></dt>
<dd>Die Art des Pooling, das angewendet werden soll. Mögliche Werte sind 'max' oder 'min'.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Die Maske in der gleichen Form wie subM.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_mask(self, subM, kernelH, kernelW, poolType = &#39;max&#39;):
    &#34;&#34;&#34;
    Erstellt eine Maske aus Einsen und Nullen, die angibt, welche Elemente der Teilmatrizen für das Max- oder Min-Pooling ausgewählt wurden.

    Parameters
    ----------
    subM : numpy.ndarray
        Die Matrix von Teilmatrizen in der Form (batch, inputC, Oh, Ow, kernelH, kernelW), wobei Oh die Ausgabehöhe, Ow die Ausgabebreite, kernelH die Kernelhöhe und kernelW die Kernelbreite ist.
    kernelH : int
        Die Höhe des Pooling-Kernels.
    kernelW : int
        Die Breite des Pooling-Kernels.
    poolType : str
        Die Art des Pooling, das angewendet werden soll. Mögliche Werte sind &#39;max&#39; oder &#39;min&#39;.

    Returns
    -------
    numpy.ndarray
        Die Maske in der gleichen Form wie subM.
    &#34;&#34;&#34;
    batch, inputC, Oh, Ow, kernelH, kernelW = subM.shape # shape of submatrix (batch- Batchsize, inputC - channel size, inputH - output height, inputW- output width, kernelH - high pooling kernel, w - width pooling kernel)asd

    a = subM.reshape(-1,kernelH*kernelW)
    idx = np.where(poolType.lower() ==&#39;max&#39;, np.argmax(a, axis=1), np.argmin(a, axis=1))
    b = np.zeros(a.shape)
    b[np.arange(b.shape[0]), idx] = 1
    mask = b.reshape((batch, inputC, Oh, Ow, kernelH, kernelW))

    return mask</code></pre>
</details>
</dd>
<dt id="Neural.Layer.Pooling2D.prepare_subMatrix"><code class="name flex">
<span>def <span class="ident">prepare_subMatrix</span></span>(<span>self, X, kernelSize, stride)</span>
</code></dt>
<dd>
<div class="desc"><p>Erstellt eine Matrix von Teilmatrizen, die den Pooling-Kerneln entsprechen, aus den Eingabedaten mit Hilfe von numpy-Strides.</p>
<h2 id="parameters">Parameters</h2>
<dl>
<dt><strong><code>X</code></strong> :&ensp;<code>numpy.ndarray</code></dt>
<dd>Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.</dd>
<dt><strong><code>kernelSize</code></strong> :&ensp;<code>tuple</code> of <code>int</code></dt>
<dd>Die Größe des Pooling-Kernels in Höhe und Breite.</dd>
<dt><strong><code>stride</code></strong> :&ensp;<code>tuple</code> of <code>int</code></dt>
<dd>Die Schrittgröße des Pooling-Kernels in Höhe und Breite.</dd>
</dl>
<h2 id="returns">Returns</h2>
<dl>
<dt><code>numpy.ndarray</code></dt>
<dd>Die Matrix von Teilmatrizen in der Form (batch, inputC, Oh, Ow, kernelH, kernelW), wobei Oh die Ausgabehöhe, Ow die Ausgabebreite, kernelH die Kernelhöhe und kernelW die Kernelbreite ist.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def prepare_subMatrix(self, X, kernelSize, stride):
    &#34;&#34;&#34;
    Erstellt eine Matrix von Teilmatrizen, die den Pooling-Kerneln entsprechen, aus den Eingabedaten mit Hilfe von numpy-Strides.

    Parameters
    ----------
    X : numpy.ndarray
        Die Eingabedaten in der Form (batch, inputC, inputH, inputW), wobei batch die Batch-Größe, inputC die Anzahl der Kanäle, inputH die Höhe und inputW die Breite ist.
    kernelSize : tuple of int
        Die Größe des Pooling-Kernels in Höhe und Breite.
    stride : tuple of int
        Die Schrittgröße des Pooling-Kernels in Höhe und Breite.

    Returns
    -------
    numpy.ndarray
        Die Matrix von Teilmatrizen in der Form (batch, inputC, Oh, Ow, kernelH, kernelW), wobei Oh die Ausgabehöhe, Ow die Ausgabebreite, kernelH die Kernelhöhe und kernelW die Kernelbreite ist.
    &#34;&#34;&#34;
    batch, inputC, inputH, inputW = X.shape #dimension of image (batch- Batch size, inputC - channel size, inputH - height, inputW- width)
    strideH, strideW = stride #strides(Schrittweite)
    kernelH, kernelW = kernelSize #size of filter

    Oh = (inputH-kernelH)//strideH + 1    #calc height of output Matrix
    Ow = (inputW-kernelW)//strideW + 1    #calc width of output Matrix

    strides = (inputC*inputH*inputW, inputH*inputW, inputW*strideH, strideW, inputW, 1) # tuple of strides of matrix (number of lement to pass) 
    strides = tuple(i * X.itemsize for i in strides) #x.itemsize - size of the item in Byte (INT =4), list of bytes to pass for each dimension

    subM = np.lib.stride_tricks.as_strided(X, shape=(batch, inputC, Oh, Ow, kernelH, kernelW), strides=strides) #create the submatrixes (Der Codeblock subM = np.lib.stride_tricks.as_strided(X, shape=(batch, inputC, Oh, Ow, kernelH, kernelW), strides=strides) verwendet die Funktion as_strided aus dem NumPy-Modul stride_tricks, um eine neue Ansicht der Eingabematrix X zu erstellen. Diese neue Ansicht, subM, repräsentiert eine Sammlung von Unter-Matrizen, die für die Pooling-Operation in einem Convolutional Neural Network (CNN) verwendet werden können. Hier ist eine detaillierte Erklärung: X: Die ursprüngliche Eingabematrix, die die Daten für das CNN enthält. shape=(m, inputC, Oh, Ow, kernelH, kernelW): Das neue Shape-Argument definiert die Form der resultierenden Unter-Matrix subM. m ist die Anzahl der Beispiele im Batch, inputC die Anzahl der Kanäle, Oh und Ow sind die Höhe und Breite der Ausgabe nach dem Pooling, und kernelH und kernelW sind die Höhe und Breite des Pooling-Kerns. strides=strides: Das Strides-Argument gibt an, wie viele Bytes in der ursprünglichen Matrix X übersprungen werden müssen, um zum nächsten Element in jeder Dimension zu gelangen. Die Funktion as_strided ermöglicht es, ohne zusätzlichen Speicherbedarf oder Kopieren von Daten, auf die verschiedenen Regionen der Eingabematrix X zuzugreifen, als wären sie separate Unter-Matrizen. Dies ist besonders nützlich für Pooling-Operationen, bei denen ein kleiner Bereich (der Pooling-Kern) über die gesamte Eingabematrix gleitet und Operationen wie Max-Pooling oder Average-Pooling durchführt. Die resultierende Unter-Matrix subM kann dann verwendet werden, um diese Pooling-Operationen effizient durchzuführen.)
    return subM </code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="Neural" href="index.html">Neural</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="Neural.Layer.Conv2D" href="#Neural.Layer.Conv2D">Conv2D</a></code></h4>
<ul class="">
<li><code><a title="Neural.Layer.Conv2D.backpropagation" href="#Neural.Layer.Conv2D.backpropagation">backpropagation</a></code></li>
<li><code><a title="Neural.Layer.Conv2D.convolve" href="#Neural.Layer.Conv2D.convolve">convolve</a></code></li>
<li><code><a title="Neural.Layer.Conv2D.dZ_D_dX" href="#Neural.Layer.Conv2D.dZ_D_dX">dZ_D_dX</a></code></li>
<li><code><a title="Neural.Layer.Conv2D.dilate2D" href="#Neural.Layer.Conv2D.dilate2D">dilate2D</a></code></li>
<li><code><a title="Neural.Layer.Conv2D.forward" href="#Neural.Layer.Conv2D.forward">forward</a></code></li>
<li><code><a title="Neural.Layer.Conv2D.get_dimensions" href="#Neural.Layer.Conv2D.get_dimensions">get_dimensions</a></code></li>
<li><code><a title="Neural.Layer.Conv2D.initialize_parameters" href="#Neural.Layer.Conv2D.initialize_parameters">initialize_parameters</a></code></li>
<li><code><a title="Neural.Layer.Conv2D.prepare_subMatrix" href="#Neural.Layer.Conv2D.prepare_subMatrix">prepare_subMatrix</a></code></li>
<li><code><a title="Neural.Layer.Conv2D.update" href="#Neural.Layer.Conv2D.update">update</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Layer.Dense" href="#Neural.Layer.Dense">Dense</a></code></h4>
<ul class="">
<li><code><a title="Neural.Layer.Dense.backpropagation" href="#Neural.Layer.Dense.backpropagation">backpropagation</a></code></li>
<li><code><a title="Neural.Layer.Dense.forward" href="#Neural.Layer.Dense.forward">forward</a></code></li>
<li><code><a title="Neural.Layer.Dense.initialize_parameters" href="#Neural.Layer.Dense.initialize_parameters">initialize_parameters</a></code></li>
<li><code><a title="Neural.Layer.Dense.update" href="#Neural.Layer.Dense.update">update</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Layer.Dropout" href="#Neural.Layer.Dropout">Dropout</a></code></h4>
<ul class="">
<li><code><a title="Neural.Layer.Dropout.backpropagation" href="#Neural.Layer.Dropout.backpropagation">backpropagation</a></code></li>
<li><code><a title="Neural.Layer.Dropout.forward" href="#Neural.Layer.Dropout.forward">forward</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Layer.Flatten" href="#Neural.Layer.Flatten">Flatten</a></code></h4>
<ul class="">
<li><code><a title="Neural.Layer.Flatten.backpropagation" href="#Neural.Layer.Flatten.backpropagation">backpropagation</a></code></li>
<li><code><a title="Neural.Layer.Flatten.forward" href="#Neural.Layer.Flatten.forward">forward</a></code></li>
<li><code><a title="Neural.Layer.Flatten.get_dimensions" href="#Neural.Layer.Flatten.get_dimensions">get_dimensions</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Layer.Padding2D" href="#Neural.Layer.Padding2D">Padding2D</a></code></h4>
<ul class="">
<li><code><a title="Neural.Layer.Padding2D.backpropagation" href="#Neural.Layer.Padding2D.backpropagation">backpropagation</a></code></li>
<li><code><a title="Neural.Layer.Padding2D.forward" href="#Neural.Layer.Padding2D.forward">forward</a></code></li>
<li><code><a title="Neural.Layer.Padding2D.get_dimensions" href="#Neural.Layer.Padding2D.get_dimensions">get_dimensions</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="Neural.Layer.Pooling2D" href="#Neural.Layer.Pooling2D">Pooling2D</a></code></h4>
<ul class="">
<li><code><a title="Neural.Layer.Pooling2D.averagepool_backprop" href="#Neural.Layer.Pooling2D.averagepool_backprop">averagepool_backprop</a></code></li>
<li><code><a title="Neural.Layer.Pooling2D.backpropagation" href="#Neural.Layer.Pooling2D.backpropagation">backpropagation</a></code></li>
<li><code><a title="Neural.Layer.Pooling2D.dZ_dZp" href="#Neural.Layer.Pooling2D.dZ_dZp">dZ_dZp</a></code></li>
<li><code><a title="Neural.Layer.Pooling2D.forward" href="#Neural.Layer.Pooling2D.forward">forward</a></code></li>
<li><code><a title="Neural.Layer.Pooling2D.get_dimensions" href="#Neural.Layer.Pooling2D.get_dimensions">get_dimensions</a></code></li>
<li><code><a title="Neural.Layer.Pooling2D.mask_dXp" href="#Neural.Layer.Pooling2D.mask_dXp">mask_dXp</a></code></li>
<li><code><a title="Neural.Layer.Pooling2D.maxmin_pool_backprop" href="#Neural.Layer.Pooling2D.maxmin_pool_backprop">maxmin_pool_backprop</a></code></li>
<li><code><a title="Neural.Layer.Pooling2D.pooling" href="#Neural.Layer.Pooling2D.pooling">pooling</a></code></li>
<li><code><a title="Neural.Layer.Pooling2D.prepare_mask" href="#Neural.Layer.Pooling2D.prepare_mask">prepare_mask</a></code></li>
<li><code><a title="Neural.Layer.Pooling2D.prepare_subMatrix" href="#Neural.Layer.Pooling2D.prepare_subMatrix">prepare_subMatrix</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>